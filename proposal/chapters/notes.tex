\chapter{Notes (will be deleted)}

\cite{arnaout2020enriching}
Enriching Knowledge Bases with Interesting Negative Statements
- 2 approaches:
1) peer-based statistical inferences
2) pattern-based query log extraction
- on-the-spot counterexamples are important to ensure the correctness of learned extraction patterns and associations
- formal entailment regimes like OWL go beyond the OWA and allow to infer negation and lack in ranking facilities
- data constraints and association rules can in principle yield negative statements but face the same challenges.
they develop two complementary approaches:
    1) statistical ranking methods for statements derived based on related entities
    2) pattern-based text extraction applied to high quality search engine query logs
they divide 2 different forms of negative statements:
    1) grounded negative statements
    2) universally negative statement
Two challenges for negative statements generation:
    1) not in conflict with positive is necessary but not sufficient condition for correctness
    2) set of correct near is near-infinite
Ideas:
    1) deletions from time-variant KBs are a natural source -> no interesting negations
    2) PCA approach: huge set -> enumerating use sets of negative statements is not insightful
    3) their approach: two major paradigms: statistical inference and text extraction
- Statistical inference methods: 
    1) association rule mining such as AMIE and RuDiK
    2) embedding models such as TransE and HolE
- Textual information extraction (IE):
    - challenges in textual IE comprise noise and sparsity in observations, canonicalization of entities and predicates
    -> their goal is to combine pattern-base and open information extraction
Peer-based candidate retrieval
- using CWA, negative statements are computed and ranked
    1) Obtain peers: 1. on structured facets 2. graph-based measures 3. entity embeddings
    2) count statements: relative frequency of all predicate-object-pairs, retain maxima
    3) subtract positives
- Ranking negative statements: 4 rank matrices
    1) PEER 2) POP 3) FRQ 4) PIVO-> Ensemble ranking which combines all 4 ranking metrices
- Pattern-based query log extraction
    - supervised methods tailored to predicates ->typically reaches higher precision
    - unsupervised open information extraction -> finds unseen predicates
    - meta-patterns: created 9 handcrafted meta-patterns to retrieve negative information in query logs
    - Query log extraction: frequent queries to the platform -> returned queries not statements, but questions -> transferred to statements
Experimental Evaluation
- ranking quality of method evaluated by Discounted Cumulative Gain (DCG)
- 3 baselines: 1) naive baseline - random 2) TransE 3) HolE
Correctness Evaluation:
- Peer-based inference
    -> only 62\% of the statements were found correct

\cite{arnaout2021negative}
Negative Knowledge for Open-world Wikidata
- review of a statistical inference method for negative statements called peer-based inference
- present Wikinegata, platform that implements this inference over Wikidata
- explicit negative statements can reduce the ambiguity, and improve the relevance of answers
-> assuming completeness within a group of related entities
- ranking features: frequency, unexpectedness, etc.
- three classes of negative statements: 
    1) grounded negative statements
    2) universally negative statements
    3) conditional negative statements
- intermediate ground is the partially-closed-world assumption (PCWA)
- several ideas for generation of negatives:
    1) Negation in Wikidate
        1. Deleted Statements
        2. Count Predicates
        3. Deprecation of Statements
        4. Negated Predicates
        5. No-values
    2) Negation in Logics and Data Management
        - Logical rules and constraints, e.g. Description Logics or OWL to derive negative statements in limited domains
       -> predict way too many correct, but uninformative negative statements
    3) Linguistics and Textual Information Extraction (IE)
        - variety of ways to express negations in human language
following like already mentioned in \cite{arnaout2020negative}
- 3 forms of negative statements
- 3 steps for candidate retrieval
- ranking negatives
- conditional negative statements
Lessons from Real Deployment
1) Thresholds
2) Hierarchical checks
3) annotate functional properties -> can only take 1 property
4) manually predefine interesting aspects for conditional negative statements
    -> automating relevant aspects for every property is a potential direction for future work
5) add offline validation to Wikidata
Issues:
- some wrong negations result from Wikidatas data model
Conclusion:
- improve approach to handle long tail entities -> difficult to find peers for these entities
(lack of facets, no wikipedia embeddings etc.)


\cite{arnaout2020negative}
Negative Statements Considered Useful
General:
    - with negatives: overcome limitations of question answering and can often contribute to informative summaries of entities
    - they present a statistical inference method for compiling and ranking negative statements
    - limitations of KBs is their inability to deal with negative information
    - KBs are not able to distinguish wether a statement is false or unknown -> poses challenges in a variety of applications
    - true negatives are important to ensure the correctness of learned extraction patterns and associations
    - 3 cases of negative statements:
    - 1) grounded negative statements
    - 2) conditional negative statements
    - 3) universal negative statements
    - they assume completeness within an group of peers
State of the art: 
    1) Negation of Existing Knowledge Base
    - statements that were once part of a KB but got subsequently deleted
    - Count and negated Predicates: counts matcfhing with instances (has 5 children -> infers that anyone els eis not child) -> no KB jas a formal way to deal with these
    - KBs contain relations that carry a negative meaning
    - WikiData: No-Values: Wikidata can capture statements about universal absence via the no-value" symbol
    2) Negation in Logics and Data Management
    - logic rules and constraints such as Description Logics or OWL (only one birth place)
    -> rules predict way too many, correct but uninformative negative statements
    3) Related Areas
    - variety of ways to express negation in language, rules-base and vocabulary-based approach
    - patter-based query log extraction
Model:
    - distringuish 2 forms of negative statements:
    - 1) grounded negative statements
    - 2) universally negative statement
    - Two challenges for compiling negative statements
    - 1) not to conflict with positive statements is necessary, but not sufficient condition
    - 2) set of correct negative statements is near-infinite -> ranking methods
Peer-based Statistical Inference:
    1) Peer-based Candidate Retrieval.
         1. Obtain peers 2. Count statements 3. subtract positives
    2) Ranking Negative Statements.
        1. peer frequency (PEER)
        2. object popoularity (POP)
        3. Frequency based Property (FRQ)
        4. Pivoting likelihood (PIVO)
        -> Ensenble Ranking Score created which combined ranking methods
Order-oriented Peer-based Inference
    - orders on peers arise when using real-valued similiarity functions (Jaccard-similiarity, cos distance, embedding vectors))
    - either 1) statial or 2) temporal features for peering
    - Two ranking features:
    - 1) Prefix-volume (VOL)
    - 2) Peer frequency (PEER)
Conditional Negative Statements:
    - challenge is again  that set of true conditional negative statements are near-infinite
    - one way would be to traverse the space + score them with another set of metrics -> ineffective
    -> instead make use of previously generated grounded negative statements
    1) generate grounded negative statements
    2) lift subset pf these into more expressive conditional negative statements -> lifting operation has to be defined
    - option: lift them based on aspects they all share -> not all may be interesting
    - propose to pre-define possible aspects for lifting, using manual definition or using methods to automatically facet discovery
Experimental Evaluation
    - Ranking Metric: Discounted Cumulative Gain (DCG) to compute ranking quality of their method against a number of    baselines
     PCA VS CWA: PCA yields significantly more correct negative statements, though losing the ability to predict universally negative statements
    - Inference with ordered peers: Time-based Qualifiers (TQ) + Time-based Properties (TP)
    - Conditional Negative Statements Evaluation: 1) compression 2) correctness 3) usefulness
    - Relevance of negative statements in:
    - 1) Entity summarization: negative information preferred for 72\% of the entities
    - 2) Decision support: negative features were chosen 16\% points more than positive-only
    - 3) Question answering: peer-based inference outperforms it by far in terms of relevance 72\% vs 44\%
Conclusion and Future work
    - Missing vs. negative statements: fewer highly correct statements vs larger sets of interesting negation candidates
    - Mining more complex negations: negation on sets of entities instead of entity-centric!!
    - Exploring textual information extraction for implicit negations
    
    
    
\cite{arnaoutwikinegata}
Wikinegata: A Knowledge Base with Interesting Negative Statements
- noteworthy negative statements to overcome current limitations
- Wikinegata: plaform to explore negative statements (peer-based inference methodology + ranking)
- KBÂ´s major limitation because of their limitations to  deal with negative information, KBs only contain positive information
- negatives could be easily inferred by the major closed-world assumption (CWA), which uis not realistic to assume
-> not contained statements in KBs are not necerssarily false, merely just unknown
- quality of summaries can be enhanced by interesting negative statements
- explicit negative statements can reduce the ambiguity + imporve the relevance of answers to queries
- partial-closed world assumption (PCWA): uses information present on related entities to find new statements of interest
- open-world assumption (OWA) applies
- additional ranking features (frequency, unexpectedness etc) tuned using a supervised regressionn model
- method is applicable to any other general purpose KB
- 3 classes of negations
- 1) grounded negative statements
- 2) universally negative statements
- 3) conditional negative statements
- Three orthogonal functions for identifying peers:
- 1) structured facets of the subject
- 2) graph-based similarity measures
- 3) embedding-based similarity
- Live Validation: precomputed offline negative statements may turn out incorrect -> changes over time
-> real time validation
- User Feednack with up- and downvotes on the correctness of created negative statements
- 3 scenarious of Wikinegata:
- 1) Understanding Peer-based inference: offer if various levels of introspection, for each negative statement set of peers for which statement is positive
- 2) Knowledge Exploration: surprised that negative statement was created
- 3) Question answering: user wants to find answers to his/her question



% ------------------------------------------------------------------------------------

\cite{wang2018incorporating}
Incorporating GAN for Negative Sampling in Knowledge Representation Learning
- need negative sampling to minimize the margin-based ranking loss
- in random sampling, negatives are too trivial to fit the model efficiently
-> they propose a novel knowledge representation learning framework based on GAN
- generator: advantage to obtain hight-quality negative samples
- discriminator learns embeddings of the entities and relations in KG
- there are great efforts in knowledge representation learning to handle data sparsity and incompleteness
-> embed a KG into a continuous vector space
- captures semantic similarity between entities, helps to solve data sparsity
- negative triplets are constructured by replacing either head or tail entities of positive triplets
-> random sampling: zero loss problem since negatives have high probability to be less usefull, uninformative
- uninformative negative samples even slow down the convergence in training
- embeddings: TransE, TransH, TransR, TransD, Unstructured Model, Structured Embedding, Single Layer Model, Neural Tensor Network
- during training positive triplets are traversed randomly
- two strategies to replace head or tail:
    1) unif: replaces head or tail with equal probability
    2) bern: according to bernoulli distribution
Generator for Negative Sampling
- goal: generate quality negative triplets for discriminator
- two separate embeddings (different from discriminator)
    1) normal relation r 2) reverse of the relation r
- two layers fully connected neural network
- output of generator is a discrete index of the entity, used policy gradient based reinforcement learning
- generator is trained to maximize the expected reward
Two training settings
    1) GAN-scratch: parameters initialized randomly
    2) GAN-pretrain: firstly train original models with random negative sampling-> embeddings used to initialize parameters of the descriminator
-> GAN-pretrain performs better
- framework does not work oin TransR and TransD well
- tested for link prediction and triplet classification
Future work:
- conduct additional analysis on the GAN-based framework
- more complicated architecture of the generator

\cite{cai2017kbgan}
KBGAN: Adversarial Learning for Knowledge Graph Embeddings
- adversial learning framework to improve performances of KGE models.
- generator: negative samples
- discriminator: desired model, learns embeddings
- traditional KGs: lack of capability of accessing the similarities among different entities and relations
- embeddings: represent entities and relations in vector space -> ML techniques to learn the continuous representation
- approaches for negative sampling:
1) randomly sample in uniform distribution
2) leverage external ontology constraints such as entity types
-> resource does not exist
KBGAN:
- generator: consider probability-based, log-loss embeddings models 
- descriminator: use distance-based, margin-loss embedding
- generator has discrete generation step -> cannot use gradient-based approach
-> one-step reinforcement learning setting, use variance-reduction REINFORCE method 
- KGEs: 
    - RESCAL
    - translation-based:
        TransE
        TransH, TransR, TransD: extend TransE by projecting to various spaces
        DistMult: diagonal matrix
        ComplEx: extends DistMult to complex number field
        ManifoldE: manifold rather than a point
        HolE: circular correlation
        ConvE: uses CNN as scoring function
- KBGAN framework relies on policy gradient
- Two common forms of training objectives:
    1) Marginal loss function: used by translation-based models
    2) Log-softmax loss function: score function has probabilistic interpretation
    -> softmax function gives probabilistic distribution -> necessary for generator
- too easy negative samples -> only learn to represent types, not semantics
-> they are less severe to models using log-softmax loss -> sample thousands of negative samples
- KBGAN: softmax probabilities and marginal loss function
- goal: produce good descriminator, GANS aim to create good generator
- future work:
    - semantic relation is still pretty weak, there are still many unrelated entities 

    
\cite{zhang2020efficient}
Efficient, Simple and Automated Negative Sampling for Knowledge Graph Embedding
- negative triplets with large gradients are important but rate 
-> keep track of them with the cache
- NSCaching extended by skip-gram model for graph embedding