\chapter{Problem}
\label{ch:problem}

% general introduction
Since \acp{KG} contain large amount of information, they provide many application possibilities.
However, to achieve good results for these applications we need to learn good embeddings which represent as much information as possible in a graph representation of a low-dimensional vector space.
Most of graph representation learning methods can be unified within a \ac{SampledNCE} framework comprising an encoder that generates node embeddings by learning to distinguish pairs of a positive and a negative triple \cite{MCNS}.
Therefore, we need negative triples for embedding learning which are not available in most \acp{KG} 

For this reason, we are left with two options to collect them \cite{safavi2021negater}: 
On one hand they can be obtained by human annotation which is very cost-prohibitive at scale, but leads to very informative and useful examples. 
On the other, they can be generated ad-hoc, which in turn does not represent a great effort, but also leads to uninformative and useless negative examples.
For this reason, we need to find a way between these two extreme approaches.

% Static Distribution-Based Sampling
Static Distribution-Based Sampling approaches are commonly used because of their simplicity and efficiency, but ignore the dynamics in the negative sampling distribution which lead to 
the vanishing gradient problem \cite{qianunderstanding}.
This problem occurs when the gradient will be vanishingly small and accordingly, small gradients prevent changing the weight value, which can impede the training process or, in the worst case, completely stops the model from further training.
While negative triples, such as those generated by randomly replacing a head or tail entity, are very likely to be negative examples, they are generally uninformative and useless.
E.g by replacing the tail entity in given positive triple (Paderborn, locatedIn, Germany) by randomly selected entity 'Apple' leads to the new negative triplet (Paderborn, locatedIn, Apple), which is a negative triple but also uninformative and useless for embedding learning.
The problem with "too easy" negative triples is less severe to models using log-softmax loss function, because they usually sample a high amount of negatives for one positive triple \cite{cai2017kbgan}.
However, the performance of marginal loss functions can be seriously damaged by the low quality of uniformly sampled negatives since negative-to-positive ratio is always 1:1 \cite{cai2017kbgan}.

% Custom Cluster-Based Sampling
To get more efficiency in the training process and to search for suitable entities in a more targeted way is aimed with the Custom Cluster-Based Sampling methods.
However, as \acp{KG} grow rapidly and are updated frequently, continuous renewing custom clusters is essential and difficult \cite{qianunderstanding}.
% ADD MORE PROBLEMS HERE -> MORE TEXT

% Dynamic Distribution-Based sampling
With these approaches of negative sampling from a fixed distribution two different problems arise:
Since they ignore changes in the distribution of negative triples, they suffer from the vanishing gradient and biased estimation problem \cite{zhang2021efficient}.
The scoring functions tend to give observed (positive) triplets large values and most of the non-observed (probably negative) triplets will have smaller values during the training.
Therefore, if negative triplets are uniformly sampled, it is very likely to pick up one with zero gradients.
This leads to the following main challenges for negative sampling \cite{zhang2021efficient}: 
(i) The negative triple's dynamic distribution has to be captured and 
(ii) triples have to be effectively sampled from this distribution.

% GAN-based approaches
Two pioneering works \ac{KBGAN} \cite{cai2017kbgan} and \ac{IGAN} \cite{IGAN} attempt to address these challenges by a \ac{GAN} \cite{zhang2021efficient}.
\acp{GAN} were originally proposed for generating examples in a continuous space such as images and adapted for the generation of hard negative examples \cite{zhang2021efficient}.

% reference to description of process in related work part
\autoref{sec:kbgan}


However, this is not an efficient training process and it is not effectively sampled from this distribution.
Additionally, it increases the number of training parameters because of the generator and the model suffers from instability and degeneracy.
This results from the fact, that the \textsc{REINFORCE} gradient has high variance for training the generator and only a few negative triples lead to large gradient \cite{zhang2021efficient}.
Consequently, the \ac{GAN}-based models have to put a lot of effort to model the negative triple distribution which leads to instable performances.
Since the generator and discriminator in negative sampling only draw information from the two underlying embedding models, pre-training is necessary, which is a time-consuming endeavor.
However, even if only a few triple pairs (negative + positive triple) are useful for learning embedding, they are not maintained to be able to learn from them at a later time.
Moreover, no other information of the graph is included except the softmax-probability outputs of the generator and the \ac{KGE} model of the discriminator.
Randomly selecting one of the sampled negative triples does not ensure that a useful negative triple is selected.
For embedding, the distinction between negative and positive triples from areas of the KG about which no or incomplete information are available could be particularly important.

In summary, there are already several approaches for Negative Sampling, but they still have problems to capture the dynamic distribution of the negative triples or effectively sample them \cite{zhang2021efficient}.
Therefore, we present a new negative sampling technique which aims to \textbf{improve the efficiency of sampling by using more informative and useful negative examples for the embedding model}.


%For this purpose, various options have already been presented that use different metrics.
%E.g. \cite{DNS} presented with \ac{DNS} a pair-wise \ac{CF} algorithms based on \ac{BPR}.
%Other approaches like \ac{PinSAGE} \cite{PinSAGE} which is based on \ac{GraphSAGE} %\cite{GraphSAGE} rely on random-walk-based \ac{GCN}. 
%One of the latest approaches for negative sampling is \ac{NegatER}
%which ranks a selection of statements using fine-tuned contextual \ac{LM} \cite{safavi2021negater}.
%With this approach, negative sampling process takes place in two steps:
%Firstly, a language model for positive knowledge is fine-tuned by using contrastive learning.
%Subsequently, statements are ranked by k-nearest-neighbors retrieval approach which are most plausible or most surprising.
