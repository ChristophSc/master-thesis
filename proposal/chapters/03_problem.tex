\chapter{Problem}
\label{ch:problem}

\acp{KG} contain large amount of information, for this reason they provide many application possibilities.
However, to achieve good precision and results for these applications we need to learn good embeddings which represent all the information in a low-dimensional vector space.
The less information is lost in this representation as embeddings of the entities and their relationships, the better accuracies can be achieved by the subsequent applications and vice versa.

Usually, applications provide positive information which leads to the fact, that most current \acp{KG} only include positive triples.
For example, for an online warehouse, only information about which products a user likes is stored.
This can be easily collected through tracked information of the user such as buying a product or clicking on it.
Negative information, i.e. which products a user does not like, is much more difficult to determine.
However, these would be particularly useful for many \ac{KG}-related tasks like question answering and recommendation by increasing the certainty in decision-making and avoiding mistakes \cite{safavi2021negater}.
Since no negative information is available in most \acp{KG}, we are left with two options to collect them \cite{safavi2021negater}: 
One one hand they can be obtained by human annotation which is very cost-prohibitive at scale, but leads to very informative and useful samples. 
On the other, they can be generated ad-hoc, which in turn does not represent a great effort, but also leads to uninformative and useless negative samples.
In addition, ignores changes in the distribution of negative triplets and
suffers from the vanishing gradient and biased estimation problem \cite{zhang2021efficient}.
For this reason, we need to find a way between these two extreme approaches.

In recent years, various machine learning approaches have proven to be very useful for finding useful and informative negative examples.
One straight forward approach is to generate negative samples degree-based.
Depending on the underlying data distribution, negative samples can be gathered e.g. randomly replacing entities in a given positive triplet.
Samples generated in this way are very likely to represent negative samples, but usually are very uninformative and useless.
E.g by replacing the head entity in given positive triple (Steve Jobs, founded, Apple Inc.) by randomly selected entity 'Ferrari' leads to the new negative triplet (Ferrari, founded, Apple Inc.), which is a negative sample but also uninformative and useless for embedding learning.

To find informative and more useful negative samples, they can be compared and ranked by different metrices which indicate the closeness to the positive sample.
For this purpose, various options have already been presented that use different metrics.
E.g. \cite{DNS} presented with \ac{DNS} a pair-wise \ac{CF} algorithms based on \ac{BPR}.
Other approaches like \ac{PinSAGE} \cite{PinSAGE} which is based on \ac{GraphSAGE} \cite{GraphSAGE} rely on random-walk-based \ac{GCN}. 
A Producer samples the network neighborhoods of a node, while the consumer 
runs \ac{SGD} on computation graphs    
One of the latest approaches for negative sampling is \ac{NegatER}
which ranks a selection of statements using fine-tuned contextual \ac{LM} \cite{safavi2021negater}.
With this approach, negative sampling process takes place in two steps:
Firstly, a language model for positive knowledge is fine-tuned by using contrastive learning.
Subsequently, statements are ranked by k-nearest-neighbors retrieval approach which are (i) most plausible or (ii) most surprising.

Instead of sampling from a fixed distribution, two pioneering works \ac{KBGAN} \cite{cai2017kbgan} and \ac{IGAN} \cite{IGAN} try to capture the dynamic distribution of negative samples by a \ac{GAN} \cite{zhang2021efficient}.
\acp{GAN} were originally proposed for generating samples in a continuous space such as images and adapted for the generation of hard negative examples.
 \cite{zhang2021efficient}.
However, these approaches still have many problems since the models still suffer from instability and degeneracy, because the REINFORCE gradient has high variance for training the generator and it is still not guaranteed that large-gradient negative triplets are created \cite{zhang2021efficient}.
In these approaches, the generator usually just replaces inefficiently the head or tail entity from a positive triplet.
Other approaches like NSCaching tried to solve these problems by using a cache \cite{zhang2021efficient} to improve the training process of the generator.
or replacing the generator by a self embedding model in \ac{Self-Adv} and sample negative triples according to the current embedding model \cite{Self-Adv}.

    %SANS (2020)
    %-> utilizes the graph structure by using a subset of entities restricted to %either the head or tail entityÂ´s k-hop neighborhood
    
In summary, there are already several approaches for negative sampling, but they still have problems to (i) capture the dynamic distribution of the negative triples or (ii) effectively sample them \cite{zhang2021efficient}.
Synthetic generated negative samples are still far away from the quality of human annotated samples.
For this reason, we present below a new approach for generating negative samples.
% describe approach very short



