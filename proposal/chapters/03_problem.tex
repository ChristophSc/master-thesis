\chapter{Problem}
\label{ch:problem}

Since \acp{KG} contain large amount of information, they provide many application possibilities.
However, to achieve good results for these applications we need to learn good embeddings which represent all the information in a low-dimensional vector space.
The less information is lost in this representation as embeddings of the entities and their relationships, the better accuracies can be achieved by the subsequent applications and vice versa.

%For reasons of efficiency and since most applications provide only positive information, most current \acp{KG} include only positive triples.
%However, these would be particularly useful for many \ac{KG}-related tasks like question answering and recommendation by increasing the certainty in decision-making and avoiding mistakes \cite{safavi2021negater}.
Since no negative information is available in most \acp{KG}, we are left with two options to collect them \cite{safavi2021negater}: 
On one hand they can be obtained by human annotation which is very cost-prohibitive at scale, but leads to very informative and useful examples. 
On the other, they can be generated ad-hoc, which in turn does not represent a great effort, but also leads to uninformative and useless negative samples.
In addition, it ignores changes in the distribution of negative triples and suffers from the vanishing gradient and biased estimation problem \cite{zhang2021efficient}.
For this reason, we need to find a way between these two extreme approaches.

% degree-based sampling
In recent years, various machine learning approaches have proven to find more useful and informative negative examples.
One straight forward approach is to generate negative examples degree-based.
For example, negative examples can be gathered by randomly replacing either head or tail entity in a given positive triple.
Examples generated in this way are very likely to represent negative examples, but usually are uninformative and useless.
E.g by replacing the head entity in given positive triple (Steve Jobs, founded, Apple Inc.) by randomly selected entity 'Paderborn' leads to the new negative triplet (Paderborn, founded, Apple Inc.), which is a negative example but also uninformative and useless for embedding learning.
The problem with "too easy" negative examples is less severe to models using log-softmax loss function, because they usually sample a high amount of negatives for one positive triple \cite{cai2017kbgan}.
However, the performance of marginal loss functions can be seriously damaged by the low quality of uniformly sampled negatives since negative-to-positive ratio is always 1:1 \cite{cai2017kbgan}.

% hard-samples negative sampling
To find informative and more useful negative samples, they can be compared and ranked by different metrics which indicate the closeness to the positive sample.
For this purpose, various options have already been presented that use different metrics.
E.g. \cite{DNS} presented with \ac{DNS} a pair-wise \ac{CF} algorithms based on \ac{BPR}.
Other approaches like \ac{PinSAGE} \cite{PinSAGE} which is based on \ac{GraphSAGE} \cite{GraphSAGE} rely on random-walk-based \ac{GCN}. 
A producer samples the network neighborhoods of a node, while the consumer 
runs \ac{SGD} on computation graphs.    
One of the latest approaches for negative sampling is \ac{NegatER}
which ranks a selection of statements using fine-tuned contextual \ac{LM} \cite{safavi2021negater}.
With this approach, negative sampling process takes place in two steps:
Firstly, a language model for positive knowledge is fine-tuned by using contrastive learning.
Subsequently, statements are ranked by k-nearest-neighbors retrieval approach which are most plausible or most surprising.

% GAN-based approaches
Instead of sampling from a fixed distribution, two pioneering works \ac{KBGAN} \cite{cai2017kbgan} and \ac{IGAN} \cite{IGAN} try to capture the dynamic distribution of negative examples by a \ac{GAN} \cite{zhang2021efficient}.
\acp{GAN} were originally proposed for generating samples in a continuous space such as images and adapted for the generation of hard negative examples \cite{zhang2021efficient}.
The process of \ac{KBGAN} is depicted in the Figure \ref{fig:overview} and described as follows:
\begin{figure*}[t]
  \centering
    \includegraphics[width=0.95\textwidth]{figures/kbgan_overview.pdf}
  \caption{An overview of the \textsc{kbgan} framework (based on: \cite{cai2017kbgan})}
  \label{fig:overview}
\end{figure*}
At first, a set of sampled corrupted triples $Neg(h,r,t)\subset\{(h',r,t)|h'\in\mathcal{E}\}\cup\{(h,r,t')|t'\in\mathcal{E}\}$ is created by uniformly sampling of $N_s$ entities $\in$ \entities to replace $h$ or $t$.
For reasons of efficiency and to reduce the likelihood of creating false negatives, $N_s$ is a relatively small number compared to the number of all possible negative examples \cite{cai2017kbgan}.
Subsequently, an adversarial training with two components starts.
The \textit{generator} G is a \ac{KGE} model with softmax probabilities to provide high-quality negative samples.
These are send to the discriminator (D) whose training objective is marginal loss function.
The generator calculates a probability distribution $p_G(h',r,t'|h,r,t)$ given a positive triple $(h,r,t)$ for all negative triples $(h', r, t') \in Neg$.
One triple is sampled according to distribution as the output and together with the ground truth triple $(h,r,t)$ sent to the discriminator, which calculates their scores. 
During the training, the objective of G is to minimize the score of generated negative triple by policy gradient, and D minimizes the marginal loss between positive and negative triples by gradient descent \cite{cai2017kbgan}.
However, this approach still has many problems since the model suffers from instability and degeneracy, because the \textsc{REINFORCE} gradient has high variance for training the generator and it is still not guaranteed that large-gradient negative triples are created \cite{zhang2021efficient}.

Other approaches like \ac{NSCaching} tried to solve these problems by using a cache \cite{zhang2021efficient} to improve the training process of the generator or replacing the generator by a self embedding model in \ac{Self-Adv} and sample negative triples according to the current embedding model \cite{Self-Adv}.
    
In summary, there are already several approaches for negative sampling, but they still have problems to capture the dynamic distribution of the negative triples or effectively sample them \cite{zhang2021efficient}.
Synthetic generated negative samples are still far away from the quality of human annotated samples.
For this reason, we present a new approach for generating negative examples.
% describe approach very short






