\chapter{Problem}
\label{ch:problem}

\acp{KG} contain large amount of information, for this reason they provide many application possibilities.
However, to achieve good precision and results for these applications we need to learn good embeddings which represent all the information in a low-dimensional vector space.
The less information is lost in this representation as embeddings of the entities and their relationships, the better accuracies can be achieved by the subsequent applications and vice versa.

Usually, applications provide positive information which leads to the fact, that most current \acp{KG} only include positive triples.
For example, for an online warehouse, only information about which products a user likes is stored.
This can be easily collected through tracked information of the user such as buying a product or clicking on it.
Negative information, i.e. which products a user does not like, is much more difficult to determine.
However, these would be particularly useful for many \ac{KG}-related tasks like question answering and recommendation by increasing the certainty in decision-making and avoiding mistakes \cite{safavi2021negater}.
Since no negative information is available in most \acp{KG}, we are left with two options to collect them \cite{safavi2021negater}: 
One one hand they can be obtained by human annotation which is very cost-prohibitive at scale, but leads to very informative and useful samples. 
On the other, they can be generated ad-hoc, which in turn does not represent a great effort, but also leads to uninformative and useless negative samples.
For this reason, we need to find a way between these two extreme approaches.

In recent years, various machine learning approaches have proven to be very useful for finding useful and informative negative examples.
One straight forward approach is to generate negative samples degree-based.
Depending on the underlying data distribution, negative samples can be gathered e.g. randomly replacing entities in a given positive triplet.
Samples generated in this way are very likely to represent negative samples, but usually are very uninformative and useless.
E.g by replacing the head entity in given positive triple (Joe Biden, isPresidentOf, USA) by randomly selected entity 'Ferrari' leads to the new negative triplet (Ferrari, isPresidentOf, USA), which is a negative sample but also uninformative and useless for embedding learning.

To find informative and more useful negative samples, they can be compared and ranked by different metrices which indicate the closeness to the positive sample.
For this purpose, various options have already been presented that use different metrics like \ac{BPR} \cite{DNS} or \ac{WARP} \cite{WARP}. 






    PinSAGE (Pinterest Sage, based on GraphSAGE(SAmple and aggreGatE),)
    -> random-walk-based Graph Convolutional Network (GCN),
    producer: samples node network neighborhoods
    consumer: runs stochastic gradient descent on computation graphs    


    NegatER (Negative Commonsense Knowledge in Enitity and Relation form) (Sep 2021):
    -> ranks a selection of statements using fine-tuned contextual language model
    1) Fine-tuning a language model for positive knowledge using contrastive learning
    2) Ranking statements from k-nearest-neighbors retrieval approach by (i) most plausible or (ii) most surprising statements
    
    
    
3) GENERATIVE ADVERSARIAL NETWORKS
    Generative Adversarial Networks
    IRGAN (a.k.a. Minimax Game): generator + discriminator  (2017) 
    -> generator is designed to fit the true conditional distribution via minimising the Jensen Shannon divergence
    
    KBGAN (Nov 17 2018)
    -> instead of randomly sample entities for tail entitiy,
    probability-based, log-loss embedding models as generator
    -> samples randomly from small subset of candidates
    distance-based, margin-loss embedding models as discriminator
    
    IGAN (Sep 2018) 
    -> generator + discriminator, generator randomly replaces either head or tail entity in given positive sample for KG
    
    NSCaching (Dec 2018)
    -> fixes the suffering from instability and degeneracy of IGAN and KBGAn by using a cache

    NSCaching extended (Jul2021)
    ->  extends NSCaching with skip-gram model for graph embedding. 
    
    
    
In this approach, the generator just replaces the head or tail entity from a positive triplet received from the \ac{KG}, which is highly inefficient.
    During the training, no similarities between entities are found nor there is any evidence to find interesting positive triplets which lead to useful negative ones.
    
??????????????
    self-adversarial sampling (RotateE) (2019)
    -> samples negative triples according to the current embedding model
    treat probability as the weight of the negative sample.

    SANS (2020)
    -> utilizes the graph structure by using a subset of entities restricted to either the head or tail entityÂ´s k-hop neighborhood






- GAN-based approaches
- discriminator takes the role as annotator, while generator provide negative samples
- the more diffcult it is for discriminator to annotate a given triples as positive or negative, the higher the reward for the generator
- models train each other
- Baseline approaches: IGAN and KBGAN
- randomly replace head or tail entity
- NSCaching: 

- infinite set of possible negative samples
- random sampling does not generate informative negative examples, probably useless
- 


COMPARE WITH ACTIVE LEARNING AND UNCERTAINTY SAMPLING



\cite{safavi2021negater}
- 


\cite{zhang2021efficient}





definition of negative samples:
