\chapter{Introduction}
\label{ch:introduction}

\acp{KG} are a network of nodes and edges which represent information of the real world. 
Especially the benefit of combining different data sources and different types of data in meaningful ways leads to many applications in the area of machine learning.
Some of them are question answering, structured search \cite{zhang2019nscaching} and link prediction \cite{cai2017kbgan, Alam2020AffinityDN}.
To store all the different information needed for these different applications, many different \ac{KG} datasets were created e.g. FB15K, WN18, YAGO \cite{ConEx} and WikiData \cite{arnaoutwikinegata}.
Within a \ac{KG} entities are stored as nodes and relations as directed edges \cite{zhang2019nscaching}.
Facts from the reality are represented as a triplet in the form of (head entity, relation, tail entity), denoted as (h,r,t) and indicating that the head entity h (subject) is connected with the tail entity t (object) by a specific relation r (predicate) \cite{zhang2019nscaching, Alam2020AffinityDN}.
One of the main challenges for \acp{KG} is the lack of capability of accessing the similarities among different entities in relations \cite{cai2017kbgan}. 
A renowned area of research in recent years was the transformation of \acp{KG} into a vector space using embedding models, known as \acp{KGE} \cite{Alam2020AffinityDN}.
These approaches aim for creating a good scoring function that can model the complex interactions between entities and relations better \cite{zhang2019nscaching}.
The more precisely the similarities of entities and relations are embedded in a vector space the better following algorithms running on these embeddings will perform.
To learn them, machine learning techniques can be used to learn continuous representation of a \ac{KG} in the latent space \cite{cai2017kbgan}. 
Most of \acp{KG} encode only the available information with true information of the reality.
However, vector-based embedding learning models require negative sampling that contrasts with the already available data and to learn efficiently \cite{Alam2020AffinityDN}.
Since \acp{KG} are incomplete and therefore we do not know if missing triples have to be considered as true or false, we can not follow the \ac{CWA} \cite{arnaout2020enriching}.
Therefore, negative sampling follows the \ac{OWA} which states that asserted statements are true, while the missing ones are just unknown.

Accordingly, in the literature, several negative sample generation methods have been proposed which aim for finding hard negative examples which are useful for embedding learning.
One of the easiest and straightforward approaches is Negative Random Sampling which replaces either the head or the tail entity in a given positive triplet (h,r,t) by any other entity of the \ac{KG}, which remains in the new negative triplet (h’,r,t) or (h,r,t’). 
Even though this approach already works, more useful variants are already being researched in the literature.
All of these methods for the generation of negative examples can be divided into the three groups of Degree-based, Hard-samples and
\ac{GAN}-based Negative Sampling.

Our approach is based on Hard-samples Negative sampling which aims for finding negative examples close to positive ones such that they have a high probability to be positive ones.
In contrast to the already existing approaches we propose a method that contains two components:
The negative sampler and an oracle giving feedback to the sampler.
Our sampler component gets two inputs:
At first, interesting positive triplets from the \ac{KG} and secondly, it receives feedback from the oracle for the quality of the negative example generated before.
Consequently, our sampler represents an Active Learner which iteratively improves the training.
The oracle in turn just receives generated negative examples from the sampler. 
The evaluation of the quality of the negative sample depends on two factors:
At first, it measures the probability of the negative example to be a positive one, 
which indicates the closeness to the positive triple. 
Secondly, it measures the uncertainty of the \ac{KGE} in this area of a \ac{KG}. 
For example, if a \ac{KG} is very incomplete or there is less information about this area, the \ac{KGE} is probably uncertain on how to embed these triples and differentiate between positives and negatives.
Depending on the overall score for the generated negative example, the oracle gives feedback to the sampler.
Either the sample was not good and has to be retrained, or a new positive sample can be retrieved from the \ac{KG} for new training.
As a result, the sampler alternates between re-training and selecting new examples in each iteration.

