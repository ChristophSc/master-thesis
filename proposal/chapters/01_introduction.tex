\chapter{Introduction}
\label{ch:introduction}

\acp{KG} are a network of nodes and edges which represent information of the real world. 
Especially the benefit of combining different data sources and different types of data in meaningful ways leads to many applications in the area of machine learning.
Some of them are question answering, structured search \cite{zhang2019nscaching} and link prediction \cite{cai2017kbgan, Alam2020AffinityDN}.
To store all the different information needed for these different applications, many different \ac{KG} datasets were created e.g. \textsc{FB15K}, \textsc{WN18}, \textsc{YAGO} \cite{ConEx} and \textsc{WikiData} \cite{arnaoutwikinegata}.
Within a \ac{KG} entities are stored as nodes and relations as directed edges \cite{zhang2019nscaching}.
Facts from the reality are represented as a triple in the form of (head entity, relation, tail entity), denoted as $(h, r, t)$ and indicating that the head entity $h$ (subject) is connected with the tail entity $t$ (object) by a specific relation $r$ (predicate) \cite{zhang2019nscaching, Alam2020AffinityDN}.
One of the main challenges for \acp{KG} is the lack of capability of accessing the similarities among different entities in relations \cite{cai2017kbgan}. 
A renowned area of research in recent years is the transformation of \acp{KG} into a low-dimensional vector space using embedding models, known as \acp{KGE} \cite{Alam2020AffinityDN}.
These approaches aim for creating a good scoring function that can model the complex interactions between entities and relations better \cite{zhang2019nscaching}.
The more precisely similarities of entities and relations are embedded in a vector space the better following algorithms on these embeddings will perform.
To learn them, machine learning techniques can be used to learn continuous representation of a \ac{KG} in the latent space \cite{cai2017kbgan}. 
Most of \acp{KG} encode only available information with true information of the reality.
However, vector-based embedding learning models require negative sampling that contrasts with the already available data and to learn efficiently \cite{Alam2020AffinityDN}.
Since \acp{KG} are incomplete and therefore we do not know if missing triples have to be considered as true or false, we can not follow the \ac{CWA} \cite{arnaout2020enriching}.
Therefore, negative sampling follows the \ac{OWA} which states that asserted statements are true, while the missing ones are just unknown.

Accordingly, in the literature, several negative sample generation methods have been proposed which aim for finding negative examples that are useful for embedding learning.
One of the easiest and straightforward approaches is Negative Random Sampling which replaces either the head or the tail entity in a given positive triple $(h, r, t)$ by any other entity of the \ac{KG}, which remains in the new negative triple $(h’,r,t)$ or $(h,r,t’)$. 
Even though this approach already works, more useful variants are already being researched in the literature.

Our approach aims to find hard negative examples which are close to given positive ones to improve the subsequent embedding learning.
In contrast to the already existing approaches we propose a method that improves the sampling process by Uncertainty Sampling.
