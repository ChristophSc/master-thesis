\chapter{Introduction}
\label{ch:introduction}

\acp{KG} are a network of nodes and edges which represent information of the real world. 
Especially the benefit of combining different data sources and different types of data in meaningful ways leads to many applications in the area of machine learning.
Some of them are question answering, structured search \cite{zhang2019nscaching} and link prediction \cite{cai2017kbgan, Alam2020AffinityDN}.
To store all the different information needed for these different applications, many different \ac{KG} datasets were created e.g. \textsc{FB15K}, \textsc{WN18}, \textsc{YAGO} \cite{ConEx} and \textsc{WikiData} \cite{arnaoutwikinegata}.
Within a \ac{KG}, entities are stored as nodes and relations as directed edges \cite{zhang2019nscaching}.
Facts from the reality are represented as a triple in the form of (head entity, relation, tail entity), denoted as $(h, r, t)$ and indicating that the head entity $h$ (subject) is connected with the tail entity $t$ (object) by a specific relation $r$ (predicate) \cite{zhang2019nscaching, Alam2020AffinityDN}.
One of the main challenges for using discrete representation of \ac{KG} is the lack of capability of accessing the similarities among different entities and relations \cite{cai2017kbgan}. 
A renowned area of research in recent years is the transformation of a \acp{KG} into a low-dimensional vector space using embedding models, known as \acp{KGE} \cite{Alam2020AffinityDN}.
Many approaches of \acp{KG} learn vector representations for \acp{KG}, while learning a parametrized function that assigns scores to input triple.
A score of a triple expected to reflect the likelihood that the input triple is true \cite{ConvE}.
Consequently, both positive and negative facts are needed for learning embeddings. 
However, this poses a major problem for \acp{KG}, since most of them do not store false facts and not all positive facts of the reality are stored in the \ac{KG} either.
Therefore, \acp{KG} are incomplete and we do not know if missing triples have to be considered as true or false facts.
Since there are no negative facts, but many embedding learning models (as well as language models \cite{MikolovSCCD13}) require them, we need to create them.
This would not be a problem in the case of the \ac{CWA}, where all facts not available could simply be assumed to be false \cite{arnaout2020enriching}.
However, since this cannot be assumed for \acp{KG}, the generation of negative examples follows the \ac{OWA} which states that asserted statements are true, while the missing ones are just unknown.

Due to the need of negative facts, in the literature several Negative Sampling methods have been proposed which samples negative triplets from non-observed ones \cite{Alam2020AffinityDN} 
Subsequently, these generated examples are used to facilitate the learning process . 
A standard technique is Negative Random Sampling which replaces either the head or the tail entity in a given positive triple $(h, r, t)$ by any other entity of the \ac{KG}, which remains in the new negative triple $(h’,r,t)$ or $(h,r,t’)$. 
Although models trained with Negative Random Sampling have been successfully applied in many applications \cite{TransE}, there have been several attempts to model more effective strategies, e.g. 
a self-adversarial technique \cite{RotatE} or the 1VsAll approach \cite{ConvE}.

Many of these approaches try to find hard negative examples that are close to positive facts from a given \ac{KG} and thus have a positive effect on the embedding learning process.
Our approach goes one step further by generating Negative triples in a way that best contributes to the embedding learning process. 
It is based on Uncertainty Sampling which aims to
find the more informative negative examples for embedding learning so that an improvement in the learning process and resulting embedding is achieved.
These can be hard negative examples, but also other negative triples which are valuable for the embedding model.