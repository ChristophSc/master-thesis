\chapter{Related Work}
\label{ch:relatedwork}

To ensure a common understanding and avoid misunderstandings, terminologies are described, important definitions are given and related works are presented.

\section{Knowledge Graphs} 
There are several definitions of a \acp{KG} in the literature. 
In the scope of this work, we work with the following definition given by \cite{ConEx}:
Let the set of entities and relations represented by \entities and \relations.
Then, a \ac{KG} $\kg= \{\triple{h}{r}{t} \}  \subseteq \entities \times \relations \times \entities$ can be formalised as a set of triples where each triple contains a head entity $\texttt{h}$ and a tail entity $\texttt{t}$ with $\texttt{h}, \texttt{t} \in \entities$ and a relation $\texttt{r} \in \relations$. 
A relation \texttt{r} in \kg is
\begin{itemize}
    \item 
    \emph{symmetric} if $\triple{h}{r}{t} \iff \triple{t}{r}{h}$ for all pairs of entities $\texttt{h},\texttt{t}\in \entities$, 
   
   \item 
   \emph{anti-symmetric} if $\triple{h}{r}{t} \in \kg \Rightarrow \triple{t}{r}{h} \not \in \kg$ for all $\texttt{h} \not= \texttt{t}$, and
    
    \item 
    \emph{transitive}/\emph{composite} if $\triple{h}{r}{t}\in\kg \wedge \triple{t}{r}{y} \in \kg  \Rightarrow \triple{h}{r}{y} \in \kg$ for all $\texttt{h},\texttt{t},\texttt{y}\in \entities$.
\end{itemize}
In addition, the inverse of a relation \texttt{r}, denoted as $\texttt{r}^{-1}$, is a relation such that for any two entities $\texttt{h}$ and $\texttt{t}$, $\triple{h}{r}{t} \in \kg \iff (\texttt{t},\texttt{r}^{-1},\texttt{h}) \in \kg $.

\section{Knowledge Graph Embeddings} 
\acp{KGE} are low-dimensional representations of the entities and relations in a \ac{KG}. 
Numerous methods have been developed in the last few years, all of them define a different score function to measure the distance between entities relative to their relation.
Depending on the dimensionality of the embedding space and which types of relations should be embedded, a different embedding can be chosen.
While some of them support only symmetric relations, others support antisymmetric ones as well. 
Besides this, the types of relations in a \ac{KG} differ from each other.
We can differentiate between 1-to-1, 1-to-N, N-to-1, and N-to-N relations.
Overall, all the embedding methods can be separated by three different aspects \cite{electronics9050750}.
How they represent entities and relations, define the scoring function and optimize the ranking criterion
On the basis of this, two different categories are derived:
\textit{Triplet Fact-Based Representation Learning Models} and \textit{Description-Based Representation Learning Mode} \cite{electronics9050750}.

\textit{Triplet Fact-Based Representation Learning Models} are separated into three groups:
\begin{enumerate}
    \item 
    \textbf{Translation-Based Models}  which are based on word embedding algorithms. 
    Example models are e.g. \textsc{TransE}, \textsc{TransH}, \textsc{TransR}, \textsc{TransD} and \textsc{RotatE}
    
    \item 
    \textbf{Tensor Factorization-Based Models}
    \textsc{RESCAL}, \textsc{DistMult}, \textsc{ComplEx}, \textsc{HolE}
    
    \item 
    \textbf{Neural Network-Based Models}
    \textsc{ConvE}, \textsc{HypER}, \textsc{ConEx}, \textsc{ConvQ}, \textsc{ConvO}
\end{enumerate}
Most of these graph representation learning methods can be unified within a \ac{SampledNCE} framework comprising an encoder that generates node embeddings by learning to distinguish pairs of a positive and a negative triple \cite{MCNS}.

\section{Negative Sampling methods} 
In the literature, several Negative Sampling methods are proposed to create synthetic negative examples which are used for subsequent embedding learning.
They can be separated into three different groups:
\begin{itemize}
    \item 
    \textbf{Degree-based negative sampling} methods like Power of Degree \cite{MikolovSCCD13}, \ac{RNS}\cite{Dupre2018Word2vec} and \ac{WRMF} \cite{Hu2008Collaborative} and can be summarized as $p_n(v) \propto deg(v)^\beta$.
    The probability is given by e.g. weighting the different nodes which leads to a 
    static and inconsiderate to the personalization of nodes \cite{MCNS} because the probability of a node being samples is not changed later on.
    Additionaly, in the quality of negative samples generated by random sampling is  often poor \cite{cai2017kbgan}.
    
    \item 
    \textbf{Hard-samples Negative Sampling} methods try to find negative examples with high positive probabilities and the best negative samples are found by rejection \cite{MCNS}. 
    In comparison to the previously presented Negative Sampling group, 
    different negative samples are created and ranked by specified ranking functions.
    These ranks are created by e.g. \ac{BPR}, \ac{WARP} or a language model depending on how similar two given words are. 
    Within this group, approaches like\ac{PinSAGE} \cite{PinSAGE}, sampling-max (\ac{DNS}) \cite{DNS} and \ac{WARP} \cite{WARP} can be mentioned.
    
    \item 
    \textbf{\ac{GAN}-based Negative Sampling} was originally proposed for
    generating samples in a continuous space such as images \cite{cai2017kbgan}.
    In recent years it was used for negative sampling approaches which achieved state-of-the-art accuracies. 
    \ac{GAN}-based approaches consist of two components:
    a generator and a discriminator. 
    While the generator adaptively generates hard negative samples according to the reward from the discriminator, the discriminator itself learns to embed the \ac{KG} using the negative triples from the generator \cite{IGAN}.
    
    %\item 
    %MUST STILL BE ASSIGNED TO GROUPS:
    %\cite{alam2020affinity}: 
    %1) Typed Sampling
    %2) Relational Sampling
    %3) Distributional Negative Sampling
    %--> \ac{ADNS}
    %4) \ac{MCNS}
\end{itemize}

\section{Standard evaluations for knowledge graph embedding models} 

To evaluate the \ac{KGE} models, different metrics are available.
Some most commonly used are the following metrices \cite{kotnis2017analysis}:
\begin{itemize}
    \item 
    \ac{MRR} which is defined as
    $$MRR = \frac{1}{N} \sum_{i=1}^{N}\frac{1}{rank_i}$$
    computes the average of the reciprocal ranks \cite{zhang2021efficient}
    
    \item 
    hits@k which calculates the percentage of appearance in the top-10 ranking \cite{zhang2021efficient}.
    $$hits@K = \frac{|\{i | rank_i < K\}|}{N}$$
    where $rank_i$ is the rank of the positive instance $i$ predicted by a model with respect to the negative examples and usually $K \in \{1, 3, 5, 10\}$.
 
\end{itemize}


\section{Uncertainty Sampling}

Add description of active Learning and Uncertainty Sampling here instead of just Active Learning??\\
=> approach is more related to Uncertainty Sampling

\section{Active Learning}
\cite{Settles2009ActiveLL} describes in its survey the different approaches of Active Learning.
The idea of Active Learning is to achieve greater accuracy with fewer labeled training instance by choosing the data from which it learns.
It is well-motivated in many modern machine learning problems, where we have an abundant amount of unlabeled data, but it is difficult, time-consuming or expensive to obtain labeled data.
For Active Learning three different settings are differentiated:
\begin{enumerate}
    \item \textbf{Membership Query Synthesis}
    In this setting the learner usually generates queries de novo, rather than those sampled from an an underlying distribution. 
    For example, the active learner rotates a given picture which is sent to and classified by the oracle.
    
    \item \textbf{Stream-Based Selective Sampling}
    is an alternative to synthesizing queries and quieries samples my selective sampling with the key assumption that obtaining an unlabeled instance is unexpensive.
    it is also called stream-based or sequential active learning, since it is done in an iterative manner.
    
    \item \textbf{Pool-Based Selective Sampling} is based on the assumption that we have a large collection of unlabeled data which can be gathered at once.
    With this strategy, instances are queried according to an informativeness measure.
    It is closely related to Stream-Based Selective Sampling since the only difference is that the instances are queried and ranked at once and not iteratively.
\end{enumerate}



%- GAN-based approaches
%- discriminator takes the role as annotator, while generator provide negative %samples
%- the more difficult it is for discriminator to annotate a given triples as positive or negative, the higher the reward for the generator
%- models train each other
%- Baseline approaches: IGAN and KBGAN
%- randomly replace head or tail entity
%- NSCaching: 

%- infinite set of possible negative samples
%- random sampling does not generate informative negative examples, probably useless


%(move following part to related work?)
%    \begin{defn}[Negative Statements] \mbox{ }
%        \begin{enumerate}
%            \item 
%            A grounded negative statement $\neg (h, r, t)$ is satisfied if $(h, r, t) %\notin$ KG.
%            
%            \item 
%            A universally  negative statement $\neg\exists(h, r, \_)$ is satisfied if %there exists no $t$ such that $(h; r; t) \in KG$.
%        \end{enumerate} 
%    \end{defn}