\chapter{Related Work}
\label{ch:relatedwork}

\paragraph{Knowledge Graph Embeddings} 
are low-dimensional representations of the entities and relations in a \ac{KG}. 
Numerous methods have been developed in the last few years, all of them define a different score function to measure the distance between entities relative to their relation.
Depending on the dimensionality of the embedding space and which types of relations should be embedded, a different embedding can be chosen.
While some of them support only symmetric relations, others support antisymmetric ones as well. 
Besides this, the types of relations in a \ac{KG} differ from each other.
We can differentiate between 1-to-1, 1-to-N, N-to-1, and N-to-N relations.
Overall, all the embedding methods can be separated by three different aspects \cite{electronics9050750}.
How they represent entities and relations, define the scoring function and optimize the ranking criterion
On the basis of this, two different categories are derived:
\textit{Triplet Fact-Based Representation Learning Models} and \textit{Description-Based Representation Learning Mode} \cite{electronics9050750}.

\textit{Triplet Fact-Based Representation Learning Models} are separated into three groups:
\begin{enumerate}
    \item 
    \textit{Translation-Based Models}  which are based on word embedding algorithms. 
    Example models are e.g. TransE, TransH, TransR, and TransD, RotatE,
    
    \item 
    \textit{Tensor Factorization-Based Models}
    RESCAL, DistMult, ComplEx,HolE,
    
    \item 
    \textit{Neural Network-Based Models}
    ConvE, HypER, ConEx, ConvQ, ConvO
\end{enumerate}

\paragraph{Negative sampling methods}  

Most of these graph representation learning methods can be unified within a \ac{SampledNCE} framework comprising an encoder that generates node embeddings by learning to distinguish positives and negatives pairs \cite{MCNS}.
Groups of Negative sampling methods are:
\begin{itemize}
    \item 
    \textbf{Degree-based negative sampling} methods like Power of Degree \cite{MikolovSCCD13}, \ac{RNS}\cite{Dupre2018Word2vec} and \ac{WRMF} \cite{Hu2008Collaborative} and can be summarized as $p_n(v) \propto deg(v)^\beta$.
    The probability is given by e.g. weighting the different nodes which leads to a 
    static and inconsiderate to the personalization of nodes \cite{MCNS} because the probability of a node being samples is not changed later on.
    Additionaly, in the quality of negative samples generated by random sampling is  often poor \cite{cai2017kbgan}.
    
    \item 
    \textbf{Hard-samples Negative Sampling} methods try to find negative examples with high positive probabilities and the best negative samples are found by rejection \cite{MCNS}. 
    In comparison to the previously presented Negative Sampling group, 
    different negative samples are created and ranked by specified ranking functions.
    These ranks are created by e.g. \ac{BPR}, \ac{WARP} or a language model depending on how similar two given words are. 
    Within this group, approaches like\ac{PinSAGE} \cite{PinSAGE}, sampling-max (\ac{DNS}) \cite{DNS} and \ac{WARP} \cite{WARP} can be mentioned.
    
    \item 
    \textbf{\ac{GAN}-based Negative Sampling} was originally proposed for
    generating samples in a continuous space such as images \cite{cai2017kbgan}.
    In recent years it was used for negative sampling approaches which achieved state-of-the-art accuracies. 
    \ac{GAN}-based approaches consist of two components:
    a generator and a discriminator. 
    While the generator adaptively generates hard negative samples according to the reward from the discriminator, the discriminator itself learns to embed the \ac{KG} using the negative triplets from the generator \cite{wang2018incorporating}.
    
    
    %\item 
    %MUST STILL BE ASSIGNED TO GROUPS:
    %\cite{alam2020affinity}: 
    %1) Typed Sampling
    %2) Relational Sampling
    %3) Distributional Negative Sampling
    %--> \ac{ADNS}
    %4) \ac{MCNS}
\end{itemize}

\paragraph{\textbf{Standard evaluations for knowledge graph embedding models:}}  
To evaluate the \ac{KGE} models, different metrics are available.
Some most commonly used are the following metrices \cite{kotnis2017analysis}:
\begin{itemize}
    \item 
    \ac{MRR}
    $$MRR = \frac{1}{N} \sum_{i=1}^{N}\frac{1}{rank_i}$$
    \item 
    hits@k
    $$hits@K = \frac{|\{i | rank_i < K\}|}{N}$$
    where $rank_i$ is the rank of the positive instance i predicted by the
    model with respect to the negative samples and usually $k \in \{1,3,5,10\}$.
\end{itemize}

\paragraph{\textbf{Standard evaluations for knowledge graph embedding models:}}
The main idea of Active Learning is that learning algorithms are allowed to choose the data from which it learns and aims to perform better with less training.
It comprises two components: the active learner and a oracle/user.
While the active learner aims to achieve high accuracy, the oracle labels the instances received by the active learner.
With this approach, Active Learning usually attempts to overcome the labeling bottleneck to minimize the cost of obtaining labeled data.
Active Learning can be divided into three main settings \cite{Settles2009ActiveLL}:
\begin{itemize}
    \item 
    In \textbf{membership query synthesis} an active learner produces an example that it would like the oracle to label the instance.
    This requires the active learner to be able to capture the data distribution and to create reasonable instances.
    
    \item 
    Second active learning strategy  \textbf{pool-based active sampling} requires a large collection of unlabeled data.
    In comparison to stream-based selective sampling this setting evaluates and ranks the entire collection before selecting the best query.
    
    
    \item 
    The last strategy is \textbf{stream-based selective sampling} where obtaining unlabeled instances is inexpensive and the learner can decide whether or not to request its label.
    Typically, each unlabeled instance is drawn iteratively from the data source,
    With "informativeness measure", "query strategy" or from a region of uncertainty.
    
\end{itemize}







