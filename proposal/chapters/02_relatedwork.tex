\chapter{Related Work}
\label{ch:relatedwork}

\section{Knowledge Graphs} 
There are several definitions of a \acp{KG} in the literature. 
In the scope of this work, we work with the following definition given by \cite{ConEx, RotatE}:
Let the set of entities and relations represented by \entities and \relations.
Then, a \ac{KG} $\kg= \{\triple{h}{r}{t} \}  \subseteq \entities \times \relations \times \entities$ can be formalised as a set of triples where each triple contains a head entity $\texttt{h}$ and a tail entity $\texttt{t}$ with $\texttt{h}, \texttt{t} \in \entities$ and a relation $\texttt{r} \in \relations$. 
A relation \texttt{r} in \kg is
\begin{itemize}
    \item 
    \emph{symmetric} if $\triple{h}{r}{t} \iff \triple{t}{r}{h}$ for all pairs of entities $\texttt{h},\texttt{t}\in \entities$, 
   
   \item 
   \emph{anti-symmetric} if $\triple{h}{r}{t} \in \kg \Rightarrow \triple{t}{r}{h} \not \in \kg$ for all $\texttt{h} \not= \texttt{t}$, and
    
    \item 
    \emph{transitive}/\emph{composite} if $\triple{h}{r}{t}\in\kg \wedge \triple{t}{r}{y} \in \kg  \Rightarrow \triple{h}{r}{y} \in \kg$ for all $\texttt{h},\texttt{t},\texttt{y}\in \entities$.
\end{itemize}
In addition, the inverse of a relation \texttt{r}, denoted as $\texttt{r}^{-1}$, is a relation such that for any two entities $\texttt{h}$ and $\texttt{t}$, $\triple{h}{r}{t} \in \kg \iff (\texttt{t},\texttt{r}^{-1},\texttt{h}) \in \kg $.

\section{Knowledge Graph Embeddings} 
\acp{KGE} are low-dimensional representations of the entities and relations in a \ac{KG}. 
Numerous methods have been developed in the last few years to tackle various problems such as defining a different score function to measure the distance between entities relative to their relation.
Depending on the dimensionality of the embedding space and which types of relations should be embedded, a different embedding can be chosen.
While some of them support only symmetric relations, others support antisymmetric ones as well. 
Besides this, the types of relations in a \ac{KG} differ from each other.
We can differentiate between 1-to-1, 1-to-N, N-to-1, and N-to-N relations.
Overall, all the embedding methods can be separated by three different aspects \cite{electronics9050750}.
How they represent entities and relations, define the scoring function and optimize the ranking criterion
On the basis of this, two different categories are derived:
\textit{Triplet Fact-Based Representation Learning Models} and \textit{Description-Based Representation Learning Mode} \cite{electronics9050750}.

\textit{Triplet Fact-Based Representation Learning Models} are separated into three groups:
\begin{enumerate}
    \item 
    \textbf{Translation-Based Models}  which are based on word embedding algorithms: 
    \textsc{TransE} \cite{TransE}, \textsc{TransH} \cite{TransH}, \textsc{TransR} \cite{TransR}, \textsc{TransD} \cite{TransD}
    and 
    \textsc{RotatE} \cite{RotatE}
    
    \item 
    \textbf{Tensor Factorization-Based Models}:
    \textsc{RESCAL} \cite{RESCAL}, \textsc{DistMult} \cite{DistMult}, \textsc{ComplEx} \cite{ComplEx}, \textsc{HolE} \cite{HolE}
    
    \item 
    \textbf{Neural Network-Based Models}:
    \textsc{ConvE} \cite{ConvE}, \textsc{HypER} \cite{HypER}, \textsc{ConEx} \cite{ConEx}, \textsc{ConvQ} and  \textsc{ConvO} \cite{demir2021convolutional}
\end{enumerate}


\section{Negative Sampling methods} 
In the literature, several Negative Sampling methods are proposed to create synthetic negative examples which are used for subsequent embedding learning.
They can be separated into three different groups \cite{qianunderstanding}:
\begin{itemize}
    \item 
    \textbf{Static Distribution-Based Sampling} include methods like Uniform, Bernoulli and Probabilistic Sampling.
	While in Uniform Sampling either the head or tail entity is replaced by an entity randomly sampled from entity set \entities,
	Bernoulli Sampling uses different probabilities for replacing head or tail entity depending on the underlying relation type.
	In contrast, Probabilistic Sampling speeds up the training process by including a train bias.
	
	\item 
    \textbf{Custom Cluster-Based Sampling} samples negative triples from a small cluster based on closeness between entities.
	By the K-Means clustering algorithm or caching techniques, approaches like TransE-\ac{SNS} \cite{TransE-SNS} or \ac{NSCaching} \cite{NSCaching}, entities are divided into a number of groups and randomly sampled from
	this group to create negative triples.	
    
    \item 
    \textbf{Dynamic Distribution-Based Sampling} tries to model the changes in negative sampling distribution by using a \ac{GAN}-based negative sampling
	framework which includes two components.
	While the generator dynamically approximates the constantly updated negative sampling distribution to provide high-qualitative negative triples, 
	the discriminator learns to distinguish positive and negative triples with its own \ac{KGE} model.
	Known approaches are \ac{KBGAN} \cite{cai2017kbgan}, \ac{IGAN}  \cite{IGAN} or \ac{MCNS} \cite{MCNS}.
\end{itemize}

\section{Standard evaluations for knowledge graph embedding models} 
To evaluate the \ac{KGE} models, different metrics are available.
Some most commonly used are the following metrices \cite{kotnis2017analysis}:
\begin{itemize}
    \item 
    \ac{MRR} which is defined as
    \begin{equation}
        MRR = \frac{1}{N} \sum_{i=1}^{N}\frac{1}{rank_i}
    \end{equation}
    
    computes the average of the reciprocal ranks \cite{zhang2021efficient}
    
    \item 
    hits@k which calculates the percentage of appearance in the top-10 ranking \cite{zhang2021efficient}.
    \begin{equation}
        hits@K = \frac{|\{i | rank_i < K\}|}{N}
    \end{equation}
    where $rank_i$ is the rank of the positive instance $i$ predicted by a model with respect to the negative examples and usually $K \in \{1, 3, 5, 10\}$.
\end{itemize}
Since in many \ac{KGE} models either the head or the tail entity is randomly replaced by other entities of the KG, these corrupted triples may be true facts.
For this reason, many approaches distinguish between \textit{raw} and \textit{filtered}, where for corrupted triples it is looked whether these occur in the train, test or validation set \cite{TransE}.

\section{Uncertainty Sampling}
Uncertainty Sampling originally comes from Active Learning and is well-motivated in many machine learning approaches where unlabeled data is abundant, but it is difficult, time-consuming, or expensive to obtain labeled data \cite{Settles2009ActiveLL}.
With Uncertainty Sampling in Active Learning, we aim for greater accuracy with fewer labeled training instances by choosing the data from which we learn from \cite{Settles2009ActiveLL}.
Uncertainty sampling methods can be subdivided into three different frameworks, which have several variants.
While the first one has been specifically developed for the purpose of active learning, the others are more general approaches for machine learning \cite{nguyen2021howtomeasure}.
\begin{enumerate}
    \item 
    \textbf{Evidence-based uncertainty (EBU)}
    The \ac{EBU} approach differentiates between uncertainty due to conflicting evidence and insufficient evidence.
    Variants of \ac{EBU} are \ac{CEU} and \ac{IEU}

    \item 
    \textbf{Credal uncertainty (CU):}
    \ac{CU} seeks to differentiate between the reducible and irreducible part of the uncertainty in a prediction.
    This approach is searching for the instance x with the highest uncertainty, i.e, the least evidence for the dominance of one of the classes \cite{nguyen2021howtomeasure}.

    \item 
    \textbf{Epistemic and aleatoric uncertainty (EAU):}
    In \ac{EAU} there is a distinction between the epistemic and aleatoric uncertainty. This approach is based on the use of relative likelihoods and justified in other settings such as possibility theory. 
    Measures are \ac{EU}, \ac{AU} and the standard \ac{ENT}, 
\end{enumerate}



% In the case of binary classifcation, i.e, Y = {0, 1}, all these measures rank unlabelled instances in the same order and look for instances with small diference



%\section{Active Learning}
%\cite{Settles2009ActiveLL} investigates different approaches of Active Learning for several scenarios with different query strategy frameworks.
%The idea of Active Learning is to achieve greater accuracy with fewer labeled training instance by choosing the data from which it learns.
%It is well-motivated in many modern machine learning problems, where we have an abundant amount of unlabeled data, but it is difficult, time-consuming or expensive to obtain labeled data.
%For Active Learning three different settings are differentiated:
%\begin{enumerate}
%    \item \textbf{Membership Query Synthesis}
%    In this setting the learner usually generates queries de novo, rather than those sampled from an an underlying distribution. 
%    For example, the active learner rotates a given picture which is sent to and classified by the oracle.
    
%    \item \textbf{Stream-Based Selective Sampling}
%    is an alternative to synthesizing queries and quieries samples my selective sampling with the key assumption that obtaining an unlabeled instance is unexpensive.
%    it is also called stream-based or sequential active learning, since it is done in an iterative manner.
    
%    \item \textbf{Pool-Based Selective Sampling} is based on the assumption that we have a large collection of unlabeled data which can be gathered at once.
%    With this strategy, instances are queried according to an informativeness measure.
%    It is closely related to Stream-Based Selective Sampling since the only difference is that the instances are queried and ranked at once and not iteratively.
%\end{enumerate}



%- GAN-based approaches
%- discriminator takes the role as annotator, while generator provide negative %samples
%- the more difficult it is for discriminator to annotate a given triples as positive or negative, the higher the reward for the generator
%- models train each other
%- Baseline approaches: IGAN and KBGAN
%- randomly replace head or tail entity
%- NSCaching: 

%- infinite set of possible negative samples
%- random sampling does not generate informative negative examples, probably useless


%(move following part to related work?)
%    \begin{defn}[Negative Statements] \mbox{ }
%        \begin{enumerate}
%            \item 
%            A grounded negative statement $\neg (h, r, t)$ is satisfied if $(h, r, t) %\notin$ KG.
%            
%            \item 
%            A universally  negative statement $\neg\exists(h, r, \_)$ is satisfied if %there exists no $t$ such that $(h; r; t) \in KG$.
%        \end{enumerate} 
%    \end{defn}