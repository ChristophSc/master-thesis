\chapter{Solution}
\label{ch:solution}

% description and general introduction to solution
Our approach of hard negative sampling for better \ac{KGE} learning is based on \ac{KBGAN} \cite{cai2017kbgan}.
There are two different ways to improve the approach, which can be traced back to the instability and degeneracy due to high variance and the non-guaranteed creation of large-gradient negative triples.
First, this can be achieved by improving the general quality of the negative examples in subset $Neg$.
Second, the learning process can be optimized by learning faster with less sampling.

Our approach optimizes the learning process by a smarter sampling technique and is inspired by Uncertainty Sampling in Active Learning.
Active Learning is originally well-motivated in many machine learning approaches where unlabeled data is abundant, but it is difficult, time-consuming, or expensive to obtain labeled data \cite{Settles2009ActiveLL}.
By Active Learning, we aim for greater accuracy with fewer labeled training instances by choosing the data from which we learn from \cite{Settles2009ActiveLL}.
In our case, we want to generate hard negatives to obtain a good \ac{KGE} and achieve higher accuracies in the subsequent tasks. 
By integrating Uncertainty Sampling into the \ac{KBGAN}-based approach, particularly interesting triples are sampled, i.e. those that are difficult to classify as positive or negative.
In Uncertainty Sampling, an active learner queries the instances where it is most uncertain about how to label it \cite{Settles2009ActiveLL}.
If we use entropy-based Uncertainty Sampling in a binary classification problem like in our case, it is identical to choose the instance whose posterior probability of being positive is nearest to 0.5 \cite{Settles2009ActiveLL}.
Entropy is defined as \cite{Settles2009ActiveLL}:
$$x^{*}_{ENT} = \argmax_{x} - \sum_{i}{\mathds{P}(y_i | x; \theta) log \mathds{P}(y_i|x; \theta)}$$
In our scenario of negative sampling, since we have a binary classification between negative ($y=0$) and positive ($y=1$) triples, $<$ 0.5 represents a negative example and $\geq$ 0.5 a positive example.
The result is that we query examples that are close to 0.5, or in other words: we query hard negatives which are close to being positive.

The structure of our approach is illustrated in Figure \ref{fig:architecture}.
It can be seen that Uncertainty Sampling replaces the original sampling process by probability distribution.
\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/architecture.png}
    \caption{Structure of our negative sampling approach including two components:
    A generator G which samples negative triples by Uncertainty Sampling and a discriminator D which scores the difference of positive triple $(h,r,t)$ and negative triple $(h',r,t')$ by given embedding score function $f_D$}
    \label{fig:architecture}
\end{figure}
Since we are working with probabilities $\mathds{P}(y_i | x; \theta)$ for the classes $y_i \in \{0, 1\}$ in entropy based Uncertainty Sampling and we want to incorporate more information into the score function, we replace the original embedding function $f_G$ of the generator G in \ac{KBGAN} with the new score function $Score$, which is defined as:
\begin{equation*}
    Score(h, r, t)=
    \begin{cases}
         \lambda_1 \text{PEER(\textit{h, r, t})} + \lambda_2 \text{POP(\textit{t})} + \lambda_3 \text{PIVO(\textit{h, t})} + \lambda_4 f_G(\textit{h, r, t})
         \\ \ \ 
         if\ \ \ \neg(\textit{h, r, t})
         \\ \\
         \lambda_1 \text{PEER(\textit{h, r, t})} + \lambda_4 \text{FRQ(\textit{r})} + \lambda_3 \text{PIVO(\textit{h, t})} + \lambda_4 f_G(h, r, t)
         \\ \ \ 
         if\ \ \ \neg \exists (\textit{h, r, \_})
         \\
    \end{cases}
\end{equation*}
where  $\neg (h, r, t)$ is a \textit{grounded negative statement} and satisfied if $(h, r, t) \notin$ KG and $\neg\exists(h, r, \_)$ is a \textit{universally negative statement} which is satisfied if there exists no $t$ such that $(h, r, t) \in KG$ \cite{arnaout2020enriching}.\\
The score contains the following features from \cite{arnaout2020enriching} for peer groups and provides additional information about the relations and entities of the triples (move this part to Related work?):
\begin{itemize}
    \item 
    \emph{Peer frequency (PEER):} 
    Relative frequency of peers within a peer group that is related to different objects, e.g. 0.9 of persons are married. 

    \item
    \emph{Object popularity (POP):} 
    The popularity of the tail entity $t$ in the \ac{KG}, so how often it appears in a \ac{KG}.
    
    \item 
    \emph{Frequency of the Property (FRQ):} 
    Frequency of a relation/predicate $r$ in triples of the \ac{KG}. 
    
    \item 
    \emph{Pivoting likelihood (PIVO):} 
    Textual background information about an entities $h$ and $t$.
    
    \item 
    \emph{$f_G$:} 
    $Score$ function of the \ac{KGE}  model like in the original \ac{KBGAN} approach (e.g. \textsc{DistMult}  or \textsc{ComplEx}).
\end{itemize}

The training process can be described in the following steps:
\begin{enumerate}
    \item  Calculate maximum score of all positive triples to have an upper bound which is later considered to be a probability of 1.
    $$score_{max} := \argmax_{(h,r,t) \in \mathcal{T}_{batch}}{Score(h,r,t)}$$

    \item 
    Like in original \ac{KBGAN} approach, create a set of negative triples by corrupting $N_s$ positive triples $\{(h_i,r,t_i)\}_{i=1\dots N_s}$ by replacing head $h$ or tail entity $t$ by uniformly random sampling over all entities $e \in \entities$ to obtain $Neg(h,r,t)=\{(h_i',r,t_i')\}_{i=1\dots N_s}$.
    
    \item 
    Calculate $Score$ of each triple $(h',r,t') \in Neg$, while $Score$-function is defined as 
    
    \item 
    Calculate the minimum score to obtain the lower bound for the probability of a triple to be negative. 
    $$score_{min} := \argmin_{(h', r, t') \in Neg}{Score(h', r, t'))}$$
    
    \item 
    To sample the negative triple $(h',r,t')^{*}_{ENT}$, we need to calculate the entropy-based uncertainty, which is defined as
    $$(h',r,t')^{*}_{ENT} = \argmax_{(h',r,t') \in Neg)} - \sum_{i}{\mathds{P}(y_i | (h',r,t')) log \mathds{P}(y_i|(h',r,t'))}$$
    Since we have a binary classification with $y_i \in \{0,1\}$:
    $$= \argmax_{(h',r,t') \in Neg)} - \mathds{P}(y = 1| (h',r,t')) log \mathds{P}(y = 1|(h',r,t')) - \mathds{P}(y = 0| (h',r,t')) log \mathds{P}(y = 0|(h',r,t'))$$
    $$= \argmax_{(h',r,t') \in Neg)} - \mathds{P}(y = 1| (h',r,t')) log \mathds{P}(y = 1|(h',r,t')) - (1 - \mathds{P}(y = 1|(h',r,t') log(1 - \mathds{P}(y = 1|(h',r,t'))$$
    To obtain the probabilities for a triple of being positive ($y=1$) as we define
    $$\mathds{P}(y = 1|(h, r, t)) = \frac{score(h, r, t) - score_{min}}{score_{max} - score_{min}} \in [0, 1]$$
    and accordingly the probability of a triple to be negative ($y=0$) as
    $$\mathds{P}(y = 0|(h, r, t)) = 1 - \mathds{P}(y = 1|(h, r, t)) \in [0,1]$$
    
    \item 
    From this step on, everything is the same as the original \ac{KBGAN} approach:
    The generated negative triple $(h',r,t')$ as well as the positive triple $(h, r, t)$ are sent to the discriminator.
    
    \item 
    The discriminator distinguishes both triples by given score function $f_D$, which is usually a translation-based \ac{KGE} model like \textsc{TransE} or \textsc{TransD}.
    
    \item 
    The reward defined by $r = - f_D(h',r,t')$ of the current triple pair is calculated and added to the reward sum $r_{sum}$.
    
\end{enumerate}
These adversarial training process steps are repeated until convergence, such that the generator improves the quality of sampled negatives and discriminator improves embedding over time.. 
The individual steps of the training process are summarized in the Algorithm \ref{alg:ucgan}, which also illustrates the changes to the original \ac{KBGAN} approach.
\input{chapters/algorithm}


% our approach tries to optimize the sampling process by smarter sampling technique
% explain Active Learning + Uncertainty sampling
% describe training process by referring to architecture figure and pseudo code algorithm
% mention all formulas, their definition and where they originally come from,
% explain the expected consequences of the approach
% 



For the implementation of our prototype for our approach, we will use rather small datasets and start with \textsc{KINSHIP} and \textsc{UMLS}.
Subsequently, we want to compare our achieved accuracy with state-of-the-art approaches.
Therefore, we want to use \ac{MRR} and Hit@10 metrics on datasets \textsc{WN18}, \textsc{WN18RR}, \textsc{FB15K}, \textsc{FB15K237}, \textsc{YAGO3-10}.
Additionally, we will test the impact of our negative sampling approach on different embeddings with the given datasets and compare in which areas our method outperforms the current state-of-the-art approaches.
For this reason, we will use the most common embeddings from the three groups of embedding types:
\textsc{TransE}, \textsc{TransH} for \textit{Translation-Based Models}, \textsc{DistMult} and \textsc{ComplEx} for \textit{Tensor Factorization-Based Models}, and \textsc{ConvE} and \textsc{ConEx} for \textit{Neural Network-Based Models}.




% ---------------------------- OLD ----------------------------  

%These interesting and useful negative samples are subsequently used to learn the embedding.
%As described in the related work chapter, we have three different settings available for active learning:
%Since we do not know anything about the distribution of negative samples at the beginning, and we have a large pool of negative samples, it would be very time-consuming to rank each instance.
%For this reason, \textit{Pool-Based Active Learning} is the least appropriate for our scenario.
%So the decision if we chose \textit{Membership Query Synthesis} or \textit{Stream-Based Selective Sampling} depends on if we already have a negative sample pool or we have to create them de novo.
%For the former scenario we can select positive samples from the \ac{KG} and generate a negative sample by replacing the head or tail entity.
%With this approach the active learner would learn how to replace entities to create useful and informative negative samples.
%The latter setting of \textit{Stream-Based Selective Sampling} would already require an existing large pool of negative samples.
%This could be done by replacing head or tail entity randomly by any other entity of the \ac{KG} for a set of chosen positive samples.
%Since the setting of \textit{Membership Query Synthesis} seems to be less time-consuming and less-memory intense, this one meets our requirements more and, therefore, is used for our scenario.

%Each of the active learning settings evaluate the informativeness of unlabeled
%%instances, which is in our case the informativeness of negative samples \cite{Settles2009ActiveLL}.
%In the literature, several ways of formulating query strategies have been proposed, which results in the following general frameworks:
%\begin{itemize}
%    \item Uncertainty Sampling
%    \item Query-By-Committee
%    \item Expected Model Change
%    \item Variance Reduction and Fisher Information Ratio
%    \item Estimated Error Reduction
%    \item Density-Weighted Methods
%\end{itemize}



%The second component of an annotator is our \textbf{oracle}. 
%Since human annotations are too time-consuming and inefficient, our annotator is represented by an oracle, which is a machine learning model.
%The job of the oracle is to distinguish negative samples from our sampler from positive triples from \ac{KG}. 
%Therefore, we have a binary classification model which labels triples 0 or 1, where 0 represents a negative and 1 a positive triple.
%Compared to the normal oracle in an active learning process, in our negative sampling process we know the ground-truth value of our triples, as they are either from the KG or generated by the negative sampler.
%We can use this knowledge to give the oracle not only the task of labeling but also feedback to the negative sampler.
%The feedback should be based on various factors, that take into account different criteria for the current generated, negative sample and evaluate it using a combined rank metric.
%For this, we have the following factors in mind, which are combined into one metric:
%\begin{enumerate}
%    \item ranking with gradients - candidates which most surprise the language model
%    \item PEER - relative frequency of relations that peers of an entity have
%    \item POP - popularity of entity
%    \item FRQ - frequency of same subject and predicate (s,p, \_)
%    \item (PIVO - textual background information)
%    \item (ranking with scores - candidates which look more plausible, almost positive)
%\end{enumerate}
%Based on the feedback of the oracle, our active learner either retrained by replacing head or tail with a different entity, or a new positive sample can be retrieved from the \ac{KG} for new training.

% DESCRIBE PROCESS OF SAMPLING in several steps


%Like in other approaches, we can replace both the head and the tail entity with any other entity of the KG to obtain a negative triple.
%This leads to an abundant amount of negative samples at our disposal, but we are not certain about their quality in sense of informativeness and usefulness.
%For this reason, we want our approach to filter out exactly those negative samples from the large pool that represent the greatest possible benefit for the creation of embedding.

%Our proposed negative sampling method contains two components:
%The negative sampler and an oracle giving feedback to the sampler.
%sampler gets two inputs:
% interesting positive triplets from the \ac{KG} 
% feedback from the oracle for the quality of the negative example generated before.
 

%The oracle in turn just receives generated negative examples from the sampler. 
%The evaluation of the quality of the negative sample depends on two factors:
%At first, it measures the probability of the negative example to be a positive one, 
%which indicates the closeness to the positive triple. 
%Secondly, it measures the uncertainty of the \ac{KGE} in this area of a \ac{KG}. 
%For example, if a \ac{KG} is very incomplete or there is less information about this area, the \ac{KGE} is probably uncertain on how to embed these triples and differentiate between positives and negatives.
%Depending on the overall score for the generated negative example, the oracle gives feedback to the sampler.
%Either the sample was not good and has to be retrained, or a new positive sample can be retrieved from the \ac{KG} for new training.
%As a result, the sampler alternates between re-training and selecting new examples in each iteration.




%p(i) = \frac{p_{sampled}(i) \cdot p_{negative}(i)}{\sum_j{p_{sampled}(j) \cdot p_{negative}(j)}}$



%\begin{multline}
%    p_G(h',r,t'|h,r,t)=\frac{\exp score(h',r,t')}{\sum\exp score(h^*,r,t^*)} \\
%    (h^*,r,t^*)\in Neg(h,r,t)
%\end{multline}