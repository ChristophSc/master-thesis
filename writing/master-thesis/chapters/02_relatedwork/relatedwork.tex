\chapter{Related Work}
\label{ch:relatedwork}

\section{Knowledge Graphs} 
There are several definitions of a \acp{KG} in the literature. 
In the scope of this work, we work with the following definition given by \cite{ConEx, RotatE}:
Let the set of entities and relations represented by \entities and \relations.
Then, a \ac{KG} $\kg= \{\triple{h}{r}{t} \}  \subseteq \entities \times \relations \times \entities$ can be formalised as a set of triples where each triple contains a head entity $\texttt{h}$ and a tail entity $\texttt{t}$ with $\texttt{h}, \texttt{t} \in \entities$ and a relation $\texttt{r} \in \relations$ which can be
\begin{itemize}
    \item 
    \emph{symmetric} if $\triple{h}{r}{t} \iff \triple{t}{r}{h}$ for all pairs of entities $\texttt{h},\texttt{t}\in \entities$, 
   
   \item 
   \emph{anti-symmetric} if $\triple{h}{r}{t} \in \kg \Rightarrow \triple{t}{r}{h} \not \in \kg$ for all $\texttt{h} \not= \texttt{t}$, and
    
    \item 
    \emph{transitive}/\emph{composite} if $\triple{h}{r}{t}\in\kg \wedge \triple{t}{r}{y} \in \kg  \Rightarrow \triple{h}{r}{y} \in \kg$ for all $\texttt{h},\texttt{t},\texttt{y}\in \entities$.
\end{itemize}
In addition, the inverse of a relation \texttt{r}, denoted as $\texttt{r}^{-1}$, is a relation such that for any two entities $\texttt{h}$ and \texttt{t}, $\triple{h}{r}{t} \in \kg \iff (\texttt{t},\texttt{r}^{-1},\texttt{h}) \in \kg $.






\section{Knowledge Graph Embeddings} 
\acp{KGE} are low-dimensional representations of entities and relations in a \ac{KG}. 
Numerous methods have been developed in the last few years to tackle various problems such as defining a different score function to measure the distance between entities relative to their relation.
Depending on the dimensionality of the embedding space and which types of relations should be embedded, a different embedding can be chosen.
While some of them support only symmetric relations, others support antisymmetric ones as well. 
Besides this, the types of relations in a \ac{KG} differ from each other.
We can differentiate between 1-to-1, 1-to-N, N-to-1, and N-to-N relations.
Overall, all the embedding methods can be separated by three different aspects \cite{electronics9050750}:
(i) How they represent entities and relations, (ii) how they define the scoring function and (iii) how they optimize the ranking criterion.
On the basis of this, two different categories are derived:
\textit{Triplet Fact-Based Representation Learning Models} and \textit{Description-Based Representation Learning Models} \cite{electronics9050750}.

\textit{Triplet Fact-Based Representation Learning Models} are separated into three groups:
\begin{enumerate}
    \item 
    \textbf{Translation-Based Models}  which are based on word embedding algorithms: 
    \textsc{TransE} \cite{TransE}, \textsc{TransH} \cite{TransH}, \textsc{TransR} \cite{TransR}, \textsc{TransD} \cite{TransD}
    and 
    \textsc{RotatE} \cite{RotatE}
    The basic idea of \transe is that a functional relation $r$ corresponds to a translation of the embeddings, i.e. $h + r \thickapprox t$ when (h,r,t) holds.
    If it does not hold, $t$ should be far away from $h + r$ which lead to a energy-based framework $d(h+r, t)$ for some dissimilarity measure $d$ ($L_1$ or $L_2$ norm) \cite{TransE}.
    The idea of \transe is depicted in \autoref{fig:translationbasedmodels} (a).
    \begin{figure*}[t]
      \centering
        \includegraphics[width=0.95\textwidth]{figures/TransE+TransD.png}
      \caption{(a) TransE (d) TransD (based on: \cite{cai2017kbgan})}
      \label{fig:translationbasedmodels}
    \end{figure*}
    Therefore, head entity $h$, tail entity $t$ and relation $r$ are in the same vector space and the model tries to create the embeddings so that $h+r$ map to $t$ as closely as possible, which leads to the scoring function
    \begin{equation}
        f_r(h,t) = || h + r - t ||_{l_1, l_2}
        \label{eq:transescoringfunction}
    \end{equation}
    \transd on the other hand works with two vectors for each entity and relation.
    The first one captures the meaning and the second one is used to construct the mapping matrices \cite{TransD}.

    \item 
    \textbf{Tensor Factorization-Based Models}:
    \textsc{RESCAL} \cite{RESCAL}, \textsc{DistMult} \cite{DistMult}, \textsc{ComplEx} \cite{ComplEx}, \textsc{HolE} \cite{HolE}

    
    \distmult \cite{DistMult} simplifies the computational complexity of \rescal and restricts matrix $M_r$ to be a diagonal matrices, i.e. $M_r = diag(r), r \in \mathbb{R}^d$ \cite{electronics9050750}. 
    Therefore, its scoring function is transformed to
    \begin{equation}
        f_r(h,r) = h^{\top}diag(r)t\label{eq:distmultscoringfunction}
    \end{equation}
    Since \distmult is not able to model asymmetric relations, \complex \cite{ComplEx} extends \distmult by complex-valued embeddings \cite{electronics9050750} which leads to the scoring function
    \begin{equation}
        f_r(h,r) = Re(h^{\top}diag(r)\bar{t})
        \label{eq:complexscoringfunction}
    \end{equation}
    where $Re(\dot)$ denotes the real part of a complex value and $\bar{t}$ is the complex conjugate of t.
    
    
    \item 
    \textbf{Neural Network-Based Models}: 
    \textsc{ConvE} \cite{ConvE}, \textsc{HypER} \cite{HypER}, \textsc{ConEx} \cite{ConEx}, \textsc{ConvQ} and  \textsc{ConvO} \cite{demir2021convolutional}
\end{enumerate}
\textit{Description-Based Representation Learning Models} can also be broken down into three further categories:
\begin{enumerate}
    \item 
    \textbf{Textual Description-Based Models} is an extension of the traditional triplet-based model and integrates additional textual information for entities to evolve its performance.
    Examples are \ac{TKRL} \cite{TKRL} and \ac{TEKE} \cite{TEKE}.
    
    \item 
    \textbf{Relation Path-Based Models} refines the performance of an embedding model by multi-step relational paths, which reveal semantic relations between the entities.
    One example is \ac{PTransE} \cite{PTransE}.
    
     \item 
    \textbf{Other Models} that cannot be assigned to the other two categories and pay attention to further triple information such as temporal aspects.
\end{enumerate}






\section{Training Objective Types}
As described in \cite{cai2017kbgan} several Training Objective Types for \ac{KGE} models exist.
Every model defines a \textit{score function} $f(h,r,t)$ which assigns a score to every possible triple in the knowledge graph.
The estimated likelihood of a triple to be true depends only on its score given by the score function.
Since different \ac{KGE} models formulate different designs for this scoring function, the interpretation of this score is also different which in turn leads to different training objectives.
The two most common forms are \textbf{Marginal Loss function} and \textbf{Log-softmax function}:
\begin{enumerate}
    \item \textbf{Marginal loss function} is mostly used by translation-based models like \textsc{TransE} and \textsc{Trans}.
    Since these models work with distances and a smaller distance indicates a higher likelihood of truth.
    It is defined as 
    \begin{equation}
        L_{m}=\sum_{(h,r,t)\in\mathcal{T}}[f(h,r,t)-f(h',r,t')+\gamma]_+\label{eq:marginalloss}
    \end{equation}
    where $\gamma$ is the margin, $[\cdot]_+=\max(0,\cdot)$ is the hinge function, and $(h',r,t')\in\{(h',r,t)|h'\in\mathcal{E}\}\cup\{(h,r,t')|t'\in\mathcal{E}\}$ is a negative triple.
    
    \item \textbf{Log-softmax loss function} is commonly used for models which have a probabilistic interpretation for score function like \distmult or \complex.
    It is defined as
    \begin{multline}
        L_{l}=\sum_{(h,r,t)\in\mathcal{T}}-\log \frac{\exp f(h,r,t)}{\sum\exp f(h',r,t')}\\
        (h',r,t')\in\{(h,r,t)\}\cup Neg(h,r,t)\label{eq:nllloss}
    \end{multline}
    where $Neg(h,r,t)\subset\{(h',r,t)|h'\in\mathcal{E}\}\cup\{(h,r,t')|t'\in\mathcal{E}\}$ is a set of sampled corrupted triples.
    Therefore, it provides a probability distribution over a set of triples where each triple $(h, r, t)$ has probability $p(h,r,t)=\frac{\exp f(h,r,t)}{\sum_{(h',r,t')}\exp f(h',r,t')}$.
    In the original \kbgan approach this distribution is provided by the generator from which one negative triple is sampled and given to the discriminator.

    \item \textbf{Other forms} like a triple-wise logistic function \cite{ConvE} exist, but the two other ones are the most common used.
\end{enumerate}




\section{Training techniques}
\begin{enumerate}
    \item 1vsAll 
    
    \item Negative Sampling 
    
\end{enumerate}



\section{Negative Sampling Methods} \label{sec:negativesamplingmethods}
In the literature, several negative sampling methods are proposed to create synthetic negative examples which are used for subsequent embedding learning.
They can be separated into three different groups \cite{qianunderstanding}:
\begin{itemize}
    \item 
    \textbf{Static Distribution-Based Sampling} include methods like the uniform, Bernoulli and probabilistic sampling technique from a fixed and static distribution of negative triples.
	While in the uniform sampling either the head or tail entity is replaced by an entity randomly sampled from entity set \entities,
	the Bernoulli sampling technique uses different probabilities for replacing head or tail entity depending on the underlying relation type.
	In contrast, probabilistic sampling speeds up the training process by including a train bias.
	
	\item 
    \textbf{Custom Cluster-Based Sampling} samples negative triples from small clusters which are based on closeness between entities.
    Instead of sampling from the whole set of entities, they a are divided into a number of groups and randomly sampled to create negative triples. Examples are TransE-\ac{SNS} \cite{TransE-SNS} or \ac{NSCaching} \cite{zhang2019nscaching} which are based on K-Means clustering algorithm or caching techniques. 
    
    \item 
    \textbf{Dynamic Distribution-Based Sampling} tries to model the changes the distribution of negative triples by using a \ac{GAN}-based framework which includes two components: A generator and a discriminator.
	While the generator dynamically approximates the constantly updated Negative Sampling distribution to provide high-qualitative negative triples, 
	the discriminator learns to distinguish positive and negative triples with its own \ac{KGE} model.
	Known approaches are \ac{KBGAN} \cite{cai2017kbgan}, \ac{IGAN}  \cite{IGAN} or \ac{MCNS} \cite{MCNS}.
\end{itemize}


\section{KBGAN} \label{sec:kbgan}
In the group of \textit{Dynamic Distribution-Based Sampling}, \ac{KBGAN} is one of the pioneering works among \ac{GAN}-based approaches and contains two components which are trained in an adversarial training process.
This process of \ac{KBGAN} is depicted in \autoref{fig:overview} and described as follows:
\begin{figure*}[t]
  \centering
    \includegraphics[width=0.95\textwidth]{figures/kbgan_original.png}
  \caption{An overview of the \textsc{kbgan} framework (based on: \cite{cai2017kbgan})}
  \label{fig:overview}
\end{figure*}

\begin{enumerate}
    \item 
    At first, a set of sampled corrupted triples $Neg(h,r,t)\subset\{(h',r,t)|h'\in\mathcal{E}\}\cup\{(h,r,t')|t'\in\mathcal{E}\}$ is created by uniformly Sampling of $N_s$ entities $\in$ \entities to replace $h$ or $t$.
    For reasons of efficiency and to reduce the likelihood of creating false negatives, $N_s$ is a relatively small number compared to the number of all possible negative examples \cite{cai2017kbgan}.
    
    \item 
    Negative triples from $Neg$ are given to the generator G as input.
    
    \item 
    G is a \ac{KGE} model with a softmax function to provide  probabilities of being sampled for each given negative triple, which gives us a probability distribution $p_G(h',r,t'|h,r,t)$.

    \item 
    Afterwards, one of these triples is sampled as output of G according to $p_G$, while high quality triples should have a high probability to be sampled.
    
    \item 
    Discriminator D receives as input both the sampled negative triple \triple{h'}{r}{t'} and the ground truth triple \triple{h}{r}{t}.
    The training objective of D is a marginal loss function, because these benefit most from high quality negative examples \cite{cai2017kbgan}.

    \item 
    Subsequently, D calculates the scores of both triples \triple{h}{r}{t} and \triple{h’}{r}{t'} with score function $f_D$ of D, which models distance between points or vector.
    Therefore, a smaller distance indicates a higher likelihood of truth \cite{cai2017kbgan}.
    
    \item 
    Accordingly, the objective of D is to minimize the marginal loss function $L_D$ which is defined as
    \begin{equation}
        L_D=\sum_{(h,r,t)\in\mathcal{T}}[f_D(h,r,t)-f_D(h',r,t')+\gamma]_+ \in \mathbb{R}^+
    \end{equation}
    
    \item 
    To give G feedback for the sampled negative triple \triple{h’}{r}{t'} its calculated score $f_D(h',r,t')$ is send back to G as a reward, which is defined as
    \begin{equation}
        r = -f_D(h',r,t') \in \mathbb{R}^-
    \end{equation}
    since $f_D(h,r,t) \in \mathbb{R}^+$.
    Therefore, the objective of G can be formulated as maximizing the expectation of negative distances:
    \begin{equation}
        R_G=\sum_{(h,r,t)\in\mathcal{T}}\mathbb{E}[-f_D(h',r,t')]
    \end{equation}
    
    \item
    This process continues until convergence
\end{enumerate}
The dynamic distribution of the negative triple is determined through this procedure of the adversarial training, 

\section{Evaluating Knowledge Graph Embeddings} 
To evaluate the \ac{KGE} models, different metrics are available.
Some most commonly used are the following metrics \cite{kotnis2017analysis}:
\begin{itemize}
    \item 
    \ac{MRR} which is defined as
    \begin{equation}
        MRR = \frac{1}{N} \sum_{i=1}^{N}\frac{1}{rank_i}
    \end{equation}
    computes the average of the reciprocal ranks \cite{zhang2021efficient}
    
    \item 
    hits@k which calculates the percentage of appearance in the top-10 ranking \cite{zhang2021efficient}.
    \begin{equation}
        hits@K = \frac{|\{i | rank_i < K\}|}{N}
    \end{equation}
    where $rank_i$ is the rank of the positive instance $i$ predicted by a model with respect to the negative examples.
    Usually $K \in \{1, 3, 5, 10\}$.
\end{itemize}
Since in many \ac{KGE} models either the head or the tail entity is randomly replaced by other entities of the KG, these corrupted triples may be true facts.
For this reason, many approaches distinguish between \textit{raw} and \textit{filtered}, where for corrupted triples it is looked whether these occur in the train, test or validation set \cite{TransE}.











\section{Uncertainty Sampling} \label{sec:uncertaintysampling}
Uncertainty Sampling originates from Active Learning, 
where labeled data for training a supervised model is obtained from a dataset of unlabeled instances.
Based on the informativeness of the unlabeled instances for the learning algorithm, a prioritization results in which they are labeled.
It is used in machine learning approaches, where unlabeled data is abundant, but it is difficult, time-consuming, or expensive to obtain labeled data \cite{Settles2009ActiveLL}.
They aim for greater accuracy with fewer labeled training instances \cite{Settles2009ActiveLL}.
These selected unlabeled instances can either be generated de novo or sampled from a given distribution.
In the literature several query strategies have been proposed with different approaches how to receive informative instances, one of them is Uncertainty Sampling \cite{Settles2009ActiveLL}.
Other query strategies for Active Learning are Query-By-Committee, Expected Model Change, Variance Reduction, Fisher Information Ration, Estimated Error Reduction and Density-Weighted Methods \cite{Settles2009ActiveLL}.
They provide different strategies to obtain informative instances from the unlabeled dataset like voting of a committee consisting of several trained models, querying the instance that would impart the greatest change to the current model if we knew its label or queries instances which minimize the learner’s future error by minimizing its variance.

In Uncertainty Sampling, given a model $\theta$ which has been trained on labeled dataset $D$, each instance $x_j$ of the unlabeled data pool $U$ will be assigned a utility score $s(\theta, x_j)$.
Subsequently, the instance with the highest score will be sampled.
Popular examples of measures for utility score include
\begin{itemize}
    \item the entropy:
     $$s(\theta, x) = - \sum_{y \in \mathcal{Y}}{p_{\theta}(y | x) \cdot log p_{\theta}(y|x)}$$

    \item the least confidence:
    $$s(\theta, x) = 1 - \max_{y \in \mathcal{Y}}{p_{\theta}(y | x)}$$
    
    \item the smallest margin:
    $$s(\theta, x) = p_{\theta}(y_m | x) - p_{\theta}(y_n|x)$$
    where
    $y_m = \argmax_{y \in \mathcal{Y}} p_{\theta}(y | x)$ 
    and 
    $y_n = \argmax_{y \in \mathcal{Y} \setminus y_m}{p_{\theta}(y | x)}$
\end{itemize}
Following three different frameworks for measuring the  uncertainty of a learner can be separated \cite{nguyen2021howtomeasure}.
While the first one has been specifically developed for the purpose of active learning, the others are more general approaches for machine learning \cite{nguyen2021howtomeasure}.
\begin{enumerate}
    \item 
    \textbf{Evidence-based uncertainty (EBU)} differentiates between uncertainty due to conflicting evidence and insufficient evidence.
    \ac{EBU} looks at the influence of individual features,
    partitions them into those that provide evidence for the positive and for the negative class, 
    and either queries the instance with the highest conflicting evidence (\ac{CEU}) or where both evidences are low (\ac{IEU}).

    \item 
    \textbf{\ac{CU}} seeks to differentiate between the reducible and irreducible part of the uncertainty in a prediction by defining a
    credal set of models with probability distributions.
    A class y is dominated by another y' if y is more likely than y' for any distribution in the credal set
    An instance x is sampled, which has the least evidence for the dominance of one of the classes \cite{nguyen2021howtomeasure}.

    \item 
    \textbf{Epistemic and aleatoric uncertainty (EAU)} are based on the use of relative likelihoods. 
    \ac{EU} samples instance for which both the positive and the negative class appear to be plausible, while \ac{AU} samples instances where none of the classes is supported.
    Therefore, the uncertainty due to either influence of the classes or lack of knowledge. 
\end{enumerate}













