\subsection{Dynamic Distribution-Based Sampling}
\label{subsec:dynamic_distribution_based_sampling}

Dynamic Distribution-Based Sampling tries to model the changes the distribution of negative triples by using a \ac{GAN}-based framework which includes two components: A generator and a discriminator.
While the generator dynamically approximates the constantly updated Negative Sampling distribution to provide high-qualitative negative triples, 
the discriminator learns to distinguish positive and negative triples with its own \ac{KGE} model.
Known approaches are \ac{KBGAN} \cite{cai2017kbgan}, \ac{IGAN}  \cite{IGAN} or \ac{MCNS} \cite{MCNS}.


\textbf{Evaluation}:\\
% DYNAMIC DISTRIBUTION-BASED SAMPLING
To capture the dynamic distribution of the negative triples, the
group of \textit{Dynamic Distribution-Based Sampling} (\autoref{sec:negativesamplingmethods}) was developed.
The two approaches \ac{KBGAN} \cite{cai2017kbgan} and \ac{IGAN} \cite{IGAN} are considered as pioneering works of these Sampling methods and attempt to address the challenge of capturing the negative distribution by a \ac{GAN} \cite{zhang2021efficient}.
\acp{GAN} were originally proposed for generating examples in a continuous space such as images and adapted for the generation of hard negative examples \cite{zhang2021efficient}.
Using an adversarial training process like in \ac{KBGAN} described in \autoref{sec:kbgan}, the distribution of negative triples is thus determined.
However, this is not an efficient training process and it is not effectively sampled from this distribution.
Additionally, it increases the number of training parameters because of the generator and the model suffers from instability and degeneracy.
This results from the fact, that the \textsc{REINFORCE} gradient has high variance for training the generator and only a few negative triples lead to large gradient \cite{zhang2021efficient}.
Consequently, the \ac{GAN}-based models have to put a lot of effort to model the negative triple distribution which leads to instable performances.
Since the generator and discriminator in Negative Sampling only draw information from the two underlying embedding models, pre-training is necessary, which is a time-consuming endeavor.
However, even if only a few triple pairs (negative + positive triple) are useful for learning embedding, they are not maintained to be able to learn from them at a later time.
Moreover, no other information of the graph is included except the softmax-probability outputs of the generator and the \ac{KGE} model of the discriminator.
Randomly selecting one of the negative triples does not ensure that a useful one is selected.
For embedding, the distinction between negative and positive triples from areas of a \ac{KG} about which no or incomplete information are available could be particularly important.

% DYNAMIC DISTRIBUTION-BASED NEGATIVE SAMPLING
KBGAN
\cite{zhang2021efficient}:
- avoid vanishing gradient problem 
-> but more complex and harder to train
- waste time on additional parameters to fit the full distribution of negative triplets
Problems: - increases the number of parameters
- learning suffers from instability and degeneracy
-> REINFORCE gradient has high variance
- only a few negatives with high gradient -> a lot of effort find them (model distribution of negatives)
-> instable performance
- pretraining necessary
- other approaches like Self adversarual sampling (Self-Adv) tried to solve this problem by using a self embedding model for the generator
-> disadvantage: can not guarantee to sample enough negative triplets with large gradients 
