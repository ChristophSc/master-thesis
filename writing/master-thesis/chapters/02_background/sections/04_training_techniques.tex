\section{Training Techniques}
\label{sec:training_techniques}

Among all of there \ac{KGE} models, three commonly used approaches are used for training and differ mainly in the way how negative examples are generated \cite{Ruffinelli2020You}:

\subsection{Negative Sampling}
In Negative Sampling a set of (pseudo-) negative triples is generated by perturbing the subject, relation or object for each positive triple $(h, r, t)$ from the training data a set.
Optionally each obtained negative triple is verified, if it exists in the training \ac{KG} (false negative triples).


\input{chapters/02_background/sections/owa_training_algorithm}



\subsection{1vsAll}:
The 1vsAll training approach omits sampling by perturbing subject and object posiitions for all positive triples from the \ac{KG}, even if these negative triples exist as positive triples.
In comparison to the Negative Sampling approach it is generally more expensive but feasible if the number of entities is not excessively large.
    
\subsection{KvsAll}:
The KvsAll training type is divided into two different steps:
At first, triples are created from non-empty rows $(h,r,t)$.
Therefore, a pair $(h,r)$ is taken and scored against all entities $t \in \entities$.
Secondly, all of these generated triples are labeled as either positive or negative.