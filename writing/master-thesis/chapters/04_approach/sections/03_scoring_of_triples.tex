\section{Scoring of Triples} \label{sec:scoring_of_triples}

To distinguish between positive and negative triples, a scoring function is defined for each \ac{KGE} model which assigns a score to each triple.
Depending on the model this score must be low or high to reflect a positive triple.
Therefore, the score is in indicator how likely a triple is positive.

Since the score plays a central role in the sampling process by uncertainty later on and and the uncertainty of a model is reflected in the underlying score for triples, we want to go into a bit more detail about the score at this point.
In the KBGAN approach the score is calculated by the generator \ac{KGE} model, which provide the logits for the subsequent calculation of sampling probability.
One example of scores for positive and negative triples is illustrated in \autoref{fig:uncertainty}.
\ref{fig:uncertainty}
\begin{figure*}[t]
  \centering
    \includegraphics[width=0.75\textwidth]{figures/uncertainty.PNG}
  \caption{Score range of positive and negative triples.
  The uncertainty of the model is in the scoring range where the model found scores of both positive and negative triples.}
  \label{fig:uncertainty}
\end{figure*}
It can be recognized that positive as well as negative triples have a different range of scores.
While positive triples achieve a score between -12 and 4, negative scores are higher in range between -3 and 12.
While in the outer areas triples can be classified more easily as positives or negatives, there is also an overlap of both score areas where triples can belong either to class "positive" or to class "negative".







1) Presentation of my idea\\
- origin of my idea\\
- Reasons for Uncertainty - advantages and expected results\\
- Why did I choose KBGAN (Sampling Negatives from Dynamic Distribution) as basis for my approach\\
- disadvantages of KBGAN that I want to solve\\
- why uncertainty sampling can solve them\\

2) Architecture and Procedure\\
- general procedure for placement in the overall context \\
- comparison between original KBGAN approach and my approach\\
- references to the following sections an why these are described\\

3) Scoring of Triples\\
- What is uncertainty in relation to KGEs?\\
-> Uncertainty how to classify a triple -> negative or positive\\
- Where does the Uncertainty come from?\\
-> overlap of scores from positive and negative triple scoring ranges\\
Background: \\
- presentation + reference to different scoring functions\\
-> scoring functions are used for loss functions\\
- different loss Functions for different scoring functions\\
- learning process goal: decrease the loss \\
-> a.k.a. increase margin between positives and negatives\\
-> Show Figure which illustrates positive and negative triples + margin between them\\
- examples:\\
    - DistMult: Score of positive triples need to be higher than for negative triples\\
    - TransE: score of negative need to be higher than for positive triples\\
- show Figure with positives + negative score ranges -> uncertainty in overlapping scores\\
- mathematical explanation with definition of loss function and scoring function\\
-> give an example of positive + negative where model is certain\\
-> give an example of positive + negative where model in uncertain\\\\

-> uncertainty according to scores which are returned by models\\
- models include only implicit information about the structure etc\\
- add additional information about structure, clusters, ...\\
- ... feature functions\\

- Option 1: Uncertainty from Generator Model: negative score < positive score\\
- Option 2: Uncertainty from Discriminator Model: negative score > positive score\\
-> depending on the, also the feature functions need to reflect the same score for positives/negatives\\

4) Feature Functions\\

- present all different feature functions\\
-> depending on option 1 or 2: they have to return same order of positive and negative triple scores\\
(like the score of the generator)\\
- 

5) Generator Score

- now we have the score of the generator which gives information about negative

6) Probabilities 

7) Uncertainty


8) Sampling by Uncertainty
    








