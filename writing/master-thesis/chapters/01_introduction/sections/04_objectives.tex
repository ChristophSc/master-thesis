\section{Objectives}

In this thesis, we aim to go one step further by generating more informative negative examples. 
By doing so, we aim to accelerate the learning process.
It is based on Uncertainty Sampling which aims to find more informative negative examples for embedding learning so that an improvement in the learning process and resulting embedding is achieved.
These can be hard negative examples, but also other negative triples which are valuable for the embedding model.


- hard negatives -> more informative triples for the KGE
- implementation of Uncertainty Sampling in KBGAN
- either: 
    - get same MRR/ Hit\@10 with less epochs
    - get same MRR/ Hit\@10 with less training time
    - get better overall MRR/ Hit\@10
- try several methods how to measure uncertainty
- add additional information to sampling process and classify triples as positives or negatives
- run model on several datasets
- run model on $PC^2$
- provide evaluation and compare it with original KBGAN approach
- compare with several other Negative Sampling approaches
- leave future work: 
    - 