\section{Problem Analysis}

% -> WHAT IS THE PROBLEM OF NEGATIVE SAMPLING
% GENERAL
Negative sampling has thus become a common method for learning embeddings.
Due to its high performance even for large \acp{KG}, it is used in different variants for many \c{KGE} models.
However, the learning method of negative sampling also brings some problems and optimization possibilities with it, which are explained in more detail in the following:

First of all, it can be said that while many negative sampling methods currently demonstrate high performance, the sampled negatives are often too simple and represent a trivial solution. 
The embedding models do not learn from the provided negative triples and therefore, they do not improve the embedding and suffer from the vanishing gradient or biased estimation problem \cite{zhang2021efficient}.
The vanishing gradient problem is present when the gradients of the loss functions approach zero and consequently, the model is unable to learn during the training process.
This results from the fact that most KGE models, due to simplicity and efficiency, use Uniform Negative Random Sampling.
In this standard technique of Negative Sampling either the head or the tail entity in a given positive triple \triple{h}{r}{t} is replaced by any other entity of the \ac{KG} which remains in the new negative triple \triple{h’}{r}{t} or \triple{h}{r}{t’}. 
Therefore, it is very likely to pick an entity which results in a zero gradient because
the negative triple can be easily discriminated from the positive one \cite{cai2017kbgan}.
For example, by replacing the head entity of the positive triple \triple{h}{r}{t} = \triple{Joe Biden}{bornIn}{USA} with head entity h' = Paderborn would result in the negative triple \triple{Paderborn}{bornIn}{USA} which is not very informative for the embedding.
By simply replacing the randomly selected head or tail entity of a again randomly selected entity of the KG does not use any further information.
For example, it would have been useful either if negative sampling had recognized that the head entity "Joe Biden" is a person and to replace it with another person.
Or by recognizing that the tail entity as well as the sampled entity "Paderborn" is a place and its replacement would have led to the  much more meaningful negative triple \triple{Joe Biden}{bornIn}{Paderborn}.  

In the direction of the latter approach goes Bernoulli Sampling.
In comparison to Negative Random Sampling, it considers types of relations between entities (one-to-many, many-to-one and many-to-many) \cite{zhang2021efficient}.
These relation types is an indicator for the sampling approach if it is better to replace the head or the tail entity.
From the above example, it would have been recognized that the relation "bornIn" is a many-to-one relation.
Therefore, the head entity cannot have this relation to multiple entities, making each replaced tail entity a proper and more useful negative triple.

In addition to these most commonly used methods, there are others which leverage external ontological constraints such as entity types.
However, this resource does not always exist or is accessible \cite{cai2017kbgan}.
In addition, a distinction must be made for which KGE models the Negative Sampling is used.
For example, the problem of too easy negative examples is less severe to models using log-softmax loss function since they sample tens or hundreds of negative triples for one positive triple.
By sampling so many negatives for just one positive it is very likely to have a few good ones which help the model to learn the embedding \cite{cai2017kbgan}.
On the other hand, the problem is more severe for KGE models with marginal loss function, because they use a one-to-one ratio for positive and sampled negatives and therefore, bad negatives seriously damage their performance \cite{cai2017kbgan}.

Consequently, there are two main challenges for sampling negative triples \cite{zhang2021efficient}:
At first, it is necessary to capture and model the dynamic distribution if negative triples to sample informative and useful negative triples with high gradients which help the model during the embedding learning process.
Secondly, these negative triples have to be sampled effectively  so that the Negative Sampling does not negatively affect the performance of embedding learning.



% WHAT ARE THE PROBLEMS OF OTHER NEGATIVE SAMPLING TECHNIQUES ?

% general introduction
Since \acp{KG} contain large amount of information, they provide many application possibilities.
However, to achieve good results for these applications we need to learn good embeddings which represent as much information as possible in a graph representation of a low-dimensional vector space.
Most of graph representation learning methods can be unified within a \ac{SampledNCE} framework comprising an encoder that generates node embeddings by learning to distinguish pairs of a positive and a negative triple \cite{MCNS}.
Therefore, we need negative triples for embedding learning which are not available in most \acp{KG}. 

For this reason, we are left with two options to collect them \cite{safavi2021negater}: 
On one hand they can be obtained by human annotation which is very cost-prohibitive at scale, but leads to very informative and useful examples. 
On the other, they can be generated ad-hoc, which in turn does not represent a great effort, but also leads to uninformative and useless negative examples.
For this reason, we need to find a way between these two extreme approaches.






% STATIC-DISTRIBUTION-BASED SAMPLING
\textit{Static Distribution-Based Sampling} (\autoref{sec:negativesamplingmethods}) approaches are commonly used because of their simplicity and efficiency, but ignore the dynamics in the Negative Sampling distribution which lead to the vanishing gradient problem \cite{qianunderstanding}.
This problem occurs when the gradient will be vanishingly small and accordingly, small gradients prevent changing the weight value.
This can impede the training process or, in the worst case, completely stop the model from further training.
% RANDOM UNIFORM NEGATIVE SAMPLING
While negative triples, such as those generated by randomly replacing a head or tail entity, are very likely to be negative examples, they are generally uninformative and useless.
For example, by replacing the tail entity in given positive triple (Paderborn, locatedIn, Germany) by randomly selected entity 'Apple' leads to the new negative triplet (Paderborn, locatedIn, Apple).
Even though it is a true negative triple, it is uninformative and useless for embedding learning.
The problem with 'too easy' negative triples is less severe to models using log-softmax loss function, because they usually sample a high amount of negatives for one positive triple \cite{cai2017kbgan}.
However, the performance of marginal loss functions can be seriously damaged by the low quality of uniformly sampled negatives since negative-to-positive ratio is always 1:1 \cite{cai2017kbgan}.


% CUSTOM CLUSTER-BASED SAMPLING
To get more efficiency in the training process and to search for suitable entities in a more targeted way is aimed with  \textit{Custom Cluster-Based Sampling} methods (\autoref{sec:negativesamplingmethods}).
By selecting negative samples only from a handful of candidates and not from the entire entity set, a better correlation between the positive and corresponding negative triple is expected.
This negative triple should be closer to the original positive triple and thus provide more valuable information for the \ac{KGE} model.
For example, Domain Sampling \cite{domainSampling} is to sample from the same domain.
For example, if it is recognized that 'Germany' is a country and that 'France' is a nearby country, the much more valuable negative triple (Paderborn, locatedIn, France) could be sampled from the  positive triple (Paderborn, locatedIn, Germany).
However, as \acp{KG} grow rapidly and are updated frequently, continuous renewing custom clusters is essential and difficult \cite{qianunderstanding}.

With these approaches of Negative Sampling from a fixed distribution two more different problems arise:
Since they ignore changes in the distribution of negative triples, they suffer from the vanishing gradient and biased estimation problem \cite{zhang2021efficient}.
The scoring functions tend to give observed (positive) triplets large values and most of the non-observed (probably negative) triplets will have smaller values during the training.
Therefore, if negative triplets are uniformly sampled, it is very likely to pick up one with zero gradients.
This leads to the following main challenges for Negative Sampling \cite{zhang2021efficient}: 
(i) The negative triple's dynamic distribution has to be captured and 
(ii) triples have to be effectively sampled from this distribution.

% DYNAMIC DISTRIBUTION-BASED SAMPLING
To capture the dynamic distribution of the negative triples, the
group of \textit{Dynamic Distribution-Based Sampling} (\autoref{sec:negativesamplingmethods}) was developed.
The two approaches \ac{KBGAN} \cite{cai2017kbgan} and \ac{IGAN} \cite{IGAN} are considered as pioneering works of these Sampling methods and attempt to address the challenge of capturing the negative distribution by a \ac{GAN} \cite{zhang2021efficient}.
\acp{GAN} were originally proposed for generating examples in a continuous space such as images and adapted for the generation of hard negative examples \cite{zhang2021efficient}.
Using an adversarial training process like in \ac{KBGAN} described in \autoref{sec:kbgan}, the distribution of negative triples is thus determined.
However, this is not an efficient training process and it is not effectively sampled from this distribution.
Additionally, it increases the number of training parameters because of the generator and the model suffers from instability and degeneracy.
This results from the fact, that the \textsc{REINFORCE} gradient has high variance for training the generator and only a few negative triples lead to large gradient \cite{zhang2021efficient}.
Consequently, the \ac{GAN}-based models have to put a lot of effort to model the negative triple distribution which leads to instable performances.
Since the generator and discriminator in Negative Sampling only draw information from the two underlying embedding models, pre-training is necessary, which is a time-consuming endeavor.
However, even if only a few triple pairs (negative + positive triple) are useful for learning embedding, they are not maintained to be able to learn from them at a later time.
Moreover, no other information of the graph is included except the softmax-probability outputs of the generator and the \ac{KGE} model of the discriminator.
Randomly selecting one of the negative triples does not ensure that a useful one is selected.
For embedding, the distinction between negative and positive triples from areas of a \ac{KG} about which no or incomplete information are available could be particularly important.

In summary, there are already several approaches for Negative Sampling, but they still have problems to capture the dynamic distribution of the negative triples or effectively sample them \cite{zhang2021efficient}.
Therefore, we present a new Negative Sampling technique which aims to \textbf{improve the efficiency of Sampling by selecting more informative and useful negative examples for the embedding model by uncertainty}.












KBGAN
\cite{zhang2021efficient}:
- avoid vanishing gradient problem 
-> but more complex and harder to train
- waste time on additional parameters to fit the full distribution of negative triplets
Problems: - increases the number of parameters
- learning suffers from instability and degeneracy
-> REINFORCE gradient has high variance
- only a few negatives with high gradient -> a lot of effort find them (model distribution of negatives)
-> instable performance
- pretraining necessary
- other approaches like Self adversarual sampling (Self-Adv) tried to solve this problem by using a self embedding model for the generator
-> disadvantage: can not guarantee to sample enough negative triplets with large gradients 
- % Why?



Many of these approaches aim to find hard negative examples that are close to positive facts from a given \ac{KG} and thus have a positive effect on the embedding learning process.


