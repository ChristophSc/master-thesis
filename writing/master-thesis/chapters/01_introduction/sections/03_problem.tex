\section{Problem}

% -> WHAT IS THE PROBLEM OF NEGATIVE SAMPLING
% GENERAL
Negative sampling has thus become a common method for learning embeddings.
Due to its high performance even for large \acp{KG}, it is used in different variants for many \c{KGE} models.
However, the learning method of negative sampling also brings some problems and optimization possibilities with it, which are explained in more detail in the following:

First of all, it can be said that while many negative sampling methods currently demonstrate high performance, the sampled negatives are often too simple and represent a trivial solution. 
The embedding models do not learn from the provided negative triples and therefore, they do not improve the embedding and suffer from the vanishing gradient or biased estimation problem \cite{zhang2021efficient}.
The vanishing gradient problem is present when the gradients of the loss functions approach zero and consequently, the model is unable to learn during the training process.
This results from the fact that most KGE models, due to simplicity and efficiency, use Uniform Negative Random Sampling.
In this standard technique of Negative Sampling either the head or the tail entity in a given positive triple \triple{h}{r}{t} is replaced by any other entity of the \ac{KG} which remains in the new negative triple \triple{h’}{r}{t} or \triple{h}{r}{t’}. 
Therefore, it is very likely to pick an entity which results in a zero gradient because
the negative triple can be easily discriminated from the positive one \cite{cai2017kbgan}.
For example, by replacing the head entity of the positive triple \triple{h}{r}{t} = \triple{Joe Biden}{bornIn}{USA} with head entity h' = Paderborn would result in the negative triple \triple{Paderborn}{bornIn}{USA} which is not very informative for the embedding.
By simply replacing the randomly selected head or tail entity of a again randomly selected entity of the KG does not use any further information.
For example, it would have been useful either if negative sampling had recognized that the head entity "Joe Biden" is a person and to replace it with another person.
Or by recognizing that the tail entity as well as the sampled entity "Paderborn" is a place and its replacement would have led to the  much more meaningful negative triple \triple{Joe Biden}{bornIn}{Paderborn}.  

In the direction of the latter approach goes Bernoulli Sampling.
In comparison to Negative Random Sampling, it considers types of relations between entities (one-to-many, many-to-one and many-to-many) \cite{zhang2021efficient}.
These relation types is an indicator for the sampling approach if it is better to replace the head or the tail entity.
From the above example, it would have been recognized that the relation "bornIn" is a many-to-one relation.
Therefore, the head entity cannot have this relation to multiple entities, making each replaced tail entity a proper and more useful negative triple.

In addition to these most commonly used methods, there are others which leverage external ontological constraints such as entity types.
However, this resource does not always exist or is accessible \cite{cai2017kbgan}.
In addition, a distinction must be made for which KGE models the Negative Sampling is used.
For example, the problem of too easy negative examples is less severe to models using log-softmax loss function since they sample tens or hundreds of negative triples for one positive triple.
By sampling so many negatives for just one positive it is very likely to have a few good ones which help the model to learn the embedding \cite{cai2017kbgan}.
On the other hand, the problem is more severe for KGE models with marginal loss function, because they use a one-to-one ratio for positive and sampled negatives and therefore, bad negatives seriously damage their performance \cite{cai2017kbgan}.

Consequently, there are two main challenges for sampling negative triples \cite{zhang2021efficient}:
At first, it is necessary to capture and model the dynamic distribution if negative triples to sample informative and useful negative triples with high gradients which help the model during the embedding learning process.
Secondly, these negative triples have to be sampled effectively  so that the Negative Sampling does not negatively affect the performance of embedding learning.


