
\section{Architecture and Procedure} 
\label{sec:architecture_and_procedure}

% 2) Architecture and Procedure
% - general procedure for placement in the overall context 
% - comparison between original KBGAN approach and my approach
% - references to the following sections an why these are described
The procedure of the original KBGAN approach was described in \autoref{sec:kbgan}.
For our approach, the random sampling from negative triple set $Neg$ is replaced by sampling by uncertainty in our approach.
This causes a change in the adversarial learning procedure, so the adapted version is explained in more detail here.
This procedure is depicted in \autoref{fig:uncertainty_sampling_architecture}.
\begin{figure*}[t]
  \centering
    \includegraphics[width=0.90\textwidth]{figures/architecture.png}
  \caption{Uncertainty Sampling replaces the original Random Sampling technique in \ac{KBGAN}.}
  \label{fig:uncertainty_sampling_architecture}
\end{figure*}
Input of the algorithm of our approach is like in original \ac{KBGAN} a pre-trained generator G with parameters $\theta_G$ and score function $f_G(h,r,t)$ and a pre-trained discriminator D with parameters $\theta_D$ and score function $f_D(h,r,t)$.
The training process can be described in the following steps:
\begin{enumerate}
    \item For each epoch in the training process a set of negative triples is created.
    This set, which is defined by $Neg(h,r,t)=\{(h_i',r,t_i')\}_{i=1\dots N_s}$, is created by uniformly randomly sampling $N_s$ negative triples by replacing head $h$ or tail entity $t$ for given positive triple $(h, r, t)$.
    
    \item 
    It receives all negative triples from $Neg$ set and the pretrained model calculates a score with score function $f_G$ for each of these negative triples.
    Based on the calculated score, negative triples can be distinguished from positive ones. Since the generator is a Tensor Factorization-Based Model, a higher score represents a higher likelihood to be a positive triple.
    A more detailed description of the scoring of triples is described in   \autoref{sec:scoring_of_triples}.
    
    \item 
    Based on the pretrained generator model, scores are calculated in the generator with scoring function $f_G$ for each negative triple $(h',r,t') \in Neg$.
    
    \item 
    All of these calculates scores from generator are handed to the Uncertainty Sampler to calculate the uncertainty of the model.
    Uncertainty is expressed in this case using an ambiguous classification.
    Therefore, different information for such a classification must be provided to the Uncertainty Sampler.
    An essential part of this classification is the calculated score (score function $f_G$) of the generator for each negative triple.
    In addition to this, further information are provided for the classification to improve the classification.
    These functions are called feature functions and are described in more detail in \autoref{sec:featurefunctions}.
    
    \item 
    % TODO: instead of using uncertainty of the model, other option: several classifiers each of them have to classify triples as positive or negative.
    % if their result is different, this is uncertainty
    All of these information are combined and weighted by hyperparameters in the $Generator\_Score$ function to obtain one single score for each negative triple (\autoref{sec:generatorscore}).
    Again, a high score reflects a higher likelihood of being a negative triple.

    \item Since Uncertainty Sampling samples instances based on probabilities for different classes, these must be calculated using the previously calculated generator score (\autoref{sec:probabilities}).

    \item At this point, the uncertainty of the model is calculated.
    The uncertainty refers to the classification of a triple as positive or negative.
    Since we can now calculate the probabilities for both the class "positive" and "negative" with the help of the generator score, we have various methods for calculating the uncertainty.
    These will be explained in more detail in \autoref{sec:uncertainty}.
    
    \item Subsequently, the calculated uncertainty can be sampled.
    There are several approaches how to sample from the previously calculated uncertainties for each negative triple which are explained in more detail in \autoref{sec:sampling_by_uncertainty}.
    
     \item 
    From this step on, everything is the same as the original \ac{KBGAN} approach:
    The generated negative triple $(h',r,t')$ as well as the positive triple $(h, r, t)$ are sent to the discriminator.
    
    \item 
    The discriminator distinguishes both triples by given scoring function $f_D$, which is usually a translation-based \ac{KGE} model like \textsc{TransE} or \textsc{TransD}.
    
    \item 
    The reward defined by $r = - f_D(h',r,t')$ of the current negative triple is calculated and returned to the Generator and the next training iteration begins.
\end{enumerate}
These adversarial training steps are repeated until convergence, such that the generator improves the quality of sampled negative triples and discriminator improves embedding over time.
Output of our algorithm is the adversarially trained discriminator and its embedding.
By incorporating additional information from the graph and Sampling triples by uncertainty that have the most added value to the embedding model, we expect promising results.



if we use softmax output from original kbgan method, this would sample negatives around probability 0.5, so just very bad negative examples
-> initial model does not classify triples as positives and negatives, it just measures the plausibibility of a negative to be true
-> the closer to 1 the better
