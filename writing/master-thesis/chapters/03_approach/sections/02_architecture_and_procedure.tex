
\section{Architecture and Procedure} 
\label{sec:architecture_and_procedure}

% 2) Architecture and Procedure
% - general procedure for placement in the overall context 
% - comparison between original KBGAN approach and my approach
% - references to the following sections an why these are described
The procedure of the original KBGAN approach was described in \autoref{sec:kbgan}.
For our approach, the random sampling from negative triple set $Neg$ is replaced by sampling by uncertainty in our approach.
This causes a change in the adversarial learning procedure, so the adapted version is explained in more detail here.
This procedure is depicted in \autoref{fig:uncertainty_sampling_architecture}.
\begin{figure*}[t]
  \centering
    \includegraphics[width=0.90\textwidth]{figures/architecture.png}
  \caption{Uncertainty Sampling replaces the original Random Sampling technique in \ac{KBGAN}.}
  \label{fig:uncertainty_sampling_architecture}
\end{figure*}

After the set of negative triples $Neg$ is formed, different scores are calculated for all triples in the set.
Based on this calculated value, the uncertainty of the model in the classification is calculated and then the example with the highest score is sampled. 

Input of the algorithm of our approach is like in original \ac{KBGAN} a pre-trained generator G with parameters $\theta_G$ and score function $f_G(h,r,t)$, 
and a pre-trained discriminator D with parameters $\theta_D$ and score function $f_D(h,r,t)$.
The training process can be described in the following steps:

According to this architecture the training process of the original \ac{KBGAN} approach is adjusted to Uncertainty Sampling and includes the following steps:
\begin{enumerate}
    \item For each epoch in the training process a set of negative triples is created.
    This set, which is defined by $Neg(h,r,t)=\{(h_i',r,t_i')\}_{i=1\dots N_s}$, is created by uniformly randomly sampling $N_s$ negative triples by replacing head $h$ or tail entity $t$ for given positive triple $(h, r, t)$.
    
    \item 
    The generator is a Tensor Factorization-Based Model and receives all negative triples from $Neg$ set and the pretrained model calculates a score with score function $f_G$ for each of these negative triples.
    Based on the calculated score, negative scores can be distinguished from positive scores:
    The higher the score, the more likely it is to be a negative triple (\autoref{sec:scoring_of_triples}).
    
    \item All of these scores from generator score function $f_G$ are handed to the Uncertainty Sampler to calculate the uncertainty of the model.
    For this purpose, the calculated scores of the generator from the function $f_G$ are included, as well as outputs from other functions which include further information of the \ac{KG}.
    These functions and which information they contain is described in more detail in \autoref{sec:featurefunctions}.
    
    \item All of these information are combined and weighted by hyperparameters in the $Generator\_Score$ function to obtain one single score for each negative triple (\autoref{sec:generatorscore}).
    Again, a high score reflects a higher likelihood of being a negative triple.
    
    \item Since Uncertainty Sampling samples instances based on probabilities for different classes, these must be calculated using the generator score (\autoref{sec:probabilities}).

    \item At this point, the uncertainty of the model is calculated.
    The uncertainty refers to the classification of a triple as positive or negative.
    Since we can now calculate the probabilities for both the class "positive" and "negative" with the help of the generator score, we have various methods for calculating the uncertainty.
    These will be explained in more detail in \autoref{sec:uncertainty}.
    
    \item Subsequently, the calculated uncertainty can be sampled.
    There are several approaches how to sample from the previously calculated uncertainties for each negative triple which are explained in more detail in \autoref{sec:sampling_by_uncertainty}.
    
     \item 
    From this step on, everything is the same as the original \ac{KBGAN} approach:
    The generated negative triple $(h',r,t')$ as well as the positive triple $(h, r, t)$ are sent to the discriminator.
    
    \item 
    The discriminator distinguishes both triples by given scoring function $f_D$, which is usually a translation-based \ac{KGE} model like \textsc{TransE} or \textsc{TransD}.
    
    \item 
    The reward defined by $r = - f_D(h',r,t')$ of the current triple pair is calculated and added to the reward sum $r_{sum}$.
\end{enumerate}
These adversarial training steps are repeated until convergence, such that the generator improves the quality of sampled negative triples and discriminator improves embedding over time.
Output of our algorithm is the adversarially trained discriminator and its embedding.
By incorporating additional information from the graph and Sampling triples by uncertainty that have the most added value to the embedding model, we expect promising results.