\section{Idea} 
\label{sec:idea}

% 1) Presentation of my idea
% - origin of my idea
% - Reasons for Uncertainty - advantages and expected results
% - Why did I choose KBGAN (Sampling Negatives from Dynamic Distribution) 
%   as basis for my approach
% - disadvantages of KBGAN that I want to solve
% - why uncertainty sampling can solve them?

% Refer to Problem section that negative triples from current sampling approaches are "too easy"
% refer to uncertainty sampling origin + give most informative samples to model where models learns the most
% + effective in other areas
% -> adopt idea to negative sampling in KGE models
As we have seen in \autoref{sec:training_techniques}, there are several ways to learn a \ac{KGE}.
Many models learn embeddings by distinguishing positive triples from the \ac{KG} and negative triples generated by Negative Sampling.
Accordingly, although many models rely on Negative Sampling, most of these approaches provide only negative triples with insufficient quality (\autoref{sec:problem_analysis}).
With our approach, we aim to improve the Negative Sampling process for embedding models by incorporating uncertainty information and thus selecting negative triples that are more informative and more valuable for the embedding model.
Informative at this point means particularly interesting triples for the embedding model because those are difficult to classify as positive or negative triple.
Therefore they help the embedding model the most to differentiate negative from positive triples.

% why uncertainty sampling can solve current problems
% Uncertainty Sampling in other approaches -> results
Including uncertainty into various procedures and training models has already yielded promising results in recent work.
For example, this makes Active Learning more efficient by only querying particularly informative instances in the labeling process \autoref{sec:uncertaintysampling}.
There are many other Active learning query frameworks (\autoref{sec:uncertaintysampling}), but Uncertainty Sampling is the most commonly used one from Active Learning \cite{Settles2009ActiveLL}.
In addition, it is easier to implement than some other frameworks, since it does not require to train multiple models as it is the case with the Query-by-Committee approach \cite{Settles2009ActiveLL}.
Nevertheless, Uncertainty Sampling provides promising results for many Active Learning scenarios and is therefore used for our approach to sample from a negative triple set.
But also in the world of uncertain \acp{KG}, confidence scores for positive triples have already been included in \acp{URGE} \cite{UKGE}.
Accordingly, we also want to optimize the Negative Sampling process by incorporating uncertainty information.

% + Why did I choose KBGAN (Sampling Negatives from Dynamic Distribution) 
The basic idea of Sampling by Uncertainty is to select particularly interesting negative triples from an existing set of negative triples.
Accordingly, any approach of the three categories Static Distribution-Based Sampling (\autoref{subsec:static_distribution_based_sampling}, Custom Cluster-Based Sampling (\autoref{subsec:custom_cluster_based_sampling}) and Dynamic Distribution-Based Sampling (\autoref{subsec:dynamic_distribution_based_sampling}) can be selected.
Since the dynamic distribution-based approach \kbgan already creates such a set of negative triples and capture the distribution of negative triples dynamically, this approach will be extended by uncertainty information.
Moreover, it specifically suffers from inefficient sampling which slows down adversarial training.
For this reason, we aim to accelerate the learning process of \kbgan by selecting more informative triples for the negative triple set $Neg$ by calculating the uncertainty.

\section{Selection of the Uncertainty Sampling Metric} 
\label{sec:selection_of_the_uncertainty_sampling_metric}
% caption: (???)
% what uncertainty?
% Option1: EBU: conflicting vs insufficient evidence (within one model)
% Option2: EAU epistemic <-> aleatoric uncertainty (among multiple models)
% CU: reducible and irreducible part of uncertainty in a prediction
In the literature we have several types of uncertainty at our disposal, based on which Negative Sampling can be performed.
As described in \autoref{sec:uncertaintysampling}, there are three different approaches of \ac{EBU}, \ac{EAU} and \ac{CU}.
These methods for calculating uncertainty are based on different viewpoints, but all of them are based on a classification problem of two or more classes.
In \ac{EBU} we consider the uncertainty within a model by looking at the individual features of a model and if they provide evidence for the classes.
This can be either very many (\textit{conflicting-evidence}) or only a few features (\textit{insufficient-evidence}) which are indicative for the respective classes.
In contrast, \ac{EAU} considers \textit{epistemic uncertainty} within a single model’s prediction, and \textit{aleatoric uncertainty} across multiple model predictions \cite{human-in-the-loop}.
Since \textit{credal uncertainty} differentiates between the
reducible and irreducible part of the uncertainty in a prediction and the domination of one class over the other, we calculate uncertainty among multiple models as well.
% for this model again have conflicting evidence or insufficient evidence for one class
% -> Firstly we focus on uncertainty within one model (generator kge model) -> can be insufficient or conflicting evidence
The \kbgan approach does not provide any additional features for the sampling process, only the scores of all negative triples from $Neg$ and their corresponding sampling probabilities.
For this reason, we will first take the point of view of epistemic uncertainty from \ac{EAU}.
If we add further models for a classification between negative and positive triples in the later process or add additional features, the other viewpoints aleatoric uncertainty, \ac{EAU} and \ac{CU} can be taken as well.



\section{Definition of the Classification Problem}
\label{sec:definition_of_the_classification_problem}
% uncertainty sampling approaches require a classification problem
% we can either: 
% 1) good <-> bad negative triple (like original approach)
% 2) provide prob of triple to be true 
Next, since Uncertainty Sampling needs a classification problem, there are two options available for the definition:
First, like in the original \kbgan approach, we could distinguish between "good" and "bad" negative triples and try to provide the best ones to the discriminator.
Since the generator of the adversarial training is a tensor factorization-based model, it provides scores which indicate plausibility of a triple.
According to this plausibility scores a sampling distribution among all negative triples is created. 
This has the advantage that we only have to calculate the scores of negative triples, but we lose the information about positive triples and their scores.
Therefore, negative triples are sampled according to their calculated scores with either linearly increasing probability or as in \cite{UKGE} with logistic function or a bounded rectifier.
To follow the open-world assumption and reduces the probability of sampling false-negatives,
\kbgan limits the number of negative triples to twenty \cite{cai2017kbgan}.

Another approach is to not only negative triples, but instead distinguish between negative and positive triples and their scores.
In contrast to the first option in this one also information about positive triples and their scores is preserved.
If we define the classes $y = 0$  for negative triples and $y = 1$ for positive triples, we can classify each triple $(h,r,t)$ in a binary classification problem.
Accordingly, we can calculate the probability of being either a negative or a positive triple for each triple $\mathbb{P}(y| (h,r,t)) \in [0,1]$ for $y \in \{0,1\}$.
Since we want to have as good negative triples as possible and therefore, triples that can be classified either positive or negative, we look for negative triples $(h',r,t')$ close to the probability of $\mathbb{P}(y = 0| (h',r,t')) = \mathbb{P}(y = 1| (h',r,t')) = 0.5$.
Since this means the triple can be assigned to both the positive and the negative class, 
the model is uncertain how to classify the triple.

The difference between the two approaches can be explained by the following example and \autoref{fig:uncertainty}, which illustrates scoring values of positive and negative triples and the uncertainty of a model´s prediction.
\begin{figure*}[t]
  \centering
    \includegraphics[width=0.75\textwidth]{figures/uncertainty.PNG}
  \caption{Score range of positive and negative triples.
  The uncertainty of a model is in the scoring range where the model found scores of both positive and negative triples.}
  \label{fig:uncertainty}
\end{figure*}
Suppose we have the positive triple $l_p=(h,r,t)$ (t marked as blue dot) in our KG and some other entities $t^* \in \entities$ (green dots) from positive triples $(h,r,t^*)$ from the \ac{KG} .
After we obtain the negative triple set $Neg$ through the Negative Sampling process and scored them by the generator embedding model, also a range of scoring values for negative triples exists. 
Therefore, for entities within the overlap of positive and negative scoring values, the model is uncertain whether they are positive or negative.
The original sampling approach of \kbgan assigns now ascending probabilities for increasing scoring values.
Therefore, it is most likely to sample $t'_3$, because the negative triple $(h,r,t'_3)$ achieves the highest score and, accordingly, has the highest probability to be positive.
Although, the entities $t'_1$ and $t'_2$ are also very interesting for the model, because a classification based on the achieved scoring values is not easy.
Consequently, instead of only including negative triple scoring information by differentiating between "good" and "bad" negative triples, the second approach incorporates information of positive triples.
Thus, this more detailed distinction allows us to avoid sampling false negative triples, since, for example, sampling $t'_3$ can be particularly problematic if it yields an even higher score and should actually be classified as a positive triple because it is a false negative triple.


\section{Calculation of Uncertainty Scores}
\label{sec:calculation_of_uncertainty_scores}
To calculate the uncertainty scores based on the calculated probabilities happens only in the following steps:
At first, for each triple $l = (h, r, t)$ a plausibility score $f_G(l)$ is calculated by the generator embedding model. 
Subsequently, the uncertainty scores are needed to perform the Negative Sampling by uncertainty.
For this a transformation function $\phi$ is needed which maps $f_G(l)$ to an uncertainty score $u(l)$ .
\begin{equation} \label{eqn:uncertainty_function}
    u(l) = \phi(f_G(l)), \phi: \mathbb{R} \rightarrow [0,1]
\end{equation}
For this transformation function we have several uncertainty metrics at our disposal which are Entropy, Least Confident, Margin of Confidence and Ratio of Confidence, which calculate uncertainty score $u(l) \in [0, 1]$ for probabilities $\mathbb{P}(y = 1 | (h,r,t))$ (\autoref{fig:sampling_distributions}).
\begin{figure}
    \centering
    \begin{minipage}{.5\textwidth}
      \centering
      \includegraphics[width=0.9\linewidth]{figures/entropy_graph.PNG}
    \end{minipage}%
    \begin{minipage}{.5\textwidth}
      \centering
      \includegraphics[width=0.9\linewidth]{figures/least_confident_graph.PNG}
    \end{minipage}
    \begin{minipage}{.5\textwidth}
      \centering
      \includegraphics[width=0.9\linewidth]{figures/smallest_margin_graph.PNG}
    \end{minipage}%
    \begin{minipage}{.5\textwidth}
      \centering
      \includegraphics[width=0.9\linewidth]{figures/smallest_ratio.PNG}
    \end{minipage}%
    \caption{Uncertainty Sampling Distributions for (a) Entropy, (b) Least Confident, (c) Margin of Confidence and (d) Ratio of Confidence metrics in a binary classification problem.
    The x-axis marks the probability of a triple belonging to the positive class $\mathbb{P}(y = 1 | l = (h,r,t))$ and the y-axis the uncertainty score $u(l)$ of the corresponding metric.}
    \label{fig:sampling_distributions}
\end{figure}
Accordingly, for the different metrics $\phi$ can be defined as follows \cite{human-in-the-loop}:
\paragraph{\textbf{Entropy}}
considers the differences between all the predictions.
\begin{equation}
    \phi(x) = - \sum_{y \in \mathcal{Y}}{\mathds{P}_{\theta}(y | l = (h,r,t)) \cdot log \mathds{P}_{\theta}(y | l = (h,r,t) )}
\end{equation}
Since we have a binary classification with $y_i \in \{0,1\}$:
\begin{equation}
= - \mathds{P}(y = 1| l) log \mathds{P}(y = 1 | l)
- \mathds{P}(y = 0| l) log \mathds{P}(y = 0 | l)
\end{equation}
\begin{equation}
= - \mathds{P}(y = 1| l ) log \mathds{P}(y = 1 | l)
- (1 - \mathds{P}(y = 1 | l ) log(1 - \mathds{P}(y = 1 | l )
\end{equation}

\paragraph{\textbf{Least Confidence}} 
measures the distance between the most confident prediction and 100\%.
Normalized for $n$ classes to an uncertainty score between 0 and 1 it is defined as follows:
\begin{equation}
    \phi(x) = (1 - \max_{y \in \mathcal{Y}}{\mathds{P}_{\theta}(y | x)}) \cdot (\frac{n}{n-1})
\end{equation}

\paragraph{\textbf{Margin of Confidence}}
deals with the two most confident predictions where
$y_m = \argmax_{y \in \mathcal{Y}} \mathds{P}_{\theta}(y | l = (h,r,t))$ 
and $y_n = \argmax_{y \in \mathcal{Y} \setminus y_m}{\mathds{P}_{\theta}(y | l = (h,r,t))}$
\begin{equation}
    \phi(x) = \mathds{P}_{\theta}(y_m | l = (h,r,t)) - \mathds{P}_{\theta}(y_n | l = (h,r,t))
\end{equation}

\paragraph{\textbf{Ratio of Confidence}}
is quite similar to Margin of Confidence, but instead of the difference it compares the ratio of the two most confident predictions
\begin{equation}
    \phi(x) = \frac{\mathds{P}_{\theta}(y_m | l = (h,r,t))}{\mathds{P}_{\theta}(y_n | l = (h,r,t))} 
\end{equation}
where $y_m$ and $y_n$ are equally defined as in Margin of Confidence.

% two sampling options: 
% 1) sample always the maximum -> all the same
% 2) sample from distribution based on uncertainty scores
% -> different
For sampling based on the calculated uncertainty scores $u(l)$, there are now two possibilities:
On the one hand, the triple with the maximum uncertainty score can always be sampled.
As can be seen from the \autoref{fig:sampling_distributions}, all functions have a maximum at $P(y = 1 | (h,r,t)) = 0.5$ with an uncertainty score of 1. 
Therefore, always sampling the triple with maximum uncertainty would result in sampling the same triple for all uncertainty functions \cite{nguyen2021howtomeasure, human-in-the-loop}.
On the other hand, sampling can be done using probabilities based on the uncertainty values.
By normalizing the Least Confident function, this results in an equal distribution only for this and Margin of Confidence.    
Due to the weaker/stronger drop in the direction of $\mathds{P}(y = 1 | (h,r,t)) = 0$ and $\mathds{P}(y = 1 | (h,r,t)) = 1$ respectively, this causes an expansion/constriction of the sampling of triples close to an uncertainty score of 1.
Therefore, negative triples with a high probability of being a positive triple are assigned a low uncertainty score according to this principle.
This also reduces the probability that false negative triples in the negative triple set $Neg$ will be sampled.
Accordingly, it is possible to generate a larger set than twenty negative triples in the negative triple set $Neg$.

% how to measure the probability of a triple to be positive
Now that we have defined how to calculate the uncertainty score based on the probability of a triple being positive, we need to define how to calculate this probability.
Based on \autoref{fig:uncertainty} with ranges of positive and negative triple scores, we want to determine two bounds: 
First, $score_{min}$ which defines at which scoring value a triple has the probability 0 to be a positive triple ($\mathds{P}(y = 1 | l = (h,r,t)) = 0$) and second $score_{max}$ at which scoring value a triple has the probability 1 to be positive one ($\mathds{P}(y = 1 | (h,r,t)) = 1$) . 
All scores lower than $score_{min}$ get the probability 0 and all scores higher than $score_{max}$ get the probability 1.

% use borders of negatives/positives for classification
First option to define these borders is depicted in \autoref{fig:positives_negatives1}.
\begin{figure*}[t]
  \centering
    \includegraphics[width=0.75\textwidth]{figures/positives_negatives1.PNG}
  \caption{First option to define probabilities of triples to be positive ($\mathds{P}(y = 1 | (h,r,t))$ ) or negative ($\mathds{P}(y = 0 | (h,r,t))$). 
  $score_{min}$ at the left border of all negative triple scores and $score_{max}$ at the right border of all positive triples.}
  \label{fig:positives_negatives1}
\end{figure*}
The advantage of this option is, that almost every negative triple is assigned a probability between 0 and 1 and therefore, an uncertainty score higher 0.
This results in a probability for almost all negative triples to be sampled.
The disadvantage is that depending on the shift of the positive and negative triple scoring ranges this would result in bad negative triples or too good negative triples being sampled.
The first option results in the following definitions of $score_{min}$ and $score_{max}$:
\begin{equation} \label{eqn:opt1_score_min}
    score_{min} := \argmin_{(h',r,t') \in \mathcal{Neg}}{f_G(h',r,t')}
\end{equation}
\begin{equation} \label{eqn:opt1_score_max}
    score_{max} := \argmax_{(h,r,t) \in \mathcal{KG}}{f_G(h,r,t)}
\end{equation}

The other option is showed in \autoref{fig:positives_negatives2}.
\autoref{fig:positives_negatives2}
\begin{figure*}[t]
  \centering
    \includegraphics[width=0.75\textwidth]{figures/positives_negatives2.PNG}
  \caption{Second option to define probabilities of triples to be positive ($\mathds{P}(y = 1 | (h,r,t))$) or negative ($\mathds{P}(y = 0 | (h,r,t))$). 
  $score_{min}$ at the left border of all positive triple scores and $score_{max}$ at the right border of all negative.}
  \label{fig:positives_negatives2}
\end{figure*}
In comparison to the first option, this one has the advantage of focusing only on the overlapping part of the model's prediction.
However, the drawback is that this static view becomes problematic when the scoring range of positive and negative triples does not overlap.
This option results in the following definitions of $score_{min}$ and $score_{max}$:
\begin{equation} \label{eqn:opt2_score_min}
    score_{min} := \argmin_{(h,r,t) \in \mathcal{KG}}{f_G(h,r,t)}
\end{equation}
\begin{equation} \label{eqn:opt2_score_max}
    score_{max} := \argmax_{(h',r,t') \in \mathcal{Neg}}{f_G(h',r,t')}
\end{equation}

Since $score_{min}$ and $score_{max}$ are now defined either with first option of 
\autoref{eqn:opt1_score_min} and \ref{eqn:opt1_score_max} or according the second option with 
\autoref{eqn:opt2_score_min} and \ref{eqn:opt2_score_max}, the probability of a triple to be positive ($y = 1$) can be calculated with the following \autoref{eqn:positive_probability}
\begin{equation}  \label{eqn:positive_probability}
    \mathds{P}(y = 1|(h, r, t)) =
    \begin{cases}
        0 \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  
        if \  f_G(h,r,t) < score_{min}
         
        \\ \\
        1 \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ 
        if \ f_G(h,r,t) > score_{max}
         
        \\ \\
        \frac{f_G(h,r,t) - score_{min}}{score_{max} - score_{min}}
        \ \ \ \ \ \ \ 
        else
        \\
    \end{cases}  \in [0, 1]
\end{equation}
and accordingly, since we have a binary classification problem for positive and negative triples, the probability of a triple to be negative ($y=0$) is
\begin{equation} \label{eqn:negative_probability}
    \mathds{P}(y = 0|(h, r, t)) := 1 - \mathds{P}(y = 1|(h, r, t)) \in [0,1]
\end{equation}

\section{Procedure of the Adversarial Training with Sampling by Uncertainty}
\label{sec:procedure}
In the original \kbgan approach two embedding models are available in form of the generator and discriminator.
Therefore, we could either sample by the uncertainty of the generator or the discriminator embedding model.
In the original \kban approach the negative triple set $Neg$ is created before they are given to the generator.
Subsequently they are sampled by probability distribution and given to discriminator.
Therefore, since it corresponds to the original course, we choose the uncertainty in the prediction of the generator and the original sampling by probabilities calculated by the scores of the generator is replaced by sampling from uncertainty (see \autoref{fig:uncertainty_sampling_architecture}).
\begin{figure*}[t]
  \centering
    \includegraphics[width=0.90\textwidth]{figures/architecture.png}
  \caption{Uncertainty Sampling replaces the original Random Sampling in \ac{KBGAN}. 
  New Uncertainty Sampling component is highlighted in green color.}
  \label{fig:uncertainty_sampling_architecture}
\end{figure*}
Accordingly, the causes a change in the adversarial learning procedure.
The input of the algorithm of our approach is like in original \ac{KBGAN} a pre-trained generator G with parameters $\theta_G$ and score function $f_G(h,r,t)$ and a pre-trained discriminator D with parameters $\theta_D$ and score function $f_D(h,r,t)$.
The training process can be described in the following steps:
\begin{enumerate}
    \item For each epoch in the training process a set of negative triples is created.
    This set, which is defined by $Neg(h,r,t)=\{(h_i',r,t_i')\}_{i=1\dots N_s}$, is created by uniformly randomly sampling $N_s$ negative triples by replacing head $h$ or tail entity $t$ for given positive triple $(h, r, t)$.
    
    \item 
    It receives all negative triples from $Neg$ set and the pretrained model calculates a score with score function $f_G$ for each of these negative triples.
    Based on the calculated score, negative triples can be distinguished from positive ones. Since the generator is a Tensor Factorization-Based Model, a higher score represents a higher likelihood to be a positive triple.
    
    \item 
    Based on the pretrained generator model, scores are calculated in the generator with scoring function $f_G$ for each negative triple $(h',r,t') \in Neg$.
    
    \item 
    All of these calculates scores from generator are handed to the Uncertainty Sampler to calculate the uncertainty of the model.
    For this, each negative triple in $Neg$ is classified first by assigning a probability $\mathds{P}(y = 1 | (h,r,t)) \forall (h,r,t) \in Neg$ with \autoref{eqn:positive_probability}.
    These probabilities are used to calculate uncertainty scores using function $u(l)$ defined in \autoref{eqn:uncertainty_function}.
    Accordingly, either the negative triple


    \item Subsequently, negative triples are sampled according to the calculated uncertainty scores, either the one with maximum uncertainty score or its distribution 
    and given to the discriminator.
    
    \item 
    From this step on, everything is the same as the original \ac{KBGAN} approach:
    The generated negative triple $(h',r,t')$ as well as the positive triple $(h, r, t)$ are sent to the discriminator.
    
    \item 
    The discriminator distinguishes both triples by given scoring function $f_D$, which is usually a translation-based \ac{KGE} model like \textsc{TransE} or \textsc{TransD}.
    
    \item 
    The reward defined by $r = - f_D(h',r,t')$ of the current negative triple is calculated and returned to the Generator and the next training iteration begins.
\end{enumerate}
These adversarial training steps are repeated until convergence, such that the generator improves the quality of sampled negative triples and discriminator improves embedding over time.
Output of our algorithm is the adversarially trained discriminator and its embedding.
By incorporating additional information from the graph and Sampling triples by uncertainty that have the most added value to the embedding model, we expect promising results.






%- presentation + reference to different scoring functions
%-> scoring functions are used for loss functions
%- different loss Functions for different scoring functions
%- learning process goal: decrease the loss
%-> a.k.a. increase margin between positives and negatives
%-> Show Figure which illustrates positive and negative triples + margin between them
%- examples:
%    - DistMult: Score of positive triples need to be higher than for negative triples
%    - TransE: score of negative need to be higher than for positive triples

%- mathematical explanation with definition of loss function and scoring function
%-> give an example of positive + negative where model is certain
%-> give an example of positive + negative where model in uncertain

%-> uncertainty according to scores which are returned by models
%- models include only implicit information about the structure etc
%- add additional information about structure, clusters, ...
%- ... feature functions








%- we need different classifiers, which indicate with which probability a given triple is positive or negative.
%- Either a classifier with its uncertainty, i.e. a probability around 0.5 of being either positive or negative.
%- or several classifiers, where the uncertainty can be recognized by a different classification of triples. 
%- Consequently, we need probabilities instead of simple scores to be able to calculate uncertainty.




    
%KGE: 
%- describe process of KGE learning embedding
%- score for each triple -> increase margin between positives and negatives

%either we have to
%- ... pretrain the models enough, because adversarial training takes a lot of time (epochs) to train and we want to enhance the models MRR and Hit10
%- ... train with adversarial approach as good as training in original models to avoid pretraining and directly train both models at the same time
%- ...




%\begin{figure*}[t]
%  \centering
%    \includegraphics[width=0.75\textwidth]{figures/uncertainty_in_svm.PNG}
%  \caption{Uncertainty in \ac{SVM} (based on \cite{human-in-the-loop}).}
%  \label{fig:uncertainty_in_svm}
%\end{figure*}


%In \autoref{fig:informativeinstances} the distinction between informative and hard negative examples is illustrated.
%\begin{figure*}[t]
%  \centering
%    \includegraphics[width=0.75\textwidth]{figures/informative_instances.PNG}
%  \caption{Example of Uncertainty Sampling for positive and negative instances. Simplified to two-dimensional embedding space d1 and d2 and borders b1 and b2 between positive and negative instances.}
%  \label{fig:informativeinstances}
%\end{figure*}
%For the sake of simplicity, positive and negative triples were only shown in a two-dimensional embedding space with dimensions d1 and d2, where positive triples were marked as green dots and negative triples as red dots.
%The embedding models sets the border between positive and negative triples at b1.
%For the approach of sampling hard negatives the embedding model would choose negative instance n1 because it is the closest one to the border b1 and therefore, it is the most difficult one to classify as positive or negative.