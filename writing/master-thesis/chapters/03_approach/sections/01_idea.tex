\section{Idea} 
\label{sec:idea}

% 1) Presentation of my idea
% - origin of my idea
% - Reasons for Uncertainty - advantages and expected results
% - Why did I choose KBGAN (Sampling Negatives from Dynamic Distribution) 
%   as basis for my approach
% - disadvantages of KBGAN that I want to solve
% - why uncertainty sampling can solve them?

% Refer to Problem section that negative triples from current sampling approaches are "too easy"
% refer to uncertainty sampling origin + give most informative samples to model where models learns the most
% + effective in other areas
% -> adopt idea to negative sampling in KGE models
As we have seen in \autoref{sec:training_techniques}, there are several ways to learn the \ac{KGE}.
Many embedding models learn by distinguishing positive triples from the \ac{KG} and negative triples generated by Negative Sampling.
Accordingly, although many models rely on Negative Sampling, most of these approaches provide only negative triples with insufficient quality (\autoref{sec:problem_analysis}).
With our approach, we aim to improve the Negative Sampling process for embedding models by incorporating uncertainty information and thus selecting negative triples that are more informative and more valuable for the embedding model.

% DEFINITION OF INFORMATIVE SAMPLES
Informative at this point means particularly interesting triples for the embedding model because those that are difficult to classify as positive or negative triple.
Therefore they help the embedding model the most to differentiate between negative and positive triples.
In many other Negative Sampling approaches, the goal is to create so-called hard negatives which have the highest probability to be a positive one.
In our approach, the result of the sampling process by uncertainty can also be hard negatives, but this need not necessarily be the case.
Instead of sampling the negative triple with the highest probability to be true, Uncertainty Sampling aims to sample the most informative negative triple, because it is difficult to distinguish between the positive and negative triples.

% why uncertainty sampling can solve current problems
% Uncertainty Sampling in other approaches -> results
Including uncertainty into various procedures and training models has already yielded promising results in recent work.
For example, this makes Active Learning more efficient by only querying particularly informative instances in the labeling process \autoref{sec:uncertaintysampling}.
But also in the world of uncertain \acp{KG}, confidence scores for positive triples have already been included in \acp{URGE} \cite{UKGE}.
Accordingly, we also want to optimize the Negative Sampling process by incorporating uncertainty information.

Among all the other Active learning query frameworks (\autoref{sec:uncertaintysampling}), Uncertainty Sampling is the most commonly used one from Active Learning \cite{Settles2009ActiveLL}.
In addition, it is easier to implement than some other frameworks, because, for example, it does not require to train multiple models as it is the case with the Query-by-Committee approach \cite{Settles2009ActiveLL}..
Nevertheless, Uncertainty Sampling provides promising results for many Active Learning scenarios and is therefore used for our approach to sample negative triples.

% + Why did I choose KBGAN (Sampling Negatives from Dynamic Distribution) 
The basic idea of Sampling by Uncertainty is to select particularly interesting negative triples from an existing set of negative triples.
Accordingly, any approach of the three categories Static Distribution-Based Sampling (\autoref{subsec:static_distribution_based_sampling}, Custom Cluster-Based Sampling (\autoref{subsec:custom_cluster_based_sampling}) and Dynamic Distribution-Based Sampling (\autoref{subsec:dynamic_distribution_based_sampling}) can be selected.
Since the dynamic distribution-based approach \kbgan already creates such a set of negative triples and capture the distribution of negative triples dynamically, we will extend this approach by uncertainty information.
Moreover, it specifically suffers from inefficient Sampling which slows down adversarial training.
For this reason, we aim to accelerate the learning process of \kbgan by selecting more informative triples for the negative triple set $Neg$ by calculating the uncertainty.

% what uncertainty?
% Option1: EBU: conflicting vs insufficient evidence (within one model)
% Option2: EAU epistemic <-> aleatoric uncertainty (among multiple models)
% CU: reducible and irreducible part of uncertainty in a prediction
In the literature we have several types of uncertainty at our disposal, based on which Negative Sampling can be performed.
As described in \autoref{sec:uncertaintysampling}, there are three different approaches of \ac{EBU}, \ac{EAU} and \ac{CU}.
These methods for calculating uncertainty are based on different viewpoints, but all of them are based on a classification problem of two or more classes.
In \ac{EBU} we consider the uncertainty within the model by looking at the individual features of the model and if they provide evidence for the classes.
This can be either very many (\textit{conflicting-evidence})  or only a few features (\textit{insufficient-evidence}) which are indicative for the respective classes.
In contrast, \ac{EAU} considers \textit{epistemic uncertainty} within a single model’s prediction, and \textit{aleatoric uncertainty} across multiple model predictions \cite{human-in-the-loop}.
Since \textit{credal uncertainty} differentiates between the
reducible and irreducible part of the uncertainty in a prediction and the domination of one class over the other, we calculate uncertainty among multiple models as well.
% for this model again have conflicting evidence or insufficient evidence for one class
% -> Firstly we focus on uncertainty within one model (generator kge model) -> can be insufficient or conflicting evidence
In the original \kbgan approach only one (embedding) model is available in form of the generator.
Furthermore, the \kbgan approach does not provide any additional features for the sampling process, only the scores of the negative triple and the sampling probabilities calculated from them.
For this reason, we will first take the point of view of epistemic uncertainty from \ac{EAU}.
If we add further models for a classification between negative and positive triples in the later process or add additional features, the other viewpoints aleatoric uncertainty, \ac{EAU} and \ac{CU} can be taken as well.

% uncertainty sampling approaches require a classification problem
% we can either: 
% 1) good <-> bad negative triple (like original approach)
% 2) provide prob of triple to be true 
Next, we need to determine the classification problem for Uncertainty Sampling, which gives us two options:
First, we could distinguish between "good" and "bad" negative triples and try to provide the best ones to the discriminator.
Since the generator of the adversarial training is a tensor factorization-based model, it provides scores which indicate plausibility of a triple.
According to this plausibility scores a sampling distribution among all negative triples is created. 
This has the advantage that we only have to calculate the scores of negative triples, but we lose the information about positive triples and their scores.
Here, as in the original \kbgan approach, negative triples can be sampled from the calculated scores with linearly increasing probability for better negative triples, or as in \cite{UKGE} with logistic function or a bounded rectifier. 
Since \kbgan follows the open-world assumption and therefore reduces the probability of sampling false-negatives by limiting the number of negative triples to twenty \cite{cai2017kbgan}.

Another approach would be to distinguish between negative and positive triples and their scores.
This would have the advantage that also information about positive triples and their scores is preserved.
In contrast to the first option this one has that also information about positive triples and their scores are preserved.
If we define the classes $y = 0$  for negative triples and $y = 1$ for positive triples, we can classify each triple $(h,r,t)$ in a binary classification problem.
Accordingly, we can calculate the probability of being either a negative or a positive triple for each triple $\mathbb{P}(y| (h,r,t)) \in [0,1]$ for $y \in \{0,1\}$.
Since we want to have as good negative triples as possible, we look for negative triples $(h',r,t')$ close to the probability $\mathbb{P}(y = 0| (h',r,t')) = \mathbb{P}(y = 1| (h',r,t')) = 0.5$, which means that they can be assigned to both the positive and the negative class.

The difference between the two approaches can be explained by the following example and \autoref{fig:uncertainty}, which illustrates scoring values of positive and negative triples and the uncertainty of the model´s prediction.
\begin{figure*}[t]
  \centering
    \includegraphics[width=0.75\textwidth]{figures/uncertainty.PNG}
  \caption{Score range of positive and negative triples.
  The uncertainty of the model is in the scoring range where the model found scores of both positive and negative triples.}
  \label{fig:uncertainty}
\end{figure*}
Suppose we have the positive triple $l_p=(h,r,t)$ (t marked as blue dot) in our KG and some other entities from  $t^*$ (green dots) from positive triples $(h,r,t^*)$ with $t^* \in \entities$ from the \ac{KG} .
After we obtain the negative triple set $Neg$ through the Negative Sampling process and scored by the generator embedding model, also a range of scoring values for negative triples exists. 
Therefore, for entities within the overlap of positive and negative scoring values, the model is unsure whether they are positive or negative.
The first sampling approach like \kbgan assigns now ascending probabilities for increasing scoring values.
Therefore, it is most likely to sample $t'_3$, because the negative triple $(h,r,t'_3)$ achieves the highest score and, accordingly, has the highest probability to be positive.
Although, the entities $t'_1$ and $t'_2$ are also very interesting for the model, because a classification based on the achieved scoring value is not easy.
Consequently, instead of sampling negative by differentiating between "good" and "bad" negative triples, the second approach incorporates information of the positive triples and there, distinguishes not only between "good" and "bad", but also between classes positive and negative.
Thus, this more detailed distinction allows us to avoid sampling false negative triples, since, for example, sampling $t'_3$ can be particularly problematic if it yields an even higher score and should actually be classified as a positive triple because it is a false negative triple.

For the implementation of the second approach, the procedure is as follows.
At first, for each triple $l = (h, r, t)$ a plausibility score $f_G(l)$ is calculated by the generator embedding model. 
Subsequently, the uncertainty scores are needed to perform the Negative Sampling by uncertainty.
For this a transformation function $\phi$ is needed which maps $f_G(l)$ to an uncertainty score $u(l)$ .
\begin{equation}
    u(l) = \phi(f_G(l)), \phi: \mathbb{R} \rightarrow [0,1]
\end{equation}
For this transformation function we have several uncertainty metrics at our disposal which are Entropy, Least Confident, Margin of Confidence and Ratio of Confidence, which calculate uncertainty score $u(l) \in [0, 1]$ for probabilities $\mathbb{P}(y = 1 | (h,r,t))$ (\autoref{fig:sampling_distributions}).
\begin{figure}
    \centering
    \begin{minipage}{.5\textwidth}
      \centering
      \includegraphics[width=0.9\linewidth]{figures/entropy_graph.PNG}
    \end{minipage}%
    \begin{minipage}{.5\textwidth}
      \centering
      \includegraphics[width=0.9\linewidth]{figures/least_confident_graph.PNG}
    \end{minipage}
    \begin{minipage}{.5\textwidth}
      \centering
      \includegraphics[width=0.9\linewidth]{figures/smallest_margin_graph.PNG}
    \end{minipage}%
    \begin{minipage}{.5\textwidth}
      \centering
      \includegraphics[width=0.9\linewidth]{figures/smallest_ratio.PNG}
    \end{minipage}%
    \caption{Uncertainty Sampling Distributions for (a) Entropy, (b) Least Confident, (c) Margin of Confidence and (d) Ratio of Confidence metrics.
    The x-axis marks the probability of a triple belonging to the positive class $\mathbb{P}(y = 1 | (h,r,t))$, the y-axis the result of the corresponding metric.}
    \label{fig:sampling_distributions}
\end{figure}
Accordingly, for the different metrics $\phi$ can be defined as follows:
\paragraph{\textbf{Entropy.}}
\begin{equation}
    \phi(x) = - \sum_{y \in \mathcal{Y}}{p_{\theta}(y | x) \cdot log p_{\theta}(y|x)}
\end{equation}

\paragraph{\textbf{Least Confidence.}}
\begin{equation}
    \phi(x) = 1 - \max_{y \in \mathcal{Y}}{p_{\theta}(y | x)}
\end{equation}

\paragraph{\textbf{Margin of Confidence.}}
\begin{equation}
    \phi(x) = p_{\theta}(y_m | x) - p_{\theta}(y_n|x)
\end{equation}
where
$y_m = \argmax_{y \in \mathcal{Y}} p_{\theta}(y | x)$ 
and 
$y_n = \argmax_{y \in \mathcal{Y} \setminus y_m}{p_{\theta}(y | x)}$
    
\paragraph{\textbf{Ratio of Confidence.}}
\begin{equation}
    \phi(x) = \frac{p_{\theta}(y_m | x)}{p_{\theta}(y_n|x)} 
\end{equation}



-> all have to be maximized





-> for this approach it makes sense to sample more negatives, since original idea was to not sample false negatives, now we would not sample them
=> according to them in a KG are far more true negatives than false negatives and therefore, it is unlikely for Neg to contain any false negatives, and the negative


--> 2 follows more the open-world assumption => missing triples in the KG can also be positive!




% ->show figure
% borders/ areas of negatives and positives will change over time

% use borders of negatives/positives for classification
- we need classification of positive and negatives for uncertainty sampling
- how measure positive/negative triples:
- borders with minimum positive score and maximum negative score
- for uncertainty sampling, we need a classification Problem: Binary Classification: positive or negative triple
- classifier may not be sure which class is correct -> classifier is uncertain
- either: uncertainty of Generator or discriminator:
-> Generator: corresponds to original process flow
-> in Uncertainty Sampling of Active Learning, sampled instances are annotated by human
-> in our case, uncertain Generator "asks" discriminator how to label it
-> because negative and original positive triple are given to discriminator, it can be labeled and rewarded, if it was difficult to classify

1) How is the probability of a triple to be positive calculated? 
- scores of negatives/positives from generator
-> how to map plausibility score to uncertainty/confidence score?
uncertainty = uncertainty of a given triple to be true

- feature functions (POP, FRQ, PEER, PIVO + more)
- other pretrained embedding models

This is used to classify 0 negative and 1 positive.

% left = prob 0, right prob 1 to be positive
% either set borders 
% 1) min negative + max positive
% 2) min positive + max negative 
% if we have defined classification, we can calculate prob of each triple to be true
% how to measure uncertainty of generator in our case?
% different measurements at our disposal



% -> show and describe structure of figures
% therefore, we can calculate uncertainty score for each negative triple
% two options: 
% 1) sample always the maximum -> all the same
% 2) sample from distribution based on uncertainty scores
% -> different
- maximum uncertainty score
- distribution from uncertainty scores











% NOTES:
- BEFORE: probability distribution, highest probability for negative triple which is likely a positive
-> results: 
    - linear, also negatives with very low score/unlikely to be positive get a probability to be sampled 
    - negatives with score higher than positives are sampled -> false negatives?
- show Figure with scores of positive and negative triples:
-> question: Which Negatives should be sampled to train the discriminator
- left ones are bad negatives -> low probability to be sampled
- right ones are close to positive scores -> may be even a false negative triple ?

    
    
    
    
    
    
    
    
    
    
KGE: 
- describe process of KGE learning embedding
- score for each triple -> increase margin between positives and negatives
-

either we have to
- ... pretrain the models enough, because adversarial training takes a lot of time (epochs) to train and we want to enhance the models MRR and Hit10
- ... train with adversarial approach as good as training in original models to avoid pretraining and directly train both models at the same time
- ...

- to the best of our knowledge, there was no previous work on sampling negatives by uncertainty for KGE




\begin{figure*}[t]
  \centering
    \includegraphics[width=0.75\textwidth]{figures/uncertainty_in_svm.PNG}
  \caption{Uncertainty in \ac{SVM} (based on \cite{human-in-the-loop}).}
  \label{fig:uncertainty_in_svm}
\end{figure*}




%In \autoref{fig:informativeinstances} the distinction between informative and hard negative examples is illustrated.
%\begin{figure*}[t]
%  \centering
%    \includegraphics[width=0.75\textwidth]{figures/informative_instances.PNG}
%  \caption{Example of Uncertainty Sampling for positive and negative instances. Simplified to two-dimensional embedding space d1 and d2 and borders b1 and b2 between positive and negative instances.}
%  \label{fig:informativeinstances}
%\end{figure*}
%For the sake of simplicity, positive and negative triples were only shown in a two-dimensional embedding space with dimensions d1 and d2, where positive triples were marked as green dots and negative triples as red dots.
%The embedding models sets the border between positive and negative triples at b1.
%For the approach of sampling hard negatives the embedding model would choose negative instance n1 because it is the closest one to the border b1 and therefore, it is the most difficult one to classify as positive or negative.