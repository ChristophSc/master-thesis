\section{Idea} 
\label{sec:idea}

% 1) Presentation of my idea
% - origin of my idea
% - Reasons for Uncertainty - advantages and expected results
% - Why did I choose KBGAN (Sampling Negatives from Dynamic Distribution) 
%   as basis for my approach
% - disadvantages of KBGAN that I want to solve
% - why uncertainty sampling can solve them


% origin of my idea
With our approach, we aim to improve the training process of embedding models by incorporating more information into the Negative Sampling process and thus selecting negative examples that are more informative and more valuable for the embedding model.
This is achieved by including Uncertainty Sampling in state-of-the-art approaches.
Accordingly, different approaches from the three categories of Static Distribution-Based Sampling (\autoref{subsec:static_distribution_based_sampling}, Custom Cluster-Based Sampling (\autoref{subsec:custom_cluster_based_sampling}) and Dynamic Distribution-Based Sampling (\autoref{subsec:dynamic_distribution_based_sampling}) are available. The basic idea of Sampling by Uncertainty is to select particularly interesting negative triples from an existing set of negatives.
Which are exactly the negative triples that are most informative for the KGE model.
Thus, it is irrelevant by which of the three negative sampling methods (\autoref{sec:uncertaintysampling}) the negative triples originate.

% Why did I choose KBGAN (Sampling Negatives from Dynamic Distribution) 
Since in the pioneer of the dynamic distribution-based sampling method KBGAN exactly such a principle is present to select from an existing set of negative triples, we will first implement our idea based on this approach.
Moreover, it specifically suffers from inefficient Sampling which slows down adversarial training.
For this reason, we aim to accelerate the learning process of KBGAN by selecting more informative triples for the negative triple set $Neg$.
But what does informative mean at this point?

% DEFINITION OF INFORMATIVE SAMPLES
Informative means particularly interesting triples for the embedding model because those that are difficult to classify as positive or negative triple.
Therefore they help the embedding model the most to differentiate between negative and positive triples.
In many other negative sampling approaches, the goal is to create so-called hard negative examples and use them for the training process of the embedding model. 
In our approach, the result of the sampling process can also be hard negative example, but this need not necessarily be the case.
In \autoref{fig:informativeinstances} the distinction between informative and hard negative examples is illustrated.
\begin{figure*}[t]
  \centering
    \includegraphics[width=0.75\textwidth]{figures/informative_instances.PNG}
  \caption{Example of Uncertainty Sampling for positive and negative instances. Simplified to two-dimensional embedding space d1 and d2 and borders b1 and b2 between positive and negative instances.}
  \label{fig:informativeinstances}
\end{figure*}
\begin{figure*}[t]
  \centering
    \includegraphics[width=0.75\textwidth]{figures/uncertainty_in_svm.PNG}
  \caption{Uncertainty in SVM (based on \cite{human-in-the-loop}).}
  \label{fig:uncertainty_in_svm}
\end{figure*}
For simplicity, positive and negative instances are shown here in two-dimensional embedding space with dimensions d1 and d2.
The embedding models sets the border between positive and negative triples at b1.
For the approach of sampling hard negatives the embedding model would choose negative instance n1 because it is the closest one to the border b1 and therefore, it is the most difficult one to classify as positive or negative.
Instead, Uncertainty Sampling aims to sample the most informative negative triple.
But what is the most informative triple in this case for an embedding model?
The most informative triple is the one that the model helps the most to distinguish between positive and negative triples.
For this reason, the negative triple n2 would be much more informative, since its classification would indicate whether the boundary between positive and negative triples should rather be set at b1 or at b2.
Thus, by setting the new boundary b2 would contribute to a more accurate embedding of the data.

% why uncertainty sampling can solve current problems




Uncertainty Sampling is the most commonly used query framework among all the other strategies frameworks (\autoref{sec:uncertaintysampling}) from Active Learning \cite{Settles2009ActiveLL}.
In addition, it is easier to implement than some other frameworks, because, for example, it does not require to train multiple models as is the case with the Query-by-Committee approach.
Nevertheless, Uncertainty Sampling provides promising results for many Active Learning scenarios and is therefore also used for our approach to sample negative triples.



% NOTES:
- for uncertainty sampling, we need a classification Problem: Binary Classification: positive or negative triple
-> differentiation to KBGAN: Discriminator is no classifier like in original GANs
- if it is a classifier: either it is positive or negative (ADD VISUALIZATION)
- classifier may not be sure which class is correct -> classifier is uncertain
- either: uncertainty of Generator or discriminator:
-> Generator: corresponds to original process flow
-> in Uncertainty Sampling of Active Learning, sampled instances are annotated by human
-> in our case, uncertain Generator "asks" discriminator how to label it
-> because negative and original positive triple are given to discriminator, it can be labeled and rewarded, if it was difficult to classify
- BEFORE: probability distribution, highest probability for negative triple which is likely a positive
-> results: 
    - linear, also negatives with very low score/unlikely to be positive get a probability to be sampled 
    - negatives with score higher than positives are sampled -> false negatives?
    - 
- show Figure with scores of positive and negative triples:
-> question: Which Negatives should be sampled to train the discriminator
- left ones are bad negatives -> low probability to be sampled
- right ones are close to positive scores -> may be even a false negative triple ?
- before we want to find the most informative triples, we need a pool of negatives from which to sample
-  
    
KGE: 
- describe process of KGE learning embedding
- score for each triple -> increase margin between positives and negatives
- 

