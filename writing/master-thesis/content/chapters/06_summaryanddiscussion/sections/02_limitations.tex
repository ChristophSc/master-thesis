\section{Limitations}  
\label{sec:limitations}
%
% datasets
Our approach of \usgan is based on \kbgan and works with all datasets from the original approach.
Even if it was not tested on other datasets, it should work with all \acp{KG} which have stored triples in form of $(h,r,t)$.

% other negative sampling types
Furthermore, the idea of extending negative sampling by uncertainty information should work with other negative sampling approaches as well, but need to be adjusted then.
\kbgan originally generated a set of negative triples where one is sampled and given to the discriminator.
Since we already have this set available, our approach extends negative triples with uncertainty scores.
Otherwise, if this set of negative triples does not exist in other approaches, it has to be created.
Therefore, extending negative sampling with uncertainty information is not only possible for Dynamic Distribution-based Negative Sampling methods such as \kbgan, but it should also work with other approaches from Static Distribution-Based or Custom Cluster-Based Sampling as well.

% KGE models
As in \kbgan, the generator and discriminator can theoretically be any \ac{KGE} model, but since our approach is based on \kbgan, the generator needs to be a tensor-factorization \ac{KGE} model where a high score indicates a high probability of a triple to be true and the discriminator needs to be a distance-based \ac{KGE} model.
Otherwise, our approach, as well as \textsc{KBGAN}, has to be adjusted.

% uncertainty types
For providing uncertainty information in the negative sample, we used the most common uncertainty measures from uncertainty sampling of active learning:
entropy, least confidence, the margin of confidence, and the ratio of confidence.
Other approaches which work with uncertainty conduct additional, other uncertainty measures.
For example, \cite{UKGE} uses a logistic function and bounded rectifier which maps scores from \ac{KGE} models to confidence scores.
Depending on how many models and which information are available, also other approaches of uncertainty sampling from active learning such as credal uncertainty or \ac{EBU} can be taken into account.
Therefore, further research on including uncertainty information in the negative sampling process is required.

% neural network-based KGE models mentioned in proposal, refer to this here and give a reason, why it was not implemented.