\section{Summary and Conclusion}  
\label{sec:summary_and_conclusion}
% summary
\acp{KG} contain a large collection of information that is stored in the form of triples of entities and relations.
To get around the problem of the sparseness of data, \ac{KGE} were proposed to embed entities and relations in a low-dimensional vector space which contains all semantic information.
To learn these embeddings, positive and negative instances are contrasted.
Thus, for the sake of space efficiency, \acp{KG} contain only positive information which leads to a variety of options to select negative ones.
Most negative sampling methods are trivial and, for example, randomly replace head or tail entity in a given positive triple.
These sampling methods usually result in sampled negative triples which are easy to distinguish from positive ones and thus do not help the \ac{KGE} model much in learning the embeddings. 
Therefore, new and more promising negative sampling methods are required.

% summary -> uncertainty 
Since including uncertainty information produces promising results in other machine learning areas like uncertainty sampling in active learning, we included uncertainty information in a negative sampling process.
Our approach of \usgan is based on \kbgan, which dynamically captures the distribution of negative samples and samples one negative triple from a given negative triple set.
\usgan extends this sampling process by adding uncertainty information to each negative triple.
Therefore, we implemented four different uncertainty metrics: entropy, least confidence, confidence margin, and confidence ratio.
Based on a classification between positive and negative triples, an uncertainty score is calculated for each negative triple and sampled according to probability distribution given by sampling method \ussoftmax or \usmax.
To analyze all sampling methods and uncertainty metrics in \usgan, we evaluated them on four different datasets and compared the results with the original \kbgan sampling approach 


% conclusion
In conclusion, our evaluation of \usgan with different uncertainty metrics, sampling methods, and different datasets show that it is possible to learn embeddings with the uncertainty sampling method.
In most cases, adding additional uncertainty information to the negative sampling process leads to an improvement of the original \ac{KGE} models.
For some cases, \usgan increased the learning curve of MRR and Hit@10 values for some data sets and seem to perform well, especially on smaller datasets like \umls and outperforms \kbgan.
Additionally, it was established that \usgan is very dependent on the underlying dataset and hyperparameter tuning since training sometimes behaves completely differently.
For example, on \textsc{WN18} rewards are slightly increasing over time, but in contrast on \textsc{FB15k-237}, they are decreasing.
Further research on generator scores during training showed that MRR and Hit@10 values are increasing, but are far away from the accuracy of the model alone on the current dataset.
This results from the fact, that the goal of the generator model is not to find a good embedding anymore but to find negative triples which are hard to distinguish from positive ones for the discriminator model.
Therefore, high scores for negative triples do not necessarily indicate high plausibility for a positive triple.
Consequently, these effects range from positive and negative triple scores and lead to inaccuracy of classification which is based on generator scores.

Thus, it can be said that in most cases the results are still not as good as those of original sampling in \kbgan which might have several reasons.
These are marked for future work explained in \Autoref{sec:futurework}.