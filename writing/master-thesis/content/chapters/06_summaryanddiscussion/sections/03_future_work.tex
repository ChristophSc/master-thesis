\section{Future Work} 
\label{sec:futurework}

In some cases, the enrichment of negative sampling process with uncertainty information in \kbgan has proven to be promising and thus represents another alternative.
Nevertheless, there is some room for improvement, which we leave to future work.
These can be separated in following aspects.

\subsubsection{Extend \usgan with other Approaches}

\usgan is based on \kbgan which is one of the pioneer works in dynamic distribution-based and presented in 2017.
Therefore, other approaches based on \kbgan were already presented which e.g. extending the approach with a cache like in \textsc{NSCaching} \cite{zhang2019nscaching}.
The idea of extending negative sampling by uncertainty information can also be applied for these approaches.

\subsubsection{Improve Classification}
Uncertainty scores are currently calculated based on a classification of positives and negative which is motivated by uncertainty sampling in active learning.
In turn, the classification is only based of generator model scores which might not be accurate enough.
Therefore, improving classification with several information about \ac{KG} structure, entities and relations of a \ac{KG} might improve classification of positive and negative examples and, therefore, uncertainty sampling.

\subsubsection{Edit Definition of Uncertainty}
Currently, motivated by uncertainty sampling from active learning, uncertainty is based on classification of triples as positive or negative ones.
Since there are several types of uncertainty available, an alternative definition of uncertainty might lead to further improvements and more effective negative sampling.
For example, the uncertainty can also be defined in sampling, i.e. how uncertain the sampler is whether sampling a triple will lead to good results.
Therefore, for a big set of negative triples uncertainty scores are assigned and if a triple was sampled in the past, uncertainty score can be decreased.

This idea can be extended by combining calculated uncertainty score with original sampling probability based on generator model scores.
Then, the combination of both leads leads to a new sampling probability distribution.
Sampling using this combination would lead to an amplification of the sampling variation.
Since in the original approach the sampling probability is based on generator score, it is very unlikely to ever sampling a negative triples which achieves a low score in the beginning.
Therefore, even if it might lead to a low distance in discriminator and consequently small marginal loss, it will never be sampled.
If uncertainties and probabilities based on scores are combined, i.e. in a multiplicative way, never sampled triples get a low generator score but high uncertainty score, triples which are sampled very often achieve a high generator score but a low uncertainty.
Therefore, both of them have the chance to be sampled.


