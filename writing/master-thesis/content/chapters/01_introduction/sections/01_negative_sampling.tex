\section{Negative Sampling}
\label{sec:negative_sampling}

Negative sampling was originally used for neuronal probabilistic language models and was referred to as importance sampling \cite{qiannegative, qianunderstanding}.
Mikolov et al. \cite{MikolovSCCD13} refer to negative sampling as a simplified version of \ac{NCE} to overcome the computational difficulty which was associated with probabilistic language models \cite{qianunderstanding}.
This was due to their partition functions summing over all words which is expensive if the amount of vocabularies is high \cite{qianunderstanding}.
In comparison, negative sampling transforms the difficult density estimation problem is transformed into a binary classification problem, where true samples are distinguished from noise samples to simplify the computation as well as accelerating training \cite{qianunderstanding}.
Originally, the partition function was normalized into a probability distribution based on the entire vocabulary.
Instead, true samples from a \ac{KG} are separated from noise distribution samples to asymptotically estimate the true distribution, which is highly efficient with low computational cost \cite{qianunderstanding}.

If we consider nodes as words and neighbors of nodes as the context of a word, graph representation learning of \acp{KG} is similar to language modeling \cite{qianunderstanding}.
Therefore, the idea was applied to \ac{KGE} learning and has become common practice over the past several years.
Instead of discriminating true samples from noise samples, in \acp{KG} positive triples are discriminated from negative ones which are generated by perturbing positive triples.
Since bad or too obviously incorrect negative triples fail to capture the latent semantics in a \ac{KG} and lead to a zero loss problem, several negative sampling methods have been developed and used in different \ac{KGE} models \cite{qiannegative}.
Since a high quality of generated negative triples ensures successful training and the learned embedding performs better in downstream tasks, negative sampling becomes a very important part in \ac{KGE} learning.


However, since there are only positive triples in a \ac{KG} and thus no information is available about the distribution of negative triples or their quality, negative sampling is a challenging task for embedding models.
There are two different ways of looking at \acp{KG} at hand for negative sampling:
Negative triples can be generated either under the \textbf{\ac{CWA}} or the \textbf{\ac{OWA}} \cite{qiannegative}.
While the former assumes that unobserved facts which are not in a \ac{KG} are false, the latter states that asserted statements are true, while the missing ones are simply unknown \cite{arnaout2020enriching, qiannegative}.
Negative sampling under the simplified \ac{CWA} does not present much difficulty, as it is easy to generate a large number of triples that are not in a \ac{KG}.
However, the quality of the negative triples may suffer if it is indeed a false negative triple.
The \ac{OWA}, on the other hand, has the two main drawbacks of worse performance in downstream tasks and scalability issues due to a large number of negative samples \cite{qiannegative}.
Therefore, generating qualitative negative triples by negative sampling with good performance represents a non-trivial step in \ac{KGE} learning.
Especially, because the quality of these generated negative triples has a direct impact on the \acp{KGE} and thus all further downstream tasks \cite{qiannegative}.

% DESCRIBE HOW NEGATIVE SAMPLING WORKS IN GENERAL
Overall, there are several variations for embedding learning with negative sampling in the literature.
However, they have the following steps in common:
At first, the negative sampling process generates negative triples by, for example, perturbing positive ones.
This can be done either randomly or by including more information about entities, relations of the \ac{KG} or even the whole \ac{KG}.
Subsequently, the generated negative triples are given to the embedding model, which distinguishes the positive from the negative triple.
Accordingly, the embedding learns particularly many from good negative samples, since its differentiation from the positive is difficult and thus the learning factor, i.e. the gradient of the model is particularly high.
However, the effective generation of negative triples to improve embedding involves some problems, which are analyzed in more detail in the following section.




