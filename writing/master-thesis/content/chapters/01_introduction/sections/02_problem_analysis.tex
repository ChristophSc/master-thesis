\section{Problem Analysis}
\label{sec:problem_analysis}
% 
Negative sampling plays an important, beneficial, and standard role in embedding learning \cite{qiannegative}.
However, since no negative triples are stored in a \ac{KG} and they have to be generated with negative sampling, which causes some problems.
On one side a negative sampling method must have high performance to not significantly influence the training time of embedding learning, on the other side sampled negative triples must present high-quality \cite{qiannegative}.
While better-quality negative triples facilitate the training process, low-quality false negative triples are unable to capture the latent semantics of a \ac{KG} \cite{qiannegative}.
Since they are too easily distinguishable for a \ac{KGE} model, they cause the zero loss problem \cite{qiannegative}.
As a result, embedding models do not learn or learn less from the provided negative triples and therefore, they do not improve the embedding and suffer from the vanishing gradient problem \cite{zhang2021efficient}. % or biased estimation problem
This problem occurs when the gradient becomes very small and thus prevents the model from further learning or even stops the training completely.
In the case of KGE learning, this is the case when triples can be easily classified \cite{zhang2021efficient}.
This classification is based on the achieved scoring values of a triple.
Depending on the KGE model, positive triples get high scores and negative triples get low scores or vice versa.
Due to simplicity and efficiency, most negative sampling approaches, create very poor, i.e. too easily classifiable negative triples.
The reason for this is that the scoring values of positive and negative triples differ greatly, which facilitates classification.
One very performant, simple, and commonly used approach for negative sampling is 
Uniform Negative Random Sampling.

% UNIFORM RANDOM SAMPLING
In uniform negative random sampling either the head or the tail entity in a given positive triple \triple{h}{r}{t} is randomly replaced by any other entity of the \ac{KG} which remains in the new negative triple \triple{h’}{r}{t} or \triple{h}{r}{t’}. 
Therefore, it is very likely to pick an entity that results in a zero gradient because the negative triple can be easily discriminated from the positive one \cite{cai2017kbgan}.
For example, by replacing the head entity of the positive triple \triple{h}{r}{t} = \triple{Joe Biden}{bornIn}{USA} with head entity \texttt{h'} = \texttt{Paderborn} would result in the negative triple \triple{h'}{r}{t} =  \triple{Paderborn}{bornIn}{USA} which is not very informative for the embedding.
Simply replacing the randomly selected head or tail entity of an again randomly selected entity of the \ac{KG} does not use any further information.
In comparison, it would have been useful if either negative sampling had recognized that the head entity \texttt{Joe Biden} is a person and replaced it with another person.
Moreover, recognizing that the tail entity, as well as the sampled entity \texttt{Paderborn}, is a location and its replacement would have led to the much more meaningful negative triple \triple{Joe Biden}{bornIn}{Paderborn}.  
Thus, while this approach is a fast and effective way to generate negative triples, it leads to a low learning factor in the embedding model.

% BERNOULLI SAMPLING
More useful negative examples are created by Bernoulli Sampling, which notes more information about a \ac{KG} and its individual entities and relations.
In comparison to Uniform Negative Random Sampling, it considers one-to-many, many-to-one and many-to-many relation types between entities \cite{zhang2021efficient}.
These relation types are an indicator for the sampling approach if it is better to replace the head or the tail entity.
From the example above, it would have been recognized that the relation \texttt{bornIn} is a many-to-one relation.
Therefore, the head entity cannot have this relation to multiple entities, making each replaced tail entity a more useful negative triple.
Even though this is still a very fast and effective way to create negative triples, they are still easy to distinguish from positive ones.

% OTHER INFORMATION USED
In addition to these most commonly used methods, there are others that leverage external constraints such as entity types.
However, this resource does not always exist or is accessible \cite{cai2017kbgan}.
Instead of sampling from all entities in a \ac{KG}, other negative sampling methods take the approach of sampling only from a handful of selected entities.
For example, by sampling entities within the same domain, they hope to increase efficiency \cite{qiannegative}.
However, due to the rapid growth and frequent updating of \acp{KG}, constantly updating custom clusters is essential and skilled \cite{qiannegative}. 
Additionally, the creation of subsets of entities leads to a degradation of sampling performance and the information needed for this is not always available or is very difficult to derive from a \ac{KG}.


%Many of these approaches aim to find hard negative examples that are close to positive facts from a given \ac{KG} and thus have a positive effect on the embedding learning process.

% CHALLENGES
%Consequently, there are two main challenges for sampling negative triples \cite{zhang2021efficient}:
%At first, it is necessary to capture and model the dynamic distribution if negative triples to sample informative and useful negative triples with high gradients which help the model during the embedding learning process.
%Secondly, these negative triples have to be sampled effectively  so that the Negative Sampling does not negatively affect the performance of embedding learning.
 
 
 
%This problem occurs when distinguishing between positive and negative triples.
%A \ac{KGE} model has a scoring function for evaluating triples and embeddings for entities and relations in a \ac{KG}.
%When distinguishing between positive and negative triples, the scoring values of these two triples are compared.
%For this comparison, two fundamentally different cases can occur.
%At first, the two scoring values differ significantly.
%Therefore, the sampled negative triple was easy distinguishable from the positive one which results in a ...
% Secondly, both scoring values are very close.
% In this case the sampled negative triple was a good one.