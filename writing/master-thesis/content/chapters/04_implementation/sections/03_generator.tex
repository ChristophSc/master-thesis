\section{Generator}
\label{sec:generator}

The adversarial training starts via the script \textit{gan\_train.py}.
Before the training begings begins, the configurations are loaded from the \textit{config.yaml} file.
The settings and parameters of the training can be defined in this file.
One example config is showed in \Autoref{lst:config_yaml}.
\input{content/chapters/04_implementation/config_yaml}
The settings under adv contain the most important settings for the adversarial training, such as the definition of the generator and discriminator model.
These settings can also be overwritten via the parameters when starting the script.
Then, training, validation and test data are loaded and the adversarial training for the specified number of epochs begins.

The adversarial training runs via the \texttt{BaseModel} class depicted in \Autoref{fig:basemodel_classdiagram}.
\begin{figure*}[t]
  \centering
    \includegraphics[width=0.95\textwidth]{figures/BaseModel.png}
  \caption{\ac{UML}-Classdiagram of \texttt{BaseModel} and inherited Models  \texttt{DistMult},  \texttt{ComplEx},  \texttt{TransE} and  \texttt{TransD}}
  \label{fig:basemodel_classdiagram}
\end{figure*}
It implements methods for loading and saving the learned models in specified files, a \texttt{pretrain} method which is overwritten by the inherited methods and a function called \texttt{test\_link} which performs an evaluation for a given data set and calculates the MRR and Hit@10.
However, the most important methods of this class are \texttt{gen\_step} and \texttt{dis\_step}, which represent an iteration step of the generator and the discriminator respectively.
The two methods are run with the generator and discriminator models set.
For the generator these are the \texttt{DistMultModel} or the \texttt{ComplExModel} and for the discriminator the \texttt{TransEModel} and the \texttt{TransDModel}.
Consequently, the scores of the negative triples are calculated in the \texttt{gen\_step} and one of the negative triples is sampled.
This sampled negative triple and the underlying positive triple are then passed to the \texttt{dis\_step} function.

The individual models all have a variable \texttt{mdl}, which is an object of the class \texttt{BaseModule} and its inheriting classes.
This and inherited classes are depicted in \Autoref{fig:basemodule_classdiagram}
\begin{figure*}[t]
  \centering
    \includegraphics[width=0.95\textwidth]{figures/BaseModule.PNG}
  \caption{\ac{UML}-Classdiagram of \texttt{BaseModule} which inherits from the class \texttt{Module} from the package \texttt{torch.nn}.
  For each implemented model for generator and discriminator there inherited Modules \texttt{DistMultModule},  \texttt{ComplExModule},  \texttt{TransEModule} and  \texttt{TransDModule}}
  \label{fig:basemodule_classdiagram}
\end{figure*}
These thus represent a \texttt{torch.nn.Module} and contain the embeddings for entities and relations.
Each class implements a \texttt{score} function which calculates a score for a given triple.
The current embeddings of the entities and relations are also reflected in the local variables of the \texttt{torch.nn.Embedding} type.
For the calculation of the losses during training a \texttt{softmax\_loss} method was implemented for the generator models and a \texttt{pair\_loss} method for the discriminator models.
The forward functions are inherited from the class \texttt{torch.nn.Module}, which defines the computation performed at every call.
This represents a simple call to the respective scoring function.

By implementing these models and passing negative and positive triples, it is now possible to perform an adversarial training.
Originally, the sampling of a negative triple was carried out in the generator.
Since in our approach there is now an extension of the original model to include Sampling by Uncertainty, we have outsourced this functionality to additional classes.
Sampling is now done by an additional class \texttt{BaseSampler}, of which our \texttt{BaseModel} has an instance named \texttt{smpl}.
% --> next section: Sampler
