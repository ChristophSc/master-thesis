\textbf{FB15k-237 Dataset}
\label{subsubsec:uncertainty_fb15k237}\\
%
If we take a closer look at learning curves in \textsc{FB15k-237}, \ussoftmax looks more promising again.
With both sampling methods MRR as well as Hit@10 values increase over time until the end of the training of 1000 epochs.
As depicted in \Autoref{fig:advtrain_fb15k237_random_vs_uncertainty}, with \origsampling a maximum MRR almost 26\% and a Hit@10 pf about 43\%.
\begin{figure}[H]
    \centering
    \begin{minipage}{.5\textwidth}
      \centering
      \includegraphics[width=0.9\linewidth]{figures/results/gan_train/not_pretrained/random/fb15k237/epochs1000/random_fb15k237_mrrs.png}
    \end{minipage}%
    \begin{minipage}{.5\textwidth}
      \centering
      \includegraphics[width=0.9\linewidth]{figures/results/gan_train/not_pretrained/uncertainty/max_distribution/entropy/fb15k237/1k_epochs/uncertainty_fb15k237_mrrs.png}
    \end{minipage}
    \begin{minipage}{.5\textwidth}
      \centering
      \includegraphics[width=0.9\linewidth]{figures/results/gan_train/not_pretrained/random/fb15k237/epochs1000/random_fb15k237_hit10.png}
    \end{minipage}%
    \begin{minipage}{.5\textwidth}
      \centering
      \includegraphics[width=0.9\linewidth]{figures/results/gan_train/not_pretrained/uncertainty/max_distribution/entropy/fb15k237/1k_epochs/uncertainty_fb15k237_hit10.png}
    \end{minipage}%
    \caption{Adversarial training on \textsc{FB15k237} dataset. 
    Left Figures show Sampling Negative triples with original Random Sampling, right Figures show Best Uncertainty Sampling (Uncertainty Distribution Entropy).
    Shown are validation H@10 and MRRs for 1000 epochs.}
    \label{fig:advtrain_fb15k237_random_vs_uncertainty}
\end{figure}
In comparison, \ussoftmax does not reach the same values, but further increases MRR and Hit@10 values until 23.5\% and 41\%.

Nevertheless, courses of rewards look different for both sampling methods.
While adversarial training with \origsampling reaches a local minimum at 50 epochs and continues increasing reward afterwards.
With \ussoftmax, a global maximum is reached in the beginning of training and subsequently, rewards are only descreasing untl the end of training (\Autoref{fig:advtrain_fb15k237_losses_rewards}).
\begin{figure}[H]
    \centering
    \begin{minipage}{.5\textwidth}
      \centering
      \includegraphics[width=0.9\linewidth]{figures/results/gan_train/not_pretrained/random/fb15k237/epochs1000/random_fb15k237_losses.png}
    \end{minipage}%
    \begin{minipage}{.5\textwidth}
      \centering
      \includegraphics[width=0.9\linewidth]{figures/results/gan_train/not_pretrained/uncertainty/max_distribution/entropy/fb15k237/1k_epochs/uncertainty_fb15k237_losses.png}
    \end{minipage}
    \begin{minipage}{.5\textwidth}
      \centering
      \includegraphics[width=0.9\linewidth]{figures/results/gan_train/not_pretrained/random/fb15k237/epochs1000/random_fb15k237_rew.png}
    \end{minipage}%
    \begin{minipage}{.5\textwidth}
      \centering
      \includegraphics[width=0.9\linewidth]{figures/results/gan_train/not_pretrained/uncertainty/max_distribution/entropy/fb15k237/1k_epochs/uncertainty_fb15k237_rew.png}
    \end{minipage}%
    \caption{Comparison of training losses and rewards over 1000 epochs on \textsc{FB15k-237} dataset.
    Left Figures show losses and rewards of \textsc{OriginalSampling} and right Figures show losses and rewards of \textsc{UncertaintySamplingSoftmax}.}
    \label{fig:advtrain_fb15k237_losses_rewards}
\end{figure}
Surprisingly, training losses with \origsampling look even very different to losses on other datasets.
After a global minimum is reached after 50 epochs, losses slightly increase and stagnate at a loss of about 0.8.
With \ussoftmax, losses continue decreasing until the end of training.