\subsection{WN18 Dataset}
\label{subsec:uncertainty_wn18}

On \textsc{WN18} dataset the learning curves of MRR values strongly for both sampling methods (\Autoref{fig:advtrain_wn18_random_vs_uncertainty}).
\begin{figure}
    \centering
    \begin{minipage}{.5\textwidth}
      \centering
      \includegraphics[width=0.9\linewidth]{figures/results/gan_train/not_pretrained/random/wn18/epochs1000/random_wn18_mrrs.png}
    \end{minipage}%
    \begin{minipage}{.5\textwidth}
      \centering
      \includegraphics[width=0.9\linewidth]{figures/results/gan_train/not_pretrained/uncertainty/max_distribution/entropy/wn18/1k_epochs/uncertainty_wn18_mrrs.png}
    \end{minipage}
    \begin{minipage}{.5\textwidth}
      \centering
      \includegraphics[width=0.9\linewidth]{figures/results/gan_train/not_pretrained/random/wn18/epochs1000/random_wn18_hit10.png}
    \end{minipage}%
    \begin{minipage}{.5\textwidth}
      \centering
      \includegraphics[width=0.9\linewidth]{figures/results/gan_train/not_pretrained/uncertainty/max_distribution/entropy/wn18/1k_epochs/uncertainty_wn18_hit10.png}
    \end{minipage}%
    \caption{Adversarial training on \textsc{WN18} dataset. 
    Left Figures show Sampling Negative triples with \origsampling, right figures show \ussoftmax with entropy metrics.
    Shown are validation H@10 and MRRs for 1000 epochs.}
    \label{fig:advtrain_wn18_random_vs_uncertainty}
\end{figure}
While both sampling methods reach a MRR value of 45\% at 100 epochs, this value further increases with \origsampling and stagnates or slightly decreases with \ussoftmax.
Nevertheless, Hit@10 values look similar forh both sampling methods.
Both of them reach their maximum of about 92\% at already 100 epochs and stay the whole time on same level.

The fact that the MRR values continue to increase with \origsampling but not with \ussoftmax can be justified on the basis of the rewards (\Autoref{fig:advtrain_wn18_losses_rewards}).
\begin{figure}
    \centering
    \begin{minipage}{.5\textwidth}
      \centering
      \includegraphics[width=0.9\linewidth]{figures/results/gan_train/not_pretrained/random/wn18/epochs1000/random_wn18_losses.png}
    \end{minipage}%
    \begin{minipage}{.5\textwidth}
      \centering
      \includegraphics[width=0.9\linewidth]{figures/results/gan_train/not_pretrained/uncertainty/max_distribution/entropy/wn18/1k_epochs/uncertainty_wn18_losses.png}
    \end{minipage}
    \begin{minipage}{.5\textwidth}
      \centering
      \includegraphics[width=0.9\linewidth]{figures/results/gan_train/not_pretrained/random/wn18/epochs1000/random_wn18_rew.png}
    \end{minipage}%
    \begin{minipage}{.5\textwidth}
      \centering
      \includegraphics[width=0.9\linewidth]{figures/results/gan_train/not_pretrained/uncertainty/max_distribution/entropy/wn18/1k_epochs/uncertainty_wn18_rew.png}
    \end{minipage}%
    \caption{Comparison of training losses and rewards over 1000 epochs on \textsc{WN18} dataset.
    Left Figures show losses and rewards of \origsampling and right Figures show losses and rewards of \ussoftmax.}
    \label{fig:advtrain_wn18_losses_rewards}
\end{figure}
While with \origsampling rewards are increasing the whole time, they are stagnating with \ussoftmax at 200 epochs and do not increase anymore.
Therefore, sampled negative triples after 200 epochs continue to have the same distance and the model does not learn anymore.
Nevertheless, curves of losses look similar for both sampling methods.