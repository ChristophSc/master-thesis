\section{Evaluation of Uncertainty Sampling}
\label{ch:evaluation:sec:evaluation_uncertainty}
%
Finally, the best sampling by uncertainty sampling method \ussoftmax with entropy uncertainty measure is evaluated by comparing it to the original sampling method \origsampling of \kbgan.
The evaluation takes place on datasets in \Autoref{subsec:results_uncertainty} and a final Conclusion of uncertainty sampling is given in \Autoref{subsec:uncertainty_conclusion}.
%
\subsection{Results on Datasets} \label{subsec:results_uncertainty}

\input{content/chapters/05_evaluation/sections/04_uncertainty/01_uncertainty_umls}

\input{content/chapters/05_evaluation/sections/04_uncertainty/02_uncertainty_kinship}

\input{content/chapters/05_evaluation/sections/04_uncertainty/03_uncertainty_wn18rr}

\input{content/chapters/05_evaluation/sections/04_uncertainty/04_uncertainty_wn18}

\input{content/chapters/05_evaluation/sections/04_uncertainty/05_uncertainty_fb15k237}

\input{content/chapters/05_evaluation/sections/04_uncertainty/06_uncertainty_fb15k}

\input{content/chapters/05_evaluation/sections/04_uncertainty/07_uncertainty_yago3_10}

\subsection{Summary and Conclusion of Uncertainty Sampling}
\label{subsec:uncertainty_conclusion}
%
The results of the experiments are listed in tables to compare them to the original approach.
These show both the MRR and Hit@10 results achieved with the original approach \kbgan and those obtained with \usgan with \ussoftmax and entropy uncertainty measure.
\Autoref{tab:result_table1} includes all results for smaller datasets \umls and \kinship,
\Autoref{tab:result_table2} lists all results for \textsc{WN18RR} and \textsc{WN18}, and \Autoref{tab:result_table2} depicts results for \textsc{FB15k-237}, \textsc{FB15k} and \textsc{YAGO3-10} datasets.

\input{content/chapters/05_evaluation/result_table1}

In some cases such as training on \textsc{FB15k-237} adversarial training with \ussoftmax seems to work and on small datasets such as \umls even better than \origsampling.
Additionally, it is noticeable that training looks very different for different datasets.
Nevertheless, in most cases, adversarial training with \origsampling still performs better and reaches higher MRR and Hit@10 values.
Therefore, measuring and sampling by the uncertainty of the generator to classify triples as positives and negatives does not necessarily imply the uncertainty of the discriminator model.
Even if the generator model might be uncertain if a given triple is positive or negative, the discriminator can be more certain about this classification.
\clearpage

\input{content/chapters/05_evaluation/result_table2}

Another reason might be that classification is not accurate enough.
It was observed that even if negative triples scoring ranges are lower than positive triple scoring ranges at the beginning of training, this sometimes changes over training time.
Therefore, the overlap of both ranges increases.
In addition, uncertainty is currently only measured based on the generator's uncertainty score and does not include any further information about the \ac{KG}, its relations, and entities.
\clearpage

\input{content/chapters/05_evaluation/result_table3}


% scores of positive and neative triples sometimes has a high overlap which leads sampling of negative triples with an average score
% hyperparameter optimization necessary for all datasets
% Models are very dependent on hyperparameters, small changes can already have large effects on learning behavior.
% Depending on the properties of a data set, the selected model pair for generator and discriminator and the set hyperparameters, a completely different training behavior can occur.