\section{Evaluation of Uncertainty Sampling}
\label{ch:evaluation:sec:evaluation_uncertainty}
%
Finally, the best sampling by uncertainty sampling method \ussoftmax with entropy uncertainty measure is evaluated by comparing it to the original sampling method \origsampling of \kbgan.
The evaluation takes place on datasets in \Autoref{subsec:results_uncertainty} and a final Conclusion of uncertainty sampling is given in \Autoref{subsec:uncertainty_conclusion}.
%
\subsection{Results on Datasets} \label{subsec:results_uncertainty}

\input{content/chapters/05_evaluation/sections/04_uncertainty/01_uncertainty_umls}

\input{content/chapters/05_evaluation/sections/04_uncertainty/02_uncertainty_kinship}

\input{content/chapters/05_evaluation/sections/04_uncertainty/03_uncertainty_wn18rr}

\input{content/chapters/05_evaluation/sections/04_uncertainty/04_uncertainty_wn18}

\input{content/chapters/05_evaluation/sections/04_uncertainty/05_uncertainty_fb15k237}

\input{content/chapters/05_evaluation/sections/04_uncertainty/06_uncertainty_fb15k}

\input{content/chapters/05_evaluation/sections/04_uncertainty/07_uncertainty_yago3_10}

\subsection{Summary and Conclusion of Uncertainty Sampling}
\label{subsec:uncertainty_conclusion}
%
The results of the experiments are listed in tables to compare them to the original approach.
These show both the MRR and Hit@10 results achieved with the original approach \kbgan and those obtained with \usgan with \ussoftmax and entropy uncertainty measure.
\Autoref{tab:result_table1} includes all results for smaller datasets \umls and \kinship,
\Autoref{tab:result_table2} lists all results for \textsc{WN18RR} and \textsc{WN18}, and \Autoref{tab:result_table2} depicts results for \textsc{FB15k-237}, \textsc{FB15k} and \textsc{YAGO3-10} datasets.

\input{content/chapters/05_evaluation/result_table1}

In some cases such as training on \textsc{FB15k-237} adversarial training with \ussoftmax seems to work and on small datasets such as \umls even better than \textsc{OrigSampling}.
Additionally, it is noticeable that training looks very different for different datasets.
Nevertheless, in most cases, adversarial training with \origsampling still performs better and reaches higher MRR and Hit@10 values.
Therefore, measuring and sampling by the uncertainty of the generator to classify triples as positives and negatives does not necessarily imply the uncertainty of the discriminator model.
Even if the generator model might be uncertain if a given triple is positive or negative, the discriminator can be more certain about this classification.
\clearpage

\input{content/chapters/05_evaluation/result_table2}

Another reason might be that classification is not accurate enough.
It was observed that even if negative triples scoring ranges are lower than positive triple scoring ranges at the beginning of training, this sometimes changes over training time.
Therefore, the overlap of both ranges increases.
In addition, uncertainty is currently only measured based on the generator's uncertainty score and does not include any further information about the \ac{KG}, its relations, and entities.

As already mentioned in \Autoref{sec:algorithm}, the creation of the negative triple sets $Neg$ takes place within the epoch loop.
However, it would also be possible to move this part into the batch loop.
This would have the consequence that the calculation of the scoring values and thus also the calculation of $score_{min}$ and $score_{max}$ would be based on the current positive and negative triples.
This could increase the performance of the approach, especially for larger knowledge graphs, since the classification of the underlying scoring ranges of the positive and negative triples would not be so high anymore.
\clearpage

\input{content/chapters/05_evaluation/result_table3}
