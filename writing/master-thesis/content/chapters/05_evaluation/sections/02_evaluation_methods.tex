\section{Evaluation of Uncertainty Sampling Methods}
\label{ch:evaluation:sec:evaluation_methods}
%
To determine the best approach of sampling by uncertainty, we first compare the two methods \usmax and \ussoftmax.
For this purpose, we compare the course of the validation metrics of the two methods in different data sets in \Autoref{subsec:methods_results}.
This is followed by a conclusion of uncertainty sampling methods in  \Autoref{subsec:methods_conclusion}.
%
\subsection{Results on Datasets} \label{subsec:methods_results}
\input{content/chapters/05_evaluation/sections/02_methods/01_methods_umls}
\input{content/chapters/05_evaluation/sections/02_methods/02_methods_kinship}
\input{content/chapters/05_evaluation/sections/02_methods/03_methods_wn18rr}
\input{content/chapters/05_evaluation/sections/02_methods/04_methods_wn18}
\input{content/chapters/05_evaluation/sections/02_methods/05_methods_fb15k237}
%
\subsection{Conclusion} 
\label{subsec:methods_conclusion}
%
In conclusion, \usmax and \ussoftmax perform equally for many embedding model pairs and datasets.
Just in some cases \ussoftmax performs better and increase MRR and Hit@10 metrics slightly over time and in addition, performance does never not decrease, it always stays on same level even if it does not increase either.
Additionally, especially for some model pairs \usmax performs bad.
Furthermore, it can be seen that adversarial training performs better with larger data sets, because the course of losses and rewards for smaller datasets is very different than normal courses of losses and rewards.

