\section{Calculation of Uncertainty Scores}
\label{sec:calculation_of_uncertainty_scores}
%
Uncertainty sampling takes place on the basis of probabilities with which the instances belong to the respective classes.
The calculation of the probabilities to belong to a class was determined in the previous step.
Therefore, the calculation of uncertainty scores can take place at this point.
%
To transform probabilities of triples to belong to positive triple class ($y = 1$) and negative triple class ($y = 0$) into uncertainty scores a transformation function is needed.
This transformation function $\phi$ uses calculated class probabilities to map to an uncertainty score.
In case of this approach probabilities $\mathds{P}(y = 1| f_G(h, r, t))$ are  mapped to $u(l)$ with \Autoref{eqn:uncertainty_function}
\begin{equation} \label{eqn:uncertainty_function}
    u(l) = \phi(\mathds{P}(y = 1| f_G(h, r, t))), \phi: \mathbb{R} \rightarrow [0,1]
\end{equation}
For this transformation function several uncertainty measures are at disposal which are entropy, least confidence, margin of confidence and ratio of confidence.
They provide distributions of uncertainty scores $u(l) \in [0, 1]$ for probabilities $\mathbb{P}(y = 1 | (h,r,t))$ of a given triple $(h, r, t)$ (\autoref{fig:sampling_distributions}).
\begin{figure}
    \centering
    \begin{minipage}{.5\textwidth}
      \centering
      \includegraphics[width=0.9\linewidth]{figures/uncertainty_metric_entropy.pdf}
    \end{minipage}%
    \begin{minipage}{.5\textwidth}
      \centering
      \includegraphics[width=0.9\linewidth]{figures/least_confident_graph.PNG}
    \end{minipage}
    \begin{minipage}{.5\textwidth}
      \centering
      \includegraphics[width=0.9\linewidth]{figures/smallest_margin_graph.PNG}
    \end{minipage}%
    \begin{minipage}{.5\textwidth}
      \centering
      \includegraphics[width=0.9\linewidth]{figures/smallest_ratio.PNG}
    \end{minipage}%
    \caption{Uncertainty sampling distributions for (a) entropy, (b) least confidence, (c) margin of confidence and (d) ratio of confidence in a binary classification problem.
    The x-axis marks the probability of a triple belonging to the positive class $\mathbb{P}(y = 1 | l = (h,r,t))$ and the y-axis the uncertainty score $u(l)$ of the corresponding uncertainty measure.}
    \label{fig:sampling_distributions}
\end{figure}

% describe sampling methods
% 1) always sample maximum uncertainty score
Based on these distributions, two different \textbf{Uncertainty Sampling Methods} can be derived:
\begin{itemize}
    \item 
    \textbf{\underline{U}ncertainty \underline{S}ampling with \underline{Max}imum Distribution (\textsc{USMax})}:\\
    On the one hand, the triple with the maximum uncertainty score can always be sampled.
    As can be seen from \autoref{fig:sampling_distributions} all uncertainty measures have the maximum uncertainty score of 1 at the high point of the function which is located at $P(y = 1 | f_G(l)) = 0.5$ with uncertainty score of 1.
    The triple with the closest probability is assigned the highest uncertainty score.
    If only one negative triple reaches a maximum uncertainty score among all negative triples, it will be sampled.
    If there are several triples with a maximum uncertainty score, one of these triples is sampled with equal probability according to a uniform distribution.
    This results in the definition of sampling method \usmax with the following sampling probabilities:
    \begin{equation} 
        \mathbb{P}_{\usmax}(h, r, t) =
        \begin{cases}
             0 \ \ \ \ \ \  \ \ \ \ \ \ \ \  \ \ 
             \text{if}\ \ \ u(h, r, t) < \max_{(h,r,t) \in Neg}(u(h, r, t)) 
             \\ \\
            \frac{1}{|\{(h,r,t) | u(h,r,t) = \max_{(h,r,t) \in Neg}(u(h, r, t)\}|} 
            \ \ \ \ \
            \text{else} 
             \\
        \end{cases}
         \label{eq:usmax}
    \end{equation}
    with $\mathbb{P}_{\usmax}(h, r, t) \in [0,1]$ 
    
    \item
    \textbf{\underline{U}ncertainty \underline{S}ampling with \underline{Softmax} Distribution (\textsc{USSoftmax})}:\\
    
    % 2) sample according to distribution
    On the other hand triples can be sampled according to a softmax probability distribution based on uncertainty scores.
    Thus, triples close to $P(y = 1 | f_G(l)) = 0.5$ still have the highest probability of being sampled, but other triples for which the model is more certain are also assigned a probability of being sampled.
    \ussoftmax is defined as the softmax of all uncertainty scores:
    \begin{equation}
        \label{eq:ussoftmax}
        \mathbb{P}_{\ussoftmax}(h,r,t) = \frac{e^{u(h,r,t)}}{\sum_{(h,r,t) \in Neg}{e^{u(h,r,t)}}} \in [0,1]
    \end{equation}
\end{itemize}
In the original \kbgan approach, triples were given a sampling probability according to their achieved score in the generator model.
Thus, negative triples with the highest score also received the highest sampling probability.
This sampling method is referred as \origsampling in the following.
For the individual uncertainty measurements, the following definitions for $\phi$ and considerations for the two sampling methods result \cite{human-in-the-loop}.

\textbf{Entropy} 
considers the differences between all the predictions.
Like the other uncertainty metrics, the high point of the function is at $x = 0.5$, where the model is most uncertain about the classification.
However, it differs to the other metrics since it slopes flatter in both directions.
Therefore, if triples are sampled by Uncertainty Distribution method, it is more likely that triples will be sampled that are further away from $x=0.5$.
According to the definition of Entropy, the mapping function $\phi$ is defined as follows:
\begin{equation}
    \phi(x) = - \sum_{y \in \mathcal{Y}}{\mathds{P}_{\theta}(y | f_G(x)) \cdot log \mathds{P}_{\theta}(y | f_G(x))}
\end{equation}
Since in our case we are dealing with a binary classification with $y_i \in \{0,1\}$, this results in calculation:
\begin{equation}
= - \mathds{P}(y = 1| f_G(x)) log \mathds{P}(y = 1 | f_G(x))
- \mathds{P}(y = 0| f_G(x)) log \mathds{P}(y = 0 | f_G(x))
\end{equation}
\begin{equation}
= - \mathds{P}(y = 1| f_G(x) ) log \mathds{P}(y = 1 | f_G(x))
- ((1 - \mathds{P}(y = 1 | f_G(x)))
     log(1 - \mathds{P}(y = 1 | f_G(x))))
\end{equation}

\textbf{Least Confidence} 
measures the distance between the most confident prediction and 100\%.
Normalized for $n$ classes to an uncertainty score between 0 and 1 it is defined as follows:
\begin{equation}
    \phi(x) = (1 - \max_{y \in \mathcal{Y}}{\mathds{P}_{\theta}(y | f_G(x))}) \cdot \left(\frac{n}{n-1}\right)
\end{equation}
Since we have a binary classification, we can set $n = 2$ and $y \in \{0,1\}$:
\begin{equation}
    \phi(x) = (1 - \max({\mathds{P}_{\theta}(y = 1| f_G(x)), \mathds{P}_{\theta}(y = 0| f_G(x))})) \cdot 2
\end{equation}
\begin{equation} \label{eq:leastconfidence}
    = 2 - 2 \max({\mathds{P}_{\theta}(y = 1| f_G(x)), \mathds{P}_{\theta}(y = 0| f_G(x))})
\end{equation}
In contrast to entropy, the graph slopes much faster in the direction of $x = 0$ and $x = 1$, so that triples with lower uncertainty of the model are also sampled with lower probability with Uncertainty Distribution method.

\textbf{Margin of Confidence}
deals with the two most confident predictions where
$y_m = \argmax_{y \in \mathcal{Y}} \mathds{P}_{\theta}(y | f_G(x))$ 
and $y_n = \argmax_{y \in \mathcal{Y} \setminus y_m}{\mathds{P}_{\theta}(y | f_G(x))}$.
To convert this to a range between 0 and 1, we need to subtract it from 1:
\begin{equation}
    \phi(x) = 1 - (\mathds{P}_{\theta}(y_m |f_G(x)) - \mathds{P}_{\theta}(y_n | f_G(x)))
\end{equation}
Again, this can be further simplified due to the binary classification because for first case of $y_m = 1$ and $y_n = 0$ this results in
\begin{equation}
    \phi(x) = 1 - (\mathds{P}_{\theta}(y = 1 |f_G(x)) - \mathds{P}_{\theta}(y = 0 | f_G(x)))
\end{equation}
\begin{equation}
    = 1 - (\mathds{P}_{\theta}(y = 1 |f_G(x)) 
    -  (1 - \mathds{P}_{\theta}(y = 1 | f_G(x)))
\end{equation}
\begin{equation}
    = 1 - (2 \mathds{P}_{\theta}(y = 1 |f_G(x)) - 1)
\end{equation}
\begin{equation}
    = 2 - 2 \mathds{P}_{\theta}(y = 1 |f_G(x))
\end{equation}
and for second case if $y_m = 0$ and $y_n = 1$ we have  $ 2 - 2 \mathds{P}_{\theta}(y = 0 |f_G(x))$ . 
And therefore:
\begin{equation} \label{eq:marginofconfidence}
    = 2 - 2 \max(\mathds{P}_{\theta}(y = 0 |f_G(x)), \mathds{P}_{\theta}(y = 1 |f_G(x)))
\end{equation}
Since \Autoref{eq:leastconfidence} and \ref{eq:marginofconfidence} are equal, for binary classification sampling by Margin of Confidence is the same as sampling by Least Confidence.

\textbf{Ratio of Confidence}
is quite similar to Margin of Confidence, but instead of the difference it compares the ratio of the two most confident predictions
\begin{equation}
    \phi(x) = \frac{\mathds{P}_{\theta}(y_m | f_G(x))}{\mathds{P}_{\theta}(y_n | f_G(x))}
\end{equation}
where $y_m$ and $y_n$ are equally defined as in Margin of Confidence.
Due to the ratio between the two most confident predictions $y_m$ and $y_n$, there is an even stronger focus on sampling only uncertain triples in the Uncertainty Distribution method around $x = 0.5$.


