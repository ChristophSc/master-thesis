\section{Calculation of Uncertainty Scores}
\label{sec:calculation_of_uncertainty_scores}
%
Uncertainty sampling takes place based on probabilities with which the instances belong to the respective classes.
After determining the calculation of probabilities in the previous section, the calculation of uncertainty scores can take place at this point.
For this, a transformation function is used to map from the generator scores to the uncertainty scores of the individual triples.
This transformation function $\phi$ uses the individual class probabilities.
Therefore, generator scores obtained by $f_G(l)$ for a triple $l = (h,r,t)$ are  mapped to uncertainty score $u(l)$ with following definition.
\begin{equation} \label{eqn:uncertainty_function}
    u(l) = \phi(f_G(x)) \in  [0,1].
\end{equation}
For this transformation function, several uncertainty measures are available.
These are entropy, least confidence, margin of confidence and ratio of confidence (\Autoref{sec:uncertaintysampling}).
They provide distributions of uncertainty scores $u(l) \in [0, 1]$ for probabilities $\mathbb{P}(y = 1 | f_G(l))$ of a given triple $l = (h, r, t)$ (\Autoref{fig:sampling_distributions}).
\clearpage
\begin{figure}[H]
    \centering
    \begin{minipage}{.4\textwidth}
      \centering
      \includegraphics[width=\linewidth]{figures/entropy_graph.PNG}
    \end{minipage}%
    \begin{minipage}{.4\textwidth}
      \centering
      \includegraphics[width=\linewidth]{figures/least_confident_graph.PNG}
    \end{minipage}
    \begin{minipage}{.4\textwidth}
      \centering
      \includegraphics[width=\linewidth]{figures/smallest_margin_graph.PNG}
    \end{minipage}%
    \begin{minipage}{.4\textwidth}
      \centering
      \includegraphics[width=\linewidth]{figures/smallest_ratio.PNG}
    \end{minipage}%
    \caption{Uncertainty sampling distributions for (a) entropy, (b) least confidence, (c) margin of confidence, and (d) ratio of confidence in a binary classification problem.
    The x-axis marks the probability of a triple belonging to the positive class $\mathbb{P}(y = 1 | f_G(l))$ and the y-axis the uncertainty score $u(l)$ of the corresponding uncertainty measure.}
    \label{fig:sampling_distributions}
\end{figure}

% describe sampling methods
% 1) always sample maximum uncertainty score
Based on the calculated probabilities, uncertainty scores are assigned to the triples.
Therefore, two different \textbf{Uncertainty Sampling Methods} are derived based on these distributions:
\begin{itemize}
    \item 
    % sampling according to max distribution
    \textbf{\underline{U}ncertainty \underline{S}ampling with \underline{Max}imum Distribution (\textsc{USMax})}:\\
    The first option is to always sample the triple with the maximum uncertainty score.
    As can be seen from \autoref{fig:sampling_distributions} all uncertainty measures have a maximum uncertainty score of 1 which is located at $P(y = 1 | f_G(l)) = 0.5$.
    Therefore, with \usmax all triples that are closest to this probability of 0.5 of being a positive triple get a sampling probability.
    If only one negative triple reaches a maximum uncertainty score among all negative triples, it will be sampled.
    If there are several triples with a maximum uncertainty score, one of these triples is sampled with an equal probability according to a uniform distribution.
    This results in the following definition of sampling method \usmax with the  sampling probability $\mathbb{P}_{\usmax}(l) \in [0,1]$ for $l = (h,r,t)$:
    \begin{equation} 
        \mathbb{P}_{\usmax}(l) =
        \begin{cases}
             0 \ \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ 
             \text{if}\ \ \ u(l) < \max_{l \in Neg}(u(l)) 
             \\ \\
            \frac{1}{|\{l | u(l) = \max_{l \in Neg}(u(l)\}|} 
            \ \ \ \
            \text{else} 
             \\
        \end{cases}
         \label{eq:usmax}
    \end{equation}
    
    \item
    \textbf{\underline{U}ncertainty \underline{S}ampling with \underline{Softmax} Distribution (\textsc{USSoftmax})}:\\    
    % 
    The second sampling method is to sample according to a softmax probability distribution based on uncertainty scores.
    Thus, triples close to $P(y = 1 | f_G(l)) = 0.5$ still have the highest probability of being sampled, but other triples for which the model is more certain are also assigned a probability of being sampled.
    \ussoftmax is defined as follows:
    \begin{equation}
        \label{eq:ussoftmax}
        \mathbb{P}_{\ussoftmax}(l) = \frac{e^{u(l)}}{\sum_{l = \in Neg}{e^{u(l)}}} \in [0,1]
    \end{equation}
\end{itemize}
In the original \kbgan approach, triples were given a sampling probability according to their achieved score in the generator model.
Thus, negative triples with the highest score also received the highest sampling probability.
This sampling method is referred to as \textsc{Original Sampling} (\origsampling) in the following.
For the individual uncertainty measures, the following definitions for $\phi$ and considerations for the two sampling methods \usmax and \ussoftmax result.

\textbf{Entropy} 
considers the differences between all predictions.
According to the definition of entropy (\Autoref{eqn:entropy_def}), the mapping function $\phi$ is defined as
\begin{equation}
    \phi(x) = - \sum_{y \in \mathcal{Y}}{\mathds{P}(y | x) \cdot log \mathds{P}(y | x)}.
\end{equation}
In this case $x = f_G(l)$ can be inserted for a triple $l = (h,r,t)$.
Additionally, we have a binary classification problem with $\mathcal{Y} = \{0,1\}$.
Inserting $x$ and $\mathcal{Y}$ leads to
\begin{equation}
= - \mathds{P}(y = 1| f_G(l)) log \mathds{P}(y = 1 | f_G(l))
- \mathds{P}(y = 0| f_G(l)) log \mathds{P}(y = 0 | f_G(l)).
\end{equation}
And since in a binary classification the probability of one class can be expressed by the counter probability of the other class 
$\mathds{P}(y = 0| f_G(l)) = 1 - \mathds{P}(y = 1 | f_G(l))$.
\begin{align*} 
&= - \mathds{P}(y = 1| f_G(l)) log \mathds{P}(y = 1 | f_G(l))\\
  &\hspace{4mm}- ((1 - \mathds{P}(y = 1 | f_G(l))) log(1 - \mathds{P}(y = 1 | f_G(l)))) \\
&= - \mathds{P}(y = 1| f_G(l)) log \mathds{P}(y = 1 | f_G(l)) \\
   &\hspace{4mm}- (log(1 - \mathds{P}(y = 1 | f_G(l))) - \mathds{P}(y = 1 | f_G(l)) log(1 - \mathds{P}(y = 1 | f_G(l)))) \\
&= - \mathds{P}(y = 1| f_G(l)) log \mathds{P}(y = 1 | f_G(l)) \\
   &\hspace{4mm}- log(1 - \mathds{P}(y = 1 | f_G(l))) + \mathds{P}(y = 1 | f_G(l)) log(1 - \mathds{P}(y = 1 | f_G(l))) 
\end{align*}
As can be seen from the graph of the entropy function (\Autoref{fig:sampling_distributions}) it has the high point of the function at $P(y = 1 | f_G(l)) = 0.5$ like the other uncertainty measures.
Therefore, at this point the model is most uncertain about the classification between negative ($y = 0$) and positive triple($y = 1$).
However, it differs to the other metrics since it slopes flatter in both directions ($P(y = 1 | f_G(l)) = 0$ and $P(y = 1 | f_G(l)) = 1$).
Therefore, if triples are sampled by sampling method \ussoftmax, it is more likely that triples will be sampled that are further away from $P(y = 1 | f_G(l)) = 0.5$ than with the other uncertainty measures.

\textbf{Least Confidence} 
measures the distance between the most confident prediction and 100\% confidence.
According to the definition of least confidence (\Autoref{eqn:least_confidence_def}) the transformation function $\phi$ for this uncertainty measure is defined as
\begin{equation}
    \phi(x) = (1 - \max_{y \in \mathcal{Y}}{\mathds{P}(y | x)}) \cdot \left(\frac{n}{n-1}\right).
\end{equation}
It is normalized for $n$ classes to an output between 0 and 1.
Again, $x = f_G(l)$ for a triple $l = (h,r,t)$ can be inserted and since we have a binary classification with $n = 2$ classes and $\mathcal{Y} = \{0,1\}$, the equation can be rewritten to
\begin{equation} \label{eq:leastconfidence}
\begin{split}
\phi_1(l) 
& = (1 - \max({\mathds{P}(y = 1| f_G(l)), \mathds{P}(y = 0| f_G(l))})) \cdot 2 \\
&= 2 - 2 \max({\mathds{P}(y = 1| f_G(l)), \mathds{P}(y = 0| f_G(l))}).
\end{split}
\end{equation}
In contrast to entropy, the graph slopes much faster in the direction of $\mathds{P}(y | f_G(l)) = 0$ and $\mathds{P}(y | f_G(l)) = 1$, so that triples with lower uncertainty of the model are also sampled with lower probability with sampling method \textsc{USSoftmax}.
\clearpage

\textbf{Margin of Confidence}
deals with the two most confident predictions $y_m = \argmax_{y \in \mathcal{Y}} \mathds{P}(y | x)$ and $y_n = \argmax_{y \in \mathcal{Y} \setminus y_m}{\mathds{P}(y | x)}$.
Given the definition of margin of confidence (\Autoref{eqn:margin_of_confidence_def}), transformation function $\phi$ for this uncertainty measure results in
\begin{equation}
    \phi(x) = 1 - (\mathds{P}(y_m |x) - \mathds{P}(y_n | x)).
\end{equation}
To unify it with the transformation functions of the other uncertainty measures, their difference is subtracted from 1.
For triple $l = (h,r,t)$ and $x = f_G(l)$ and due to the fact that we have a binary classification problem with either $y_m = 1$ and $y_n = 0$ or vice versa, there are two different cases.
For first case of $y_m = 1$ and $y_n = 0$ this results in
\begin{equation}
\begin{split}
\phi_1(l) 
&= 1 - (\mathds{P}(y = 1 |f_G(l)) - \mathds{P}(y = 0 | f_G(l))) \\
&= 1 - (\mathds{P}(y = 1 |f_G(l)) -  (1 - \mathds{P}(y = 1 | f_G(l))) \\
&= 1 - (2 \mathds{P}(y = 1 |f_G(l)) - 1) \\
&= 2 - 2 \mathds{P}(y = 1 |f_G(l))
\end{split}
\end{equation}
For second case of $y_m = 0$ and $y_n = 1$ we have  $\phi_2 = 2 - 2 \mathds{P}(y = 0 |f_G(l))$. 
And therefore this leads to overall definition of $\phi(x)$ for margin of confidence
\begin{equation} \label{eq:marginofconfidence}
    \phi(l) = 2 - 2 \max(\mathds{P}(y = 0 |f_G(l)), \mathds{P}(y = 1 |f_G(l))).
\end{equation}
Since the equations of least confidence (\Autoref{eq:leastconfidence}) and margin of confidence (\ref{eq:marginofconfidence}) are equal, sampling according to these uncertainty measures in a binary classification is equal.
This is also evident when looking at the function graphs depicted in \Autoref{fig:sampling_distributions}.
Therefore, for this case of binary classification, the same uncertainty scores are assigned and \usmax as well as \ussoftmax sample triples with the same probability.

\textbf{Ratio of Confidence}
is quite similar to margin of confidence, but instead of the difference it compares the ratio of the two most confident predictions
\begin{equation}
    \phi(x) = \frac{\mathds{P}(y_m | x)}{\mathds{P}(y_n |x)}
\end{equation}
where $y_m$ and $y_n$ are equally defined as in margin of confidence.
Inserted $x = f_G(l)$ for a triple $l = (h,r,t)$ leads to
\begin{equation}
    \phi(l) = \frac{\mathds{P}(y_m |  f_G(l))}{\mathds{P}(y_n |f_G(l))}.
\end{equation}
Looking at the function graph of ratio of confidence in \Autoref{fig:sampling_distributions}, due to the ratio between the two most confident predictions $y_m$ and $y_n$, there is an even stronger focus on sampling only uncertain triples with \ussoftmax around $\mathds{P}(y = 1 | f_G(l)) = 0.5$.