\section{Calculation of Uncertainty Scores}
\label{sec:calculation_of_uncertainty_scores}


The calculation of uncertainty scores based on the calculated probabilities takes place in the following steps.
At first, for each triple $l= (h, r, t)$ a plausibility score $f_G(l)$ is calculated by the generator embedding model. 
Subsequently, a classification is performed to obtain probabilities for positive triple class ($y = 1$) and negative triple class ($y = 0$).
To finally sample by uncertainty we need to calculate uncertainty scores now. 
For this a transformation function $\phi$ is needed which uses calculated class probabilities to map from plausibility scores from generator to an uncertainty score $u(l)$.
\begin{equation} \label{eqn:uncertainty_function}
    u(l) = \phi(\mathds{P}(y = 1| f_G(h, r, t))), \phi: \mathbb{R} \rightarrow [0,1]
\end{equation}
For this transformation function we have several uncertainty measurements at our disposal which are entropy, least confidence, margin of confidence and ratio of confidence.
They provide an uncertainty score $u(l) \in [0, 1]$ for probabilities $\mathbb{P}(y = 1 | (h,r,t))$ of a given triple $(h, r, t)$ (\autoref{fig:sampling_distributions}).
\begin{figure}
    \centering
    \begin{minipage}{.5\textwidth}
      \centering
      \includegraphics[width=0.9\linewidth]{figures/uncertainty_metric_entropy.pdf}
    \end{minipage}%
    \begin{minipage}{.5\textwidth}
      \centering
      \includegraphics[width=0.9\linewidth]{figures/least_confident_graph.PNG}
    \end{minipage}
    \begin{minipage}{.5\textwidth}
      \centering
      \includegraphics[width=0.9\linewidth]{figures/smallest_margin_graph.PNG}
    \end{minipage}%
    \begin{minipage}{.5\textwidth}
      \centering
      \includegraphics[width=0.9\linewidth]{figures/smallest_ratio.PNG}
    \end{minipage}%
    \caption{Uncertainty sampling distributions for (a) entropy, (b) least confidence, (c) margin of confidence and (d) ratio of confidence in a binary classification problem.
    The x-axis marks the probability of a triple belonging to the positive class $\mathbb{P}(y = 1 | l = (h,r,t))$ and the y-axis the uncertainty score $u(l)$ of the corresponding uncertainty measure.}
    \label{fig:sampling_distributions}
\end{figure}

% describe sampling methods
% 1) always sample maximum uncertainty score
Based on these distribution functions, two different \textbf{Uncertainty Sampling Methods} can be derived:
\begin{itemize}
    \item 
    \textbf{\underline{U}ncertainty \underline{S}ampling with \underline{Max}imum Distribution (\textsc{USMax})}:\\
    On the one hand, the triple with the maximum uncertainty score can always be sampled.
    As can be seen from the \autoref{fig:sampling_distributions} all uncertainty measures have the maximum uncertainty score of 1 at the high point of the function which is located at $P(y = 1 | f_G(l)) = 0.5$ with uncertainty score of 1, the closest triple is assigned the highest uncertainty score.
    If only one negative triple reaches this maximum uncertainty score, it will be sampled.
    If there are several triples with a maximum uncertainty score, one of the triples is sampled using a uniform distribution.
    This results in the definition of \usmax with the following sampling probabilities:
    \begin{equation} 
        p_{sampled}(h, r, t)=
        \begin{cases}
             0 \ \ \ \ \ \  \ \ \ \ \ \ \ \  \ \ 
             \text{if}\ \ \ u(h, r, t) < \max_{(h,r,t) \in Neg}(u(h, r, t)) 
             \\ \\
            \frac{1}{|\{(h,r,t) | u(h,r,t) = \max_{(h,r,t) \in Neg}(u(h, r, t)\}|} 
            \ \ \ \ \
            \text{else} 
             \\
        \end{cases}
         \label{eq:uncertainty_max}
    \end{equation}
    
    
    \item
    \textbf{\underline{U}ncertainty \underline{S}ampling with \underline{Softmax} Distribution (\textsc{USSoftmax})}:\\
    
    % 2) sample according to distribution
    On the other hand triples can be sampled according to a softmax probability distribution based on uncertainty scores.
    Thus, triples close to $P(y = 1 | f_G(l)) = 0.5$ still have the highest probability of being sampled, but other triples for which the model is more certain are also assigned a probability in the classification.
    \ussoftmax is defined as the softmax of all uncertainty scores:
    \begin{equation}
        \label{eq:uncertainty_distribution}
        p_{sampled}(h,r,t) = \frac{e^{u(h,r,t)}}{\sum_{(h,r,t) \in Neg}{e^{u(h,r,t)}}}
    \end{equation}
\end{itemize}
In the original \kbgan approach, triples were given a sampling probability according to their achieved score in the generator model.
Thus, negative triples with the highest score also received the highest sampling probability.
This sampling method is referred as \origsampling in the following.
For the individual uncertainty measurements, the following definitions for $\phi$ and considerations for the two sampling methods result \cite{human-in-the-loop}.

\textbf{Entropy} 
considers the differences between all the predictions.
Like the other uncertainty metrics, the high point of the function is at $x = 0.5$, where the model is most uncertain about the classification.
However, it differs to the other metrics since it slopes flatter in both directions.
Therefore, if triples are sampled by Uncertainty Distribution method, it is more likely that triples will be sampled that are further away from $x=0.5$.
According to the definition of Entropy, the mapping function $\phi$ is defined as follows:
\begin{equation}
    \phi(x) = - \sum_{y \in \mathcal{Y}}{\mathds{P}_{\theta}(y | f_G(x)) \cdot log \mathds{P}_{\theta}(y | f_G(x))}
\end{equation}
Since in our case we are dealing with a binary classification with $y_i \in \{0,1\}$, this results in calculation:
\begin{equation}
= - \mathds{P}(y = 1| f_G(x)) log \mathds{P}(y = 1 | f_G(x))
- \mathds{P}(y = 0| f_G(x)) log \mathds{P}(y = 0 | f_G(x))
\end{equation}
\begin{equation}
= - \mathds{P}(y = 1| f_G(x) ) log \mathds{P}(y = 1 | f_G(x))
- ((1 - \mathds{P}(y = 1 | f_G(x)))
     log(1 - \mathds{P}(y = 1 | f_G(x))))
\end{equation}

\textbf{Least Confidence} 
measures the distance between the most confident prediction and 100\%.
Normalized for $n$ classes to an uncertainty score between 0 and 1 it is defined as follows:
\begin{equation}
    \phi(x) = (1 - \max_{y \in \mathcal{Y}}{\mathds{P}_{\theta}(y | f_G(x))}) \cdot \left(\frac{n}{n-1}\right)
\end{equation}
Since we have a binary classification, we can set $n = 2$ and $y \in \{0,1\}$:
\begin{equation}
    \phi(x) = (1 - \max({\mathds{P}_{\theta}(y = 1| f_G(x)), \mathds{P}_{\theta}(y = 0| f_G(x))})) \cdot 2
\end{equation}
\begin{equation} \label{eq:leastconfidence}
    = 2 - 2 \max({\mathds{P}_{\theta}(y = 1| f_G(x)), \mathds{P}_{\theta}(y = 0| f_G(x))})
\end{equation}
In contrast to entropy, the graph slopes much faster in the direction of $x = 0$ and $x = 1$, so that triples with lower uncertainty of the model are also sampled with lower probability with Uncertainty Distribution method.

\textbf{Margin of Confidence}
deals with the two most confident predictions where
$y_m = \argmax_{y \in \mathcal{Y}} \mathds{P}_{\theta}(y | f_G(x))$ 
and $y_n = \argmax_{y \in \mathcal{Y} \setminus y_m}{\mathds{P}_{\theta}(y | f_G(x))}$.
To convert this to a range between 0 and 1, we need to subtract it from 1:
\begin{equation}
    \phi(x) = 1 - (\mathds{P}_{\theta}(y_m |f_G(x)) - \mathds{P}_{\theta}(y_n | f_G(x)))
\end{equation}
Again, this can be further simplified due to the binary classification because for first case of $y_m = 1$ and $y_n = 0$ this results in
\begin{equation}
    \phi(x) = 1 - (\mathds{P}_{\theta}(y = 1 |f_G(x)) - \mathds{P}_{\theta}(y = 0 | f_G(x)))
\end{equation}
\begin{equation}
    = 1 - (\mathds{P}_{\theta}(y = 1 |f_G(x)) 
    -  (1 - \mathds{P}_{\theta}(y = 1 | f_G(x)))
\end{equation}
\begin{equation}
    = 1 - (2 \mathds{P}_{\theta}(y = 1 |f_G(x)) - 1)
\end{equation}
\begin{equation}
    = 2 - 2 \mathds{P}_{\theta}(y = 1 |f_G(x))
\end{equation}
and for second case if $y_m = 0$ and $y_n = 1$ we have  $ 2 - 2 \mathds{P}_{\theta}(y = 0 |f_G(x))$ . 
And therefore:
\begin{equation} \label{eq:marginofconfidence}
    = 2 - 2 \max(\mathds{P}_{\theta}(y = 0 |f_G(x)), \mathds{P}_{\theta}(y = 1 |f_G(x)))
\end{equation}
Since \Autoref{eq:leastconfidence} and \ref{eq:marginofconfidence} are equal, for binary classification sampling by Margin of Confidence is the same as sampling by Least Confidence.

\textbf{Ratio of Confidence}
is quite similar to Margin of Confidence, but instead of the difference it compares the ratio of the two most confident predictions
\begin{equation}
    \phi(x) = \frac{\mathds{P}_{\theta}(y_m | f_G(x))}{\mathds{P}_{\theta}(y_n | f_G(x))}
\end{equation}
where $y_m$ and $y_n$ are equally defined as in Margin of Confidence.
Due to the ratio between the two most confident predictions $y_m$ and $y_n$, there is an even stronger focus on sampling only uncertain triples in the Uncertainty Distribution method around $x = 0.5$.


