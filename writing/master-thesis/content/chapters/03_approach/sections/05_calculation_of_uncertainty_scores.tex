\section{Calculation of Uncertainty Scores}
\label{sec:calculation_of_uncertainty_scores}


The calculation of uncertainty scores based on the calculated probabilities takes place in the following steps.
At first, for each triple $l= (h, r, t)$ a plausibility score $f_G(l)$ is calculated by the generator embedding model. 
Subsequently, a classification is performed to obtain probabilities for positive triple class ($y = 1$) and negative triple class ($y = 0$).
To finally sample by Uncertainty we need to calculate uncertainty scores now. 
For this a transformation function $\phi$ is needed which uses calculated class probabilities to map from plausibility scores from generator to an uncertainty score $u(l)$.
\begin{equation} \label{eqn:uncertainty_function}
    u(l) = \phi(\mathds{P}(y = 1| f_G(h, r, t))), \phi: \mathbb{R} \rightarrow [0,1]
\end{equation}
For this transformation function we have several uncertainty metrics at our disposal which are Entropy, Least Confident, Margin of Confidence and Ratio of Confidence, which calculate uncertainty score $u(l) \in [0, 1]$ for probabilities $\mathbb{P}(y = 1 | (h,r,t))$ (\autoref{fig:sampling_distributions}).
\begin{figure}
    \centering
    \begin{minipage}{.5\textwidth}
      \centering
      \includegraphics[width=0.9\linewidth]{figures/entropy_graph.PNG}
    \end{minipage}%
    \begin{minipage}{.5\textwidth}
      \centering
      \includegraphics[width=0.9\linewidth]{figures/least_confident_graph.PNG}
    \end{minipage}
    \begin{minipage}{.5\textwidth}
      \centering
      \includegraphics[width=0.9\linewidth]{figures/smallest_margin_graph.PNG}
    \end{minipage}%
    \begin{minipage}{.5\textwidth}
      \centering
      \includegraphics[width=0.9\linewidth]{figures/smallest_ratio.PNG}
    \end{minipage}%
    \caption{Uncertainty Sampling Distributions for (a) Entropy, (b) Least Confident, (c) Margin of Confidence and (d) Ratio of Confidence metrics in a binary classification problem.
    The x-axis marks the probability of a triple belonging to the positive class $\mathbb{P}(y = 1 | l = (h,r,t))$ and the y-axis the uncertainty score $u(l)$ of the corresponding metric.}
    \label{fig:sampling_distributions}
\end{figure}

% describe sampling methods
% 1) always sample maximum uncertainty score
% 2) sample according to distribution
Based on these distribution functions, two different \textbf{Uncertainty Sampling Methods} can be derived.
On the one hand, the triple with the maximum uncertainty score can always be sampled, which we refer to in the following as \textbf{Uncertainty Sampling Max}.
As can be seen from the \autoref{fig:sampling_distributions} all metrics have the maximum uncertainty score of 1 at the high point of the function which is located at $P(y = 1 | f_G(l)) = 0.5$ with uncertainty score of 1, the closest triple is assigned the highest uncertainty score and is therefore sampled.
Therefore, always sampling the triple with maximum uncertainty results in sampling the same triple for all uncertainty metrics \cite{nguyen2021howtomeasure, human-in-the-loop}.

On the other hand triples can be sampled according to a probability distribution based on uncertainty scores.
This method is referred to as \textbf{Uncertainty Distribution}.
Thus, triples close to $P(y = 1 | f_G(l)) = 0.5$ still have the highest probability of being sampled, but other triples for which the model is more certain are also assigned a probability in the classification.
For the individual uncertainty metrics, the following definitions for $\phi$ and considerations for the two sampling methods result \cite{human-in-the-loop}:

\textbf{Entropy} considers the differences between all the predictions.
Like the other uncertainty metrics, the high point of the function is at $x = 0.5$, where the model is most uncertain about the classification.
However, it differs to the other metrics since it slopes flatter in both directions.
Therefore, if triples are sampled by Uncertainty Distribution method, it is more likely that triples will be sampled that are further away from $x=0.5$.
According to the definition of Entropy, the mapping function $\phi$ is defined as follows:
\begin{equation}
    \phi(x) = - \sum_{y \in \mathcal{Y}}{\mathds{P}_{\theta}(y | f_G(x)) \cdot log \mathds{P}_{\theta}(y | f_G(x))}
\end{equation}
Since in our case we are dealing with a binary classification with $y_i \in \{0,1\}$, this results in calculation:
\begin{equation}
= - \mathds{P}(y = 1| f_G(x)) log \mathds{P}(y = 1 | f_G(x))
- \mathds{P}(y = 0| f_G(x)) log \mathds{P}(y = 0 | f_G(x))
\end{equation}
\begin{equation}
= - \mathds{P}(y = 1| f_G(x) ) log \mathds{P}(y = 1 | f_G(x))
- ((1 - \mathds{P}(y = 1 | f_G(x)))
     log(1 - \mathds{P}(y = 1 | f_G(x))))
\end{equation}

\textbf{Least Confidence} 
measures the distance between the most confident prediction and 100\%.
Normalized for $n$ classes to an uncertainty score between 0 and 1 it is defined as follows:
\begin{equation}
    \phi(x) = (1 - \max_{y \in \mathcal{Y}}{\mathds{P}_{\theta}(y | f_G(x))}) \cdot \left(\frac{n}{n-1}\right)
\end{equation}
Since we have a binary classification, we can set $n = 2$ and $y \in \{0,1\}$:
\begin{equation}
    \phi(x) = (1 - \max({\mathds{P}_{\theta}(y = 1| f_G(x)), \mathds{P}_{\theta}(y = 0| f_G(x))})) \cdot 2
\end{equation}
\begin{equation} \label{eq:leastconfidence}
    = 2 - 2 \max({\mathds{P}_{\theta}(y = 1| f_G(x)), \mathds{P}_{\theta}(y = 0| f_G(x))})
\end{equation}
In contrast to entropy, the graph slopes much faster in the direction of $x = 0$ and $x = 1$, so that triples with lower uncertainty of the model are also sampled with lower probability with Uncertainty Distribution method.

\textbf{Margin of Confidence}
deals with the two most confident predictions where
$y_m = \argmax_{y \in \mathcal{Y}} \mathds{P}_{\theta}(y | f_G(x))$ 
and $y_n = \argmax_{y \in \mathcal{Y} \setminus y_m}{\mathds{P}_{\theta}(y | f_G(x))}$.
To convert this to a range between 0 and 1, we need to subtract it from 1:
\begin{equation}
    \phi(x) = 1 - (\mathds{P}_{\theta}(y_m |f_G(x)) - \mathds{P}_{\theta}(y_n | f_G(x)))
\end{equation}
Again, this can be further simplified due to the binary classification because for first case of $y_m = 1$ and $y_n = 0$ this results in
\begin{equation}
    \phi(x) = 1 - (\mathds{P}_{\theta}(y = 1 |f_G(x)) - \mathds{P}_{\theta}(y = 0 | f_G(x)))
\end{equation}
\begin{equation}
    = 1 - (\mathds{P}_{\theta}(y = 1 |f_G(x)) 
    -  (1 - \mathds{P}_{\theta}(y = 1 | f_G(x)))
\end{equation}
\begin{equation}
    = 1 - (2 \mathds{P}_{\theta}(y = 1 |f_G(x)) - 1)
\end{equation}
\begin{equation}
    = 2 - 2 \mathds{P}_{\theta}(y = 1 |f_G(x))
\end{equation}
and for second case if $y_m = 0$ and $y_n = 1$ we have  $ 2 - 2 \mathds{P}_{\theta}(y = 0 |f_G(x))$ . 
And therefore:
\begin{equation} \label{eq:marginofconfidence}
    = 2 - 2 \max(\mathds{P}_{\theta}(y = 0 |f_G(x)), \mathds{P}_{\theta}(y = 1 |f_G(x)))
\end{equation}
Since \autoref{eq:leastconfidence} and \ref{eq:marginofconfidence} are equal, for binary classification sampling by Margin of Confidence is the same as sampling by Least Confidence.

\textbf{Ratio of Confidence}
is quite similar to Margin of Confidence, but instead of the difference it compares the ratio of the two most confident predictions
\begin{equation}
    \phi(x) = \frac{\mathds{P}_{\theta}(y_m | f_G(x))}{\mathds{P}_{\theta}(y_n | f_G(x))}
\end{equation}
where $y_m$ and $y_n$ are equally defined as in Margin of Confidence.
Due to the ratio between the two most confident predictions $y_m$ and $y_n$, there is an even stronger focus on sampling only uncertain triples in the Uncertainty Distribution method around $x = 0.5$.


