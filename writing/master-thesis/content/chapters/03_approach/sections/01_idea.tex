\section{Idea} 
\label{sec:idea}

% 1) Presentation of my idea
% - origin of my idea
% - Reasons for Uncertainty - advantages and expected results
% - Why did I choose KBGAN (Sampling Negatives from Dynamic Distribution) 
%   as basis for my approach
% - disadvantages of KBGAN that I want to solve
% - why uncertainty sampling can solve them?

% Refer to Problem section that negative triples from current sampling approaches are "too easy"
% refer to uncertainty sampling origin + give most informative samples to model where models learns the most
% + effective in other areas
% -> adopt idea to negative sampling in KGE models
As we have seen in \autoref{sec:training_techniques}, there are several ways to learn a \ac{KGE}.
Many models learn embeddings by distinguishing positive triples from the \ac{KG} and negative triples generated by Negative Sampling.
Accordingly, although many models rely on Negative Sampling, most of these approaches provide only negative triples with insufficient quality (\autoref{sec:problem_analysis}).
With our approach, we aim to improve the Negative Sampling process for embedding models by incorporating uncertainty information and thus selecting negative triples that are more informative and more valuable for the embedding model.
Informative at this point means particularly interesting triples for the embedding model because those are difficult to classify as positive or negative triple.
Therefore they help the embedding model the most to differentiate negative from positive triples.

% why uncertainty sampling can solve current problems
% Uncertainty Sampling in other approaches -> results
Including uncertainty into various procedures and training models has already yielded promising results in recent work.
For example, this makes Active Learning more efficient by only querying particularly informative instances in the labeling process \autoref{sec:uncertaintysampling}.
There are many other Active learning query frameworks (\autoref{sec:uncertaintysampling}), but Uncertainty Sampling is the most commonly used one from Active Learning \cite{Settles2009ActiveLL}.
In addition, it is easier to implement than some other frameworks, since it does not require to train multiple models as it is the case with the Query-by-Committee approach \cite{Settles2009ActiveLL}.
Nevertheless, Uncertainty Sampling provides promising results for many Active Learning scenarios and is therefore used for our approach to sample from a negative triple set.
But also in the world of uncertain \acp{KG}, confidence scores for positive triples have already been included in \acp{URGE} \cite{UKGE}.
Accordingly, we also want to optimize the Negative Sampling process by incorporating uncertainty information.

% + Why did I choose KBGAN (Sampling Negatives from Dynamic Distribution) 
The basic idea of Sampling by Uncertainty is to select particularly interesting negative triples from an existing set of negative triples.
Accordingly, any approach of the three categories Static Distribution-Based Sampling (\autoref{subsec:static_distribution_based_sampling}, Custom Cluster-Based Sampling (\autoref{subsec:custom_cluster_based_sampling}) and Dynamic Distribution-Based Sampling (\autoref{subsec:dynamic_distribution_based_sampling}) can be selected.
Since the dynamic distribution-based approach \kbgan already creates such a set of negative triples and capture the distribution of negative triples dynamically, this approach will be extended by uncertainty information.
Moreover, it specifically suffers from inefficient sampling which slows down adversarial training.
For this reason, we aim to accelerate the learning process of \kbgan by selecting more informative triples for the negative triple set $Neg$ by calculating the uncertainty.












