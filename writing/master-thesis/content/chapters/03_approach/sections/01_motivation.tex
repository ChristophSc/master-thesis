\section{Motivation} 
\label{sec:motivation}

% 1) Presentation of my idea
% - origin of my idea
% - Reasons for Uncertainty - advantages and expected results
% - Why did I choose KBGAN (Sampling Negatives from Dynamic Distribution) 
%   as basis for my approach
% - disadvantages of KBGAN that I want to solve
% - why uncertainty sampling can solve them?

% Refer to Problem section that negative triples from current sampling approaches are "too easy"
% refer to uncertainty sampling origin + give most informative samples to model where models learns the most
% + effective in other areas
% -> adopt idea to negative sampling in KGE models
As we have seen in \Autoref{sec:training_techniques}, there are several ways to learn a \ac{KGE}.
Many models learn embeddings by distinguishing positive triples from a \ac{KG} and negative triples generated by negative sampling.
Accordingly, although many models rely on negative sampling, most of these approaches provide only negative triples with insufficient quality \cite{qiannegative}.
With our approach, we want to incorporate uncertainty information in a \ac{KGE} learning process and select negative triples that are more informative and more valuable for the embedding model.
Informative at this point means particularly interesting triples for the embedding model because those are difficult to classify as positive or negative triple.
Therefore they help the embedding model the most to differentiate negative from positive triples.

% why uncertainty sampling can solve current problems
% Uncertainty Sampling in other approaches -> results
Including uncertainty into various procedures and trainings of machine learning models has already yielded promising results in recent work.
For example, this makes active learning more efficient by only querying particularly informative instances in the labeling process.
There are many other active learning query frameworks, but uncertainty sampling is the most common one \cite{Settles2009ActiveLL}.
In addition, it is easier to implement than some other frameworks, since it does not require to train multiple models as it is the case with query-by-committee  \cite{Settles2009ActiveLL}.
Nevertheless, uncertainty sampling provides promising results for many active learning scenarios and is therefore used for our approach to sample from a negative triple set.
Besides this, in the world of uncertain \acp{KG}, confidence scores for positive triples have already been included in a \ac{URGE} \cite{UKGE}.
Accordingly, we also want to incorporate uncertainty information in an embedding learning process with negative sampling.

% + Why did I choose KBGAN (Sampling Negatives from Dynamic Distribution) 
The basic idea of sampling by uncertainty is to select particularly interesting negative triples from an already existing set of negative triples.
Since most embedding models from the three negative sampling categories static distribution-based sampling (\Autoref{subsec:static_distribution_based_sampling}, 
custom cluster-based sampling (\Autoref{subsec:custom_cluster_based_sampling}) and 
dynamic distribution-based sampling (\Autoref{subsec:dynamic_distribution_based_sampling})
can sample negative triples, they can also create a negative triple set,
Accordingly, any approach of the three categories can be selected and extended by uncertainty information.
Since the dynamic distribution-based approach \kbgan already creates such a set of negative triples and captures the distribution of negative triples dynamically, we have decided to extend this approach.
Therefore, instead of sampling based only on scoring values of negative triples, our approach will sample them based on uncertainty scores.
Since this new approach is based on a \ac{GAN} with an adversarial learning process where negative triples are sampled by uncertainty and given to discriminator, we call this approach \textbf{\underline{U}ncertainty \underline{S}ampling \underline{G}enerative \underline{A}dversarial \underline{N}etwork (\textsc{USGAN})}.
