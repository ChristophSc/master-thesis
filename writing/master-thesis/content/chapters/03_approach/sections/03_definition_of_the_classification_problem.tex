\section{Definition of a Classification Problem}
\label{sec:definition_of_a_classification_problem}
%
% JUSTIFICATION FOR A CLASSIFICATION
Since all uncertainty measures are based on a classification problem, this needs to be defined first.
Before this definition, we take a closer look at an example of uncertainty sampling in a \ac{SVM} (\Autoref{fig:svm}).
\begin{figure}[H]
  \centering
    \includegraphics[width=0.45\textwidth]{figures/SVM.pdf}
  \caption{Example of uncertainty sampling in a \ac{SVM} classification. Instances with least distance to linear plane are sampled.
  The width of boundary is maximized  (figure based on \cite{human-in-the-loop}).}
  \label{fig:svm}
\end{figure}
The figure shows a binary classification of labels A and B in a two-dimensional space.
Instances of classes A and B are separated as best as possible by a boundary.
Some of the instances are still unlabelled.
Now the instances that are most informative for the model and most helpful in defining the boundary between instances of class A and class B are sampled by uncertainty sampling.
The labeling of these instances in an active learning process helps to define a more precise boundary and thus enables a better distinction between instances with label A and label B.

In contrast to \acp{SVM}, \ac{KGE} models try to find the best possible embedding for a given set of entities and relations by distinguishing positive triples from the \ac{KG} and negative triples sampled by negative sampling.
What is the boundary between two labels in an \ac{SVM} is the scoring function in an embedding model.
In the case of a tensor factorization-based model, true triples achieve higher scores, and negative triples lower scores.
Therefore, a separation between positive and negative triples in a \ac{KGE} is achieved.
An optimal embedding would be able to perfectly distinguish between positive and negative triples.
However, since such embedding does not exist, the scoring function returns high scoring values for positive triples and low scores for negative triples.
Therefore, there is no clear 'decision boundary' between positive and negative instances.
To figure out uncertain cases of negative triples and apply uncertainty measures based on probabilities for different classes, the scoring values of the generator need to be converted into a classification problem.
Therefore, if the embedding model is uncertain about the classification of a negative triple, its uncertainty score will be high.
By sampling these uncertain triples to the discriminator it learns to distinguish between positive and uncertain negative triples.
Thus, uncertainty sampling is used to select instances based on a classification of positive and negative triples.
Since the generator is a tensor factorization-based model and their score indicates the plausibility of a triple to be true, the area of positive triple scores is slightly higher than the area of negative scores.
Nevertheless, since a perfect embedding model is unknown, there is an overlap of positive and negative triple scores.
Based on these different areas of the positive and negative triple scores, two different uncertainty areas can be defined.

The first possible definition of classification is depicted in \Autoref{fig:badVsGoodApproach}.
According to this definition, the classification is based only on low and high negative triple scores and therefore, uncertainty is only measured among all negative triple scores.
This way of defining the classification problem results in a sampling of instances that are easily distinguishable from the positive triples and therefore, less helpful to learn a good embedding.
For this reason, information about positive triple scores needs to be included.
\clearpage
\begin{figure}[H]
  \centering
    \includegraphics[width=0.9\textwidth]{figures/badVsGoodApproach.pdf}
  \caption{Illustration of sampled instances by uncertainty according to negative triple scores.
  Range of negative triple scores is in $[a, b]$ and range of positive triple scores is in  $[c, d]$ with $a,b,c,d \in \mathbb{R}$.}
  \label{fig:badVsGoodApproach}
\end{figure}

Therefore, the second option of the definition for a classification problem includes positive triple scores as well.
The resulting uncertainty area is illustrated in \Autoref{fig:positiveVsNegativeApproach}.
According to this approach, by defining class $y = 0$ for negative triples and class $y = 1$ for positive triples, each triple $l = (h,r,t)$ can be classified in a binary classification problem.
Accordingly, we can calculate the probability of being either a negative or a positive triple for each triple $\mathbb{P}(y| f_G(l)) \in [0,1]$ for $y \in \{0,1\}$.
Since we want to have as good negative triples as possible and therefore, triples that can be classified either positive or negative, we look for negative triples $l' = (h',r,t')$ that are close to the probability of $\mathbb{P}(y = 0| f_G(l')) = \mathbb{P}(y = 1| f_G(l')) = 0.5$.
Since this means the triple can be assigned to both the positive and the negative class.
Accordingly, the model is uncertain about how to classify the triple.
\begin{figure}[H]
  \centering
    \includegraphics[width=0.9\textwidth]{figures/positiveVsNegativeApproach.pdf}
  \caption{Illustration of the sampled instances by uncertainty according to negative and positive triple scores.
  Range of negative triple scores is in $[a, b]$ and range of positive triple scores is in  $[c, d]$ with $a,b,c,d \in \mathbb{R}$.}
  \label{fig:positiveVsNegativeApproach}
\end{figure}
% Now uncertainty sampling measures high uncertainty for triples which are close to positive ones, so which could be either a negative or a positive one.
After setting up the definition of the classification, the next step is to calculate the probabilities with which a triple belongs to a class.
\clearpage