\subsection{Negative Sampling Methods} 
\label{subsec:negative_sampling_methods}
%
In the literature, several negative sampling methods are proposed to create synthetic negative examples which are used for subsequent embedding learning.
They can be separated into the following three different groups \cite{qianunderstanding}.

\textbf{Static Distribution-Based Sampling} \label{static_distribution_based_sampling}\\
Static distribution-based sampling includes methods such as the uniform, Bernoulli, and probabilistic sampling techniques from a fixed and static distribution of negative triples.
While in uniform sampling either the head or tail entity is replaced by an entity randomly sampled from entity set \entities,
Bernoulli sampling uses different probabilities for replacing head or tail entity depending on the underlying relation type.
In contrast, probabilistic sampling speeds up the training process by including a training bias.
	
% Evaluation
Static distribution-based sampling approaches are commonly used because of their simplicity and efficiency, but ignore the dynamics in the negative sampling distribution \cite{qianunderstanding}.
This problem occurs when the gradient will be vanishing small and accordingly, small gradients prevent changing the weight value.
This can impede the training process or, in the worst case, completely stop the model from further training.
% RANDOM UNIFORM NEGATIVE SAMPLING
While negative triples, such as those generated by randomly replacing a head or tail entity, are very likely to be negative examples, they are generally uninformative and useless.
The problem with 'too easy' negative triples is less severe to models using the log-softmax loss function because they usually sample a high amount of negatives for one positive triple \cite{cai2017kbgan}.
However, the performance of marginal loss functions can be seriously damaged by the low quality of uniformly sampled negatives since the negative-to-positive ratio is always 1:1 \cite{cai2017kbgan}.


\textbf{Custom Cluster-Based Sampling} \label{subsubsec:custom_cluster_based_sampling}\\
In custom cluster-based sampling negative triples are sampled from small clusters which are based on the closeness between entities.
Instead of sampling from the whole set of entities, they are divided into a number of groups.
Subsequently, from this group, one entity is sampled to create a negative triple. 
Examples are TransE-\ac{SNS} \cite{TransE-SNS} or \ac{NSCaching} \cite{zhang2019nscaching} which are based on K-Means clustering algorithm or caching techniques. 

With custom cluster-based sampling methods more efficiency in the training process and suitable entities are found in a more targeted way.
By selecting entities only from a handful of candidates and not from the entire entity set, a better correlation between the positive and corresponding negative triple is expected.
Consequently, this negative triple should be closer to the original positive triple and provide more valuable information for the \ac{KGE} model.
For instance, domain sampling \cite{domainSampling} aims to sample entities within the same domain.
Therefore, if in a given positive triple \triple{Paderborn}{locatedIn}{Germany} the entity \texttt{Germany} is recognized as a country and that \texttt{France} as a country nearby \texttt{Germany}, the much more valuable negative triple \triple{Paderborn}{locatedIn}{France} can be sampled.
However, as \acp{KG} grow rapidly and are updated frequently, continuous renewing custom clusters is essential and difficult \cite{qianunderstanding}


\textbf{Dynamic Distribution-Based Sampling} \label{subsubsec:dynamic_distribution_based_sampling}\\
%
Dynamic distribution-based sampling tries to model the changes of negative triple distribution by using a \ac{GAN}-based framework which includes two components: 
A generator and a discriminator \cite{zhang2021efficient}.
While the generator dynamically approximates the constantly updated negative triple distribution to provide high-qualitative negative triples,
the discriminator learns to distinguish positive and negative triples with its own \ac{KGE} model.
Pioneer works are \ac{KBGAN} \cite{cai2017kbgan} and \ac{IGAN} \cite{IGAN}, but several other approaches such as \ac{MCNS} \cite{MCNS} exist.
\acp{GAN} were originally proposed for generating examples in a continuous space such as images.
This approach is adapted for the generation of hard negative examples \cite{zhang2021efficient}.
With its adaptation to \ac{KGE} learning, the distribution of negative triples is determined through the adversarial learning process.  

% Evaluation
Capturing the negative triple distribution has some drawbacks.
It is not effectively sampled from this distribution and the number of training parameters increases.
In addition, the model suffers from instability and degeneracy \cite{zhang2021efficient}.
This results from the fact, that the reinforce gradient has a high variance for training the generator and only a few negative triples lead to a large gradient \cite{zhang2021efficient}.
Consequently, the \ac{GAN}-based models have to put much effort to capture the negative triple distribution \cite{zhang2021efficient}.
However, even if only a few triple pairs consisting of a negative and a positive triple are useful for embedding learning, they are not maintained to be able to learn from them at a later time.
Moreover, no other information in the graph is included except the softmax-probability outputs of the generator and the \ac{KGE} model of the discriminator.