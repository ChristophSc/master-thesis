\section{Uncertainty Sampling} 
\label{sec:uncertaintysampling}
%
The use of uncertainty has already led to promising improvements in many areas of machine learning.
The term uncertainty sampling is most often associated with active learning, where labeled data for training a supervised model is obtained from a dataset of unlabeled instances \cite{Settles2009ActiveLL}.
Based on the informativeness of unlabeled instances for a learning algorithm, a prioritization results in which order they are labeled.
It is used in machine learning approaches, where unlabeled data is abundant, but it is difficult, time-consuming, or expensive to obtain labeled data \cite{Settles2009ActiveLL}.
Active learning aims for greater accuracy with fewer labeled training instances \cite{Settles2009ActiveLL}.
These selected unlabeled instances can either be generated de novo or sampled from a given distribution.
In the literature, several query strategies have been proposed with different approaches how to receive informative instances.
One of them is called uncertainty sampling.
Other query strategies for active learning are query-by-committee, expected model change, variance reduction, fisher information ratio, estimated error reduction, and density-weighted methods \cite{Settles2009ActiveLL}.

In uncertainty sampling, given a model $\theta$ which has been trained on labeled dataset $D$, each instance $x_j$ of unlabeled data pool $U$ will be assigned a utility score $s(\theta, x_j)$ \cite{nguyen2021howtomeasure}.
Subsequently, the instance with the highest score will be sampled.
Most common used uncertainty measures are the following \cite{human-in-the-loop}:
\begin{equation}
    \text{Entropy-based:}\hspace{15mm} \label{eqn:entropy_def}
     s(\theta, x) = - \sum_{y \in \mathcal{Y}}{p_{\theta}(y | x) \cdot log p_{\theta}(y|x)}
\end{equation}
\begin{equation}
    \text{Least confidence:}\hspace{27mm} \label{eqn:least_confidence_def}
     s(\theta, x) = 1 - \max_{y \in \mathcal{Y}}{p_{\theta}(y | x)}
\end{equation}
\begin{equation}
    \text{Margin of confidence:}\hspace{12mm} \label{eqn:margin_of_confidence_def}
    s(\theta, x) = p_{\theta}(y_m | x) - p_{\theta}(y_n|x)
\end{equation}
\begin{equation} \label{eqn:ratio_of_confidence_def}
    \text{Ratio of confidence:}\hspace{33mm}
    s(\theta, x) = \frac{p_{\theta}(y_m | x)}{p_{\theta}(y_n|x)}
\end{equation}
where
$y_m = \argmax_{y \in \mathcal{Y}} p_{\theta}(y | x)$ 
and 
$y_n = \argmax_{y \in \mathcal{Y} \setminus y_m}{p_{\theta}(y | x)}$ are the first and second most probable classes.

\Autoref{fig:uncertainty_sampling_heatmap} illustrates an example of target areas for the different uncertainty measures when there are three labels.
This shows, for example, that entropy measures samples in a much larger area than least confidence.
For measuring the uncertainty of a learner the following three different frameworks can be separated \cite{nguyen2021howtomeasure}.
\begin{figure}[H]
  \centering
    \includegraphics[width=0.9\textwidth]{figures/heat_map.pdf}
  \caption{Heat map of a multilabel classification with three labels for all four uncertainty measures (obtained from \cite{human-in-the-loop}).
  Each dot is an instance with a different label.}
  \label{fig:uncertainty_sampling_heatmap}
\end{figure}
\clearpage

\input{content/chapters/02_background/sections/08_uncertainty_sampling/01_EBU}

\input{content/chapters/02_background/sections/08_uncertainty_sampling/02_CU}

\input{content/chapters/02_background/sections/08_uncertainty_sampling/03_EAU}