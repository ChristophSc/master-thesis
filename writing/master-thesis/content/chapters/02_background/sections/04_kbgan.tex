\section{KBGAN} 
\label{sec:kbgan}
%
\begin{figure}[H]
  \centering
    \includegraphics[width=0.95\textwidth]{figures/kbgan_original.png}
  \caption{An overview of the \textsc{kbgan} framework (based on: \cite{cai2017kbgan}).}
  \label{fig:overview}
\end{figure}
In the course of this work, uncertainty information is included in an existing negative sampling process.
For this purpose, the pioneering work of dynamic distribution-based negative sampling methods \kbgan is selected.
The embedding is learned with an adversarial training process which consists of two components: a generator and a discriminator.
This process of \kbgan is depicted in \autoref{fig:overview} and described as follows \cite{cai2017kbgan}:
\begin{enumerate}
    \item 
    At first, a set of sampled corrupted triples $Neg(h,r,t)\subset\{(h',r,t) \text{ } | \text{ } h' \in \mathcal{E}\} \cup \{(h,r,t') \text{ } | \text{ } t'\in\mathcal{E}\}$ is created.
    Ideally, this set contains all possible negative triples \cite{cai2017kbgan}.
    Since \acp{KG} are usually highly incomplete, some of them might be false negatives that are actually true facts.
    Therefore, \kbgan uniformly samples only $N_s$ entities and replaces either head entity \texttt{h} or tail entity \texttt{t} of a given positive triple \triple{h}{r}{t}.
    In comparison to the number of all possible negatives, $N_s$ is a relatively small number \cite{cai2017kbgan}.
    In the original approach, $N_s$ is set to twenty.
    For simplicity, in our example (\autoref{fig:overview}) the negative triple set   $Neg$ contains only five negative triples.
    For the positive triple \triple{NewOrleans}{LocatedIn}{Louisiana} five negative triples
    are created by replacing the tail entity \texttt{Louisiana}.
    This results in the following negative triples:
    \triple{NewOrleans}{LocatedIn}{Florida},
    \triple{NewOrleans}{LocatedIn}{China},
    \triple{NewOrleans}{LocatedIn}{BarackObama},
    \triple{NewOrleans}{LocatedIn}{StarTrek}, and 
    \triple{NewOrleans}{LocatedIn}{Google}.
    
    \item 
    These negative triples from $Neg$ are given to the generator G as input.
    
    \item 
    The generator G receives all negative triples from the negative triple set $Neg$ and scores them with the scoring function $f_G$ of G.
    Since G is a tensor factorization-based \ac{KGE} model such as \distmult and \textsc{ComplEx}, negative triples with higher scores indicate a higher plausibility to be true.
    The scores are passed to the softmax function, which assigns a sampling probability $p_i$ for each negative triple $l_i = (h_i, r_i, t_i)$ \cite{cai2017kbgan}:
    \begin{equation} \label{eq:origsampling}
        p_i = \frac{\exp{f_G(l_i)}}{\sum_{j=1}^{N_s}{\exp{f_G(l_j)}}} \in [0,1]
    \end{equation}
    Therefore, a probability distribution $p_G(h',r,t'|h,r,t)$ is obtained where for each negative triple \triple{h'}{r}{t'} a probability is assigned based on a given positive triple \triple{h}{r}{t}. 
    All sampling probabilities sum up to 1.
    
    \item
    According to this probability distribution, one negative triple is sampled for each positive triple.
    Since generator G assigns higher sampling probabilities to negative triples with high scores, it is more probable to sample a negative triple that achieved a high score.

    \item 
    Afterwards, the sampled negative triple \triple{h'}{r}{t'} and corresponding positive triple \triple{h}{r}{t} are given to discriminator D.
    In our example, the positive triple is \triple{NewOrleans}{LocatedIn}{Louisiana} and the sampled negative triple is \triple{NewOrleans}{LocatedIn}{Florida}.
    
    \item 
    Discriminator D receives as input the sampled negative triple \triple{h'}{r}{t'} and the ground truth triple \triple{h}{r}{t}.
    The \ac{KGE} model of discriminator D scores with its scoring function $f_D$ both triples, the received positive and negative one.
    Since discriminator D is a translation-based \ac{KGE} model, it returns distances where a lower distance indicates a higher likelihood of truth and vice versa \cite{cai2017kbgan}.
    Therefore, the distance of the received positive triple \triple{h}{r}{t} should be small and if it was a good negative triple \triple{h'}{r}{t'}, its distance should be small as well.
    In our example, the distance $d = 1.0$ is calculated for the positive triple and $d = 2.0$ for the sampled negative triple.
     
    \item 
    The training objective of D is a marginal loss function.
    Therefore, the marginal loss is calculated from positive and negative triple distances \cite{cai2017kbgan}:
     \begin{equation}
        L_{marginal}
        =\sum_{(h,r,t)\in\kg}[f(h,r,t)-f(h',r,t')+\gamma]_+ 
        = \sum_{(h,r,t)\in\kg}[d_p-d_n+\gamma]_+ 
    \end{equation}
    where $d_p = f(h,r,t)$ is the distance of positive triple \triple{h}{r}{t} and $d_n = f(h',r,t')$ the distance of the negative triple  \triple{h'}{r}{t'}.
    Therefore, the better the sampled negative triple is, the lower is its distance and consequently a lower marginal loss is achieved.
    
    \item 
    To give generator G a feedback for the sampled negative triple \triple{hâ€™}{r}{t'} a reward $r$ is send back to G, which is defined as \cite{cai2017kbgan}:
    \begin{equation}
        r = -f_D(h',r,t') \in \mathbb{R}^-.
    \end{equation}
    Therefore, the lower the distance of a negative triple is, the higher the reward. 
    Consequently, maximizing the following expectation of negative distances defines the objective of G \cite{cai2017kbgan}:
    \begin{equation}
        R_G=\sum_{(h,r,t)\in\kg}\mathbb{E}[-f_D(h',r,t')]
    \end{equation}
    
    \item
    This process ends until the defined number of epochs has been reached.
\end{enumerate}
\clearpage