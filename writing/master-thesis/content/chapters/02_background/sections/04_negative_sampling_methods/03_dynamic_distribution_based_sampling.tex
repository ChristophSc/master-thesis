\subsection{Dynamic Distribution-Based Sampling}
\label{subsec:dynamic_distribution_based_sampling}

Dynamic distribution-based sampling tries to model the changes of negative triple distribution by using a \ac{GAN}-based framework which includes two components: 
A generator and a discriminator \cite{zhang2021efficient}.
While the generator dynamically approximates the constantly updated negative triple distribution to provide high-qualitative negative triples, 
the discriminator learns to distinguish positive and negative triples with its own \ac{KGE} model.
Pioneer works are \ac{KBGAN} \cite{cai2017kbgan} and \ac{IGAN} \cite{IGAN}, but several other approaches like \ac{MCNS} \cite{MCNS} exist.
\acp{GAN} were originally proposed for generating examples in a continuous space such as images and adapted for the generation of hard negative examples \cite{zhang2021efficient}.
With its adaptation to \ac{KGE} learning, the distribution of negative triples is determined through the adversarial learning process.  

\textbf{Evaluation}:\\
% DYNAMIC DISTRIBUTION-BASED SAMPLING
However, capturing the negative triple distribution has some drawbacks.
It is not effectively sampled from this distribution and the number of training parameters increases.
In addition, the model suffers from instability and degeneracy \cite{zhang2021efficient}.
This results from the fact, that the \textsc{REINFORCE} gradient has high variance for training the generator and only a few negative triples lead to large gradient \cite{zhang2021efficient}.
Consequently, the \ac{GAN}-based models have to put a lot of  effort to capture the negative triple distribution \cite{zhang2021efficient}.
However, even if only a few triple pairs (negative + positive triple) are useful for learning embedding, they are not maintained to be able to learn from them at a later time.
Moreover, no other information of the graph is included except the softmax-probability outputs of the generator and the \ac{KGE} model of the discriminator.