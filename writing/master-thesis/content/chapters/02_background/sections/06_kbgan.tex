\section{KBGAN} 
\label{sec:kbgan}

In the group of \textit{Dynamic Distribution-Based Sampling}, \ac{KBGAN} is one of the pioneering works among \ac{GAN}-based approaches and contains two components which are trained in an adversarial training process.
This process of \ac{KBGAN} is depicted in \autoref{fig:overview} and described as follows:
\begin{figure*}[t]
  \centering
    \includegraphics[width=0.95\textwidth]{figures/kbgan_original.png}
  \caption{An overview of the \textsc{kbgan} framework (based on: \cite{cai2017kbgan})}
  \label{fig:overview}
\end{figure*}

\begin{enumerate}
    \item 
    At first, a set of sampled corrupted triples $Neg(h,r,t)\subset\{(h',r,t)|h'\in\mathcal{E}\}\cup\{(h,r,t')|t'\in\mathcal{E}\}$ is created by uniformly Sampling of $N_s$ entities $\in$ \entities to replace $h$ or $t$.
    For reasons of efficiency and to reduce the likelihood of creating false negatives, $N_s$ is a relatively small number compared to the number of all possible negative examples \cite{cai2017kbgan}.
    
    \item 
    Negative triples from $Neg$ are given to the generator G as input.
    
    \item 
    $G$ is a \ac{KGE} model with a softmax function to provide  probabilities of being sampled for each given negative triple, which gives us a probability distribution $p_G(h',r,t'|h,r,t)$.
    Other models are also possible at this point, but the approach would then have to be adapted, so that it outputs a probability distribution over all negative triples of the set $Neg$ as well.

    \item 
    Afterwards, one of these triples is sampled as output of G according to $p_G$, while high quality triples should have a high probability to be sampled.
    
    \item 
    Discriminator D receives as input both the sampled negative triple \triple{h'}{r}{t'} and the ground truth triple \triple{h}{r}{t}.
    The training objective of D is a marginal loss function, because these benefit most from high quality negative examples \cite{cai2017kbgan}.

    \item 
    Subsequently, D calculates the scores of both triples \triple{h}{r}{t} and \triple{h’}{r}{t'} with score function $f_D$ of D, which models distance between points or vector.
    Therefore, a smaller distance indicates a higher likelihood of truth \cite{cai2017kbgan}.
    
    \item 
    Accordingly, the objective of D is to minimize the marginal loss function $L_D$ which is defined as
    \begin{equation}
        L_D=\sum_{(h,r,t)\in\mathcal{T}}[f_D(h,r,t)-f_D(h',r,t')+\gamma]_+ \in \mathbb{R}^+
    \end{equation}
    
    \item 
    To give G feedback for the sampled negative triple \triple{h’}{r}{t'} its calculated score $f_D(h',r,t')$ is send back to G as a reward, which is defined as
    \begin{equation}
        r = -f_D(h',r,t') \in \mathbb{R}^-
    \end{equation}
    since $f_D(h,r,t) \in \mathbb{R}^+$.
    Therefore, the objective of G can be formulated as maximizing the expectation of negative distances:
    \begin{equation}
        R_G=\sum_{(h,r,t)\in\mathcal{T}}\mathbb{E}[-f_D(h',r,t')]
    \end{equation}
    
    \item
    This process continues until convergence
\end{enumerate}
The dynamic distribution of the negative triple is determined through this procedure of the adversarial training, 





%\cite{cai2017kbgan}
%- probability-based, log-loss embedding models as the generator
%- distance-based, margin-loss embedding as discriminator
