\section{State of the Art}
\label{sec:related}

%################################################
%################################################

\subsection{Negation in Existing Knowledge Bases}
\label{sec:existing}

%Before discussing research works, we review the state of the art of negation representation in existing popular KBs.
%GW: unnecessary intro, the headings tell everything already

%\GW{This entire subsection is too long and too verbose. The emphasis is on one example after the other -- the general point are not made concisely. Usually, I am all in favor of illustrative examples, but in the Related Work section they should be kept to a minimum.}\ha{**I'll work on shortening it, but should I keep the paragraphs and shorten their content or merge them together?}\\

%SR: Done, I removed many examples

\noindent
\textbf{Deleted Statements.\ }
Statements that were once part of a KB but got subsequently deleted are promising candidates for negative information~\cite{edithistory2019}. As an example, we studied deleted statements between two Wikidata versions from 1/2017 and 1/2018, focusing in particular on statements for people (close to 0.5m deleted statements). On a random sample of 1k deleted statements, we found that over 82\% were just caused by ontology modifications, granularity changes, rewordings, or prefix modifications. %, such as: \term{(Ghandi; lifes\-tyle; Vegetarian)} changed to \term{(Ghandi; lifestyle; Veget\-arianism)}, \term{(Ghandi; place of death; New Delhi)} changed to \term{(Ghandi; place of death; Gandhi Smriti)}, and \term{(James Green; oxfordID; 101011386)} to \term{(James Green; oxfordID; 11386)}. 
Another 15\% were statements that were actually restored a year later, so presumably reflected erroneous deletions. The remaining 3\% represented actual negation, yet we found them to be rarely noteworthy, i.e., presenting mostly things like corrections of birth dates or location updates reflecting geopolitical changes.

In Wikidata, erroneous changes can also be directly recorded via the deprecated rank feature~\cite{MKGGB2018}. Yet again we found that this mostly relates to errors coming from various import sources, and did not concern the active collection of interesting negations, as advocated in this article.%, like that ``Stephen Hawking did not win the Nobel Prize in Physics.''} 

\noindent
\textbf{Count and Negated Predicates.\ }
Another way of expressing negation is via counts matching with instances, for instance, storing 5 children statements for \textit{Trump} and numerical statement \term{(number of children; 5)} allow to infer that anyone else is not a child of \textit{Trump}. Yet while such count predicates exist in popular KBs, none of them has a formal way of dealing with these, especially concerning linking them to instance-based predicates~\cite{ghoshSWJ}.

Moreover, some KBs contain relations that carry a negative meaning. For example, DBpedia has predicates like \emph{carrier never available} (for phones), %, e.g., \term{(LG Shine; carrier never available; --02-07)} and 
or \emph{never exceed alt} (for airplanes), %, e.g., \term{(Piper PA 48 Enforcer; never exceed alt; 350)}. Another example is the biomedical KB 
Knowlife~\cite{ernst2015knowlife} contains medical predicates like \emph{is not caused by} %, e.g., term{(asthma; is not caused by; exercise pain management)} 
and \emph{is not healed by}, %, e.g., term{(asthma; is not healed by; brovana)}. A third example is Wikidata, for instance 
and Wikidata contains \emph{does not have part} and % (243 statements), or 
\emph{different from}. % (492K statements). 
Yet these present very specific pieces of knowledge, and do not generalize. %, e.g., \term{(arm; does not have part; hand)}, \term{(Hover Church; does not hav\-e part; bell tower)}, and \term{(brain death; different from; d\-eath)} which do not generalize to other Wikidata properties. 
Although there have been discussions to extend the Wikidata data model to allow generic opposites\footnote{\url{https://www.wikidata.org/wiki/Wikidata:Property_proposal/fails_compliance_with}}, these have not been worked out so far.

\noindent
\textbf{Wikidata No-Values.\ }
Wikidata can capture statements about \textit{universal absence} via the ``no-value'' symbol~\cite{erxleben2014introducing}. This allows KB editors to add a statement where the object is empty. For example, what we express as \term{$\neg \exists x$(Angela Merkel; child; x)}, the current version of Wikidata allows to be expressed as \term{(Angela Merkel; child; no-value)}\footnote{\url{https://www.wikidata.org/wiki/Q567}}. As of 8/2021, there exist 135k of such ``no-value'' statements, yet only used in narrow domains. For instance, 53\% of these statements come for just two properties \textit{country} (used almost exclusively for geographic features in Antarctica), and \textit{follows} (indicating that an artwork is not a sequel).


%\noindent
%\newtext{\textbf{Ongoing discussions.\ }
%An interesting discussion took place on the \textit{Wikidata's Project Chat} webpage\footnote{\url{https://www.wikidata.org/wiki/Wikidata:Property_proposal/fails_compliance_with}} about the need for an ``opposite'' to a property. More particularly, the opposite of the property \emph{complies with (P5009)} to state when an entity does not comply with the criterion associated with an entity. There is a way of stating that the film \term{Beauty and the Beast; complies with; the Bechdel test}. However, stating that the film ``Hackers'' fails to comply with the ``Bechdel test'' cannot be stated by simply negating the property \emph{complies with}, but through a workaround that introduced the negative entity (object) \emph{``fails the Bechdel Test (Q45172088)''}, and then stating that ``Hackers'' \emph{has quality} \emph{``fails the Bech\-del Test (Q45172088)''}. This, however, is not a practical nor generalizable way to deal with every possible negation that the KB presents.}






\subsection{Negation in Logics and Data Management}

%\GW{you need to spell out CWA and OWA upon first mention;
%you need to define and explain PCA;
%I prefer LCA (local closed world assumption) over the ambiguous acronym PCA)}\ha{** wait, aren't we confusing two different concepts here? the first is when your peer has it, you should have it. So if Einstein has the nobel, Hawking should. Otherwise it's negative, and that's what we describe as LCWA. The other concept is in order to go forward with a candidate negation, the entity should have at least one other object for the same predicate. If Hawking has won at least one other award, then we proceed with the candidate he has not won the nobel. We are referring to this as PCA.}\\

Negation has a long history in logics and data management. Early database paradigms usually employed the closed-world assumption (CWA), i.e., assumed that all statements not stated to be true were false \cite{Reiter78}, \cite{ICWA}. On the Semantic Web and for KBs, in contrast, the open-world assumption (OWA) has become the standard. The OWA asserts that the truth of statements not stated explicitly is unknown. Both semantics represent somewhat extreme positions, as in practice it is neither conceivable that all statements not contained in a KB are false, nor is it useful to consider the truth of all of them as unknown, since in many cases statements not contained in KBs are indeed not there because they are known to be false~\cite{razniewskilimits}. Between these two assumptions, there is also the so-called local (partial) closed-world assumption~\cite{RPZ2010c}, where the open-world assumption is used in general, while the  closed-world assumption can be applied to some predicates (classes or properties).

In limited domains, logical rules and constraints, such as Description Logics \cite{logichandbook}, \cite{Calvanese2007} or OWL, can be used to derive negative statements. An example is the statement that every person has only one birth place, which allows to deduce with certainty that a given person who was born in \textit{France} was not born in \textit{Italy}. OWL also allows to explicitly assert negative statements \cite{mcguinness2004owl}, yet so far is predominantly used as ontology description language and for inferring intensional knowledge, not for extensional information (i.e., instances of classes and relations), with a few exceptions, like the rewriting based approach to instance retrieval for negated concepts, based on the notion of inconsistency-based first-order-rewritability~\cite{DuPa2015}. Different levels of negations and inconsistencies in Description Logic-based ontologies are proposed in a general framework~\cite{FHPPW2006}.

In~\cite{DBLP:journals/amai/AnalytiADP13,Analyti04negationand}, a thorough study on negative information in the Resource Description Framework (RDF) argues in favor of explicit negation. In particular, it makes the point that any knowledge representation formalism must be able to deal with \textit{informative} negative information, on top of informative positive information. The authors then propose ERDF (extended RDF), where an ERDF triple can be either positive or negative. The framework also distinguishes between two kinds of negation: weak (``she doesn't like snow'') and strong (``she dislikes snow''). The former is denoted using the \textapprox \   symbol, and the latter using the $\neg$ symbol.

The notion of \texttt{noValue} in RDF was introduced in~\cite{DBLP:books/aw/AbiteboulHV95}. It has been recently adapted in~\cite{DBLP:conf/semweb/DarariPN15} for representing no-value information in RDF and incorporating such information into query answering. The intuition behind it is to distinguish whether a result set of a SPARQL query is empty due to lack of information or actual negation.

The AMIE framework \cite{galarraga2017predicting} employed rule mining to predict the completeness of properties for given entities. This corresponds to learning  whether the CWA holds in a local part of the KB, inferring that all absent values for a subject-predicate pair are false. For our task, this could be a building block, but it does not address the inference of {\em useful} negative statements.

RuDiK~\cite{ortona2018rudik} is a rule mining system that can learn rules with negative atoms in rule heads (e.g., people born in \textit{Germany} cannot be
\textit{U.S.} president). This could be utilized towards predicting negative statements. %Unfortunately, the mining also discovers many convoluted and exotic rules (e.g., people whose body weight is less than their birth year cannot win the Nobel prize), often with  a large number of atoms in the rule body, and such rules are among the top-ranked ones. 
%Even good rules, such as ``people with birth year after 2000 do not win the Nobel prize'', are not that useful for our task. 
Unfortunately, such rules predict way too many -- correct, but uninformative -- negative statements, essentially enumerating a huge set of people who are not \textit{U.S.} presidents. 
%\GW{do we really need this lengthy example? what is the key point that matters for the paper's big picture? is there any?}\ha{**}\\
The same work also proposed a precision-oriented variant of the CWA that assumes negation only if subject and object are connected by at least one other relation. Unfortunately, this condition is rarely met in interesting cases. For instance, most of the negative statements in Table~\ref{tbl:may:einstein} have alternative connections between subject and object in Wikidata.




\subsection{Related Areas}

\noindent
\textbf{\small Linguistics and Textual Information Extraction (IE).\ } Negation is an important feature of human language \cite{Morante2012}. While there exists a variety of ways to express negation, state-of-the-art methods are able to detect quite reliably whether a segment of text is negated or not \cite{extendingnegex}, \cite{wu2014}.  There is also work on using knowledge graphs to help detect false statements in texts, such as news~\cite{PPLL+2018}. %Yet theories of conversational schemes indicate that negative statements can also be inferred from sentences that do not contain explicit negation: For instance, following Grice's maxims of cooperative communication \cite{grice1975logic}, a reasonable conclusion from the sentence ``John has two children, Mary and Bob'' is that nobody else is a child of John. Such inferences are called scalar implicatures, and they play a considerable role in language pragmatics~\cite{carston1998informativeness}.
%

A body of work targets negation in medical data and health records. In \cite{cruzdiaz}, a supervised system for detecting negation, speculation and their scope in biomedical data is developed, based on the annotated BioScope corpus \cite{bioscope}.
In \cite{Goldin03learningto}, the focus is on negations via the keyword ``not''. The challenge here is the right scoping, e.g., ``Examination could not be performed due to the Aphasia'' does not negate the medical observation that the patient has Aphasia.
In \cite{KEhealth}, a rule-based approach based on NegEx~\cite{CHAPMAN}, and a vocabulary-based approach for prefix detection were introduced.
PreNex \cite{PRENEX} also deals with negation prefixes. The authors propose to break terms into prefixes and root words to identify this kind of negation. They rely on a pattern matching approach over medical documents. 



In \cite{AKB}, an anti-knowledge base containing negations is mined from Wikipedia change logs, with the focus however being again on factual mistakes, and precision, not interestingness, is employed as main evaluation metric. In~\cite{CSKB}, the focus is to obtain meaningful negative samples for augmenting commonsense KBs.
We explore text extraction in more details in the proposed \emph{pattern-based query log extraction} method in our earlier conference publication~\cite{negationakbc}.


\noindent
\textbf{Statistical Inference and KB Completion.\ } As text extraction often has limitations, data mining and machine learning are frequently used on top of extracted or user-built KBs, in order to detect interesting patterns in existing data, or in order to predict statements not yet contained in a KB. There exist at least three popular approaches, rule mining, tensor factorization, and vector space embeddings \cite{KGembsurvey}. Rule mining is an established, interpretable technique for pattern discovery in structured data, and has been successfully applied to KBs for instance by the AMIE system \cite{AMIE3}. Tensor factorization and vector space embeddings are latent models, i.e., they discover hidden commonalities by learning low-dimensional feature vectors \cite{global2014}. To date, all these approaches only discover positive statements. On the other hand, if one considers logical entailments as a means to enhance such rule mining and latent model based approaches, such as in an iterative manner~\cite{WPKD2020}, negative statements in theory can be discovered with the help of disjoint axioms; however, the quality of knowledge graph completion methods still have room for improvement. Recently, an inference model has been proposed to build a knowledge graph with commonsense contradictions~\cite{ANION}, like ``Wearing a mask is seen as responsible'' is the contradiction of ``Not wearing a mask is seen as carefree''.

\noindent
\textbf{Ranking KB Statements.\ } In applications such as entity summarization over web-scale KBs, returned result sets are often very large. Ranking statements is a core task in managing access to KBs, with techniques often combining generative language-models for queries on weighted and labeled graphs~\cite{NAGA,Yahya2016,arnaoutjws2018}. In \cite{Bast}, the authors propose a variety of functions to rank values of type-like predicates. These algorithms include retrieving entity-related texts, binary classifiers with textual features, and counting word occurrences.  In \cite{huang2019contextual}, the focus is on identifying the informativeness of statements within the context of the query, by exploiting deep learning techniques. In this work, applications such as entity summarization returns a set of \textit{negative} statements. To assign each statement a relevance score, we use a mixture of the metrics that are usually used for ranking positive statements (e.g., frequency of property), and metrics that are specific for negative statements (e.g., unexpectedness).


%\newtext{Although these approaches has not tackled the specifics of negative statements, we use some of the proven useful features in our work, namely frequency and popularity of entities and relations. On top of that, we introduce additional ranking metrics.}
%%%GW: unnecessary generic text, no extra information



