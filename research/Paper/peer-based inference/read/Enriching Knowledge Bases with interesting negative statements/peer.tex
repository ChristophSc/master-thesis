\section{Peer-based Statistical Inference}
\label{sec:inference}
%\ha{we need to rename our methods, emphasize the peer group (but they both include peers so I am not sure; we can call the peer groups something else in the temporal method}
We next present a method to derive useful negative statements by combining information from similar entities (``peers'') with supervised calibration of ranking heuristics. The idea is that peers that are similar to a given entity can give expectations on relevant statements that \textit{should} hold for the entity. For instance, several entities similar to the physicist \emph{Stephen Hawking} have won the \textit{Nobel in Physics}. We may thus conclude that him not winning this prize could be an especially useful statement. Yet related entities also share other traits, e.g., many famous physicists are \textit{U.S.} citizens, while \textit{Hawking} is \textit{British}. We thus need to devise ranking methods that take into account various clues such as frequency, importance, unexpectedness, etc.\\

\noindent
\textbf{Peer-based Candidate Retrieval.\ }
To scale the method to web-scale KBs, in the first stage, we compute a candidate set of negative statements using the CWA on certain parts of the KB, to be ranked in the second stage. Given a subject $e$, we proceed in three steps:
\begin{enumerate}[noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt]
    \item \textit{Obtain peers:} We collect entities that set expectations for statements that $e$ could have, the so-called \textit{peer groups} of $e$. These groups can be based on (i) structured facets of the subject~\cite{RECOIN}, such as \textit{occupation, nationality}, or \emph{field of work} for people, or classes/types for other entities, (ii) graph-based measures such as distance or connectivity~\cite{ponza}, or (iii) entity embeddings such as TransE~\cite{transE}, possibly in combination with clustering, thus reflecting latent similarity. 
    \item \textit{Count statements:} We count the relative frequency of all predicate-object pairs (i.e., \term{(\_,p,o)}) and predicates (i.e., \term{(\_,p,\_)}) within the peer groups, and retain the maxima, if candidates occur in several groups. This way, statements are retained if they occur frequently in at least one of the possibly orthogonal peer groups.
    \item \textit{Subtract positives:} We remove those predicate-object pairs and predicates that exist for $e$.
\end{enumerate}
\  \\
Algorithm~\ref{alg:peer} shows the full procedure of the peer-based inference method. 
In line 2, groups of peers $P[]$ are selected based on some blackbox function \textit{peer\_groups}.
\begin{equation*}
P = [P_1, ... P_n] \text{, with } n>=1.
\end{equation*}
\noindent
Every group $P_i$ is a set of peers, defined as follows.
\begin{equation*}
P_i = \{pe_1, ..., pe_m\} \text{, with } m<=s.
\end{equation*}
Subsequently, for each peer group, it collects all the positive information that these peers have (line 6 and 7), and stores them as a list of candidate statements.
\begin{equation*}
candidates = \{st_1, ..., st_w\}.
\end{equation*}
A statement $st_j$ in $candidates$ is either a predicate P or a predicate-object pair PO.
After collecting information about the peers, the loop at line 10 iterates over the list of unique statements $ucandidates$, computes their relative frequency, and stores them in the final list of negations $N$. $N$ is a list of negation objects~\footnote{Here, object is meant as a data type and not a KB-triple object.}, where every object consists of a negation statement and its score.
\begin{equation*}
N = [(\neg st_1, sc_1), ..., (\neg st_r, sc_r)].
\end{equation*}
Across peer groups, it retains the maximum relative frequencies (hence, line 13), if a property or statement occurs across several. Before returning the top $k$ results as output (line 18), it subtracts those already possessed by entity $e$ (line 17).

\begin{algorithm*}[t!]
    \SetKwInOut{Input}{Input}
    \SetKwInOut{Output}{Output}
    \Input{\small knowledge base $\mathit{KB}$, entity $e$, peer collection function \textit{peer\_groups}, \small max. size of a peer group $s$, \small number of results $k$}
    \Output{ \small $k$-most frequent negative statement candidates for $e$}
        \textit{\textbf{P}}$[]$= \textit{peer\_groups}$(e, s)$ \Comment{List of peer group(s); Group $P_i$ at position $i$ is one group (set) with at most $s$ peers.}\\
         \textit{\textbf{N}}$[]$= $\emptyset$ \Comment{\small Ranked list of negative statements about $e$.}\\
    \For{$P_i$ $\in$ \textbf{P}}{ 
        $candidates$ = [] \Comment{\small Positive statements (i.e., predicate and predicate-object pairs) of $P_i$ members.}\\
        \For{$pe$ $\in$ $P_i$}{ 
            $candidates$+=$collectP(pe)$ \Comment{\small Collecting predicates that hold for one peer (pe).}\\
            $candidates$+=$collectPO(pe)$ \Comment{\small Collecting predicate-object pairs that hold for pe.}\\
        }
        $ucandidates$ = $unique(candidates)$\Comment{\small List of unique statements in $candidates$.}\\
        \For{$st$ $\in$ $ucandidates$}{
            $sc$ = $\frac{count(st, candidates)}{s}$ \Comment{\small sc computes how many peers share the statement st, normalized by $s$.}\\
            \If{$getnegation(N, st).score$ $<$ $sc$}{ $setscore(N, st, sc)$}  
        }
     }
        $N$-=$\mathit{inKB}(e,N)$ \Comment{\small Remove statements $e$ already has.}\\
        return $max(N,k)$
\caption{Peer-based candidate retrieval algorithm.}
\label{alg:peer}
\end{algorithm*}


\begin{example}
Consider the entity $e$=\textit{Brad Pitt}. Table \ref{tab:brad} shows a few examples of his peers and candidate negative statements.
We instantiate the peer group choice to be based on structured information, in particular, shared occupations with the subject, as in Recoin~\cite{RECOIN}. In Wikidata, \textit{Pitt} has 9 occupations, thus we would obtain 9 peer groups of entities sharing one of these with \textit{Pitt}.
\begin{equation*}
P = [\text{actors, film directors, ..., models}] \text{, with } n=9.
\end{equation*}
For readability, let us consider statements derived from only one of these peer groups, \emph{actor}. Let us assume 3 entities in that peer group.
\begin{equation*}
P_\text{actor} = \{\text{Russel Crowe, Tom Hanks,  Denzel Washington}\}
\end{equation*}
The list of negative candidates, $candidates$, are all the predicate and predicate-object pairs shown in the columns of the 3 actors. And in this particular example, $N$ is just $ucandidates$ with scores for only the \textit{actor} group.
\begin{equation*}
\begin{split}
N = [(\neg (\text{award; Oscar for Best Actor}), 1.0),\\  (\neg \exists x(\text{instagram; }x\text), 0.67),\\ 
(\neg (\text{citizen; New Zealand}), 0.33),\\ 
(\neg \exists x(\text{convicted; }x), 0.33),\\ 
(\neg \exists x(\text{child; }x), 1.0),\\  (\neg(\text{occupation; screenwriter}), 1.0),\\  (\neg(\text{citizen; U.S.}), 0.67)].
\end{split}
\end{equation*}
Candidates that \textit{hold} for \textit{Pitt} are then dropped.
\begin{equation*}
\begin{split}
N = [(\neg (\text{award; Oscar for Best Actor}), 1.0),\\  (\neg \exists x(\text{instagram; }x), 0.67),\\ 
(\neg (\text{citizen; New Zealand}), 0.33),\\ 
(\neg \exists x(\text{convicted;} x), 0.33),\\  (\neg(\text{occupation; screenwriter}), 1.0)].
\end{split}
\end{equation*}
 The top-k of the rest of candidates in $N$ are finally returned. The top-3 negative statements, for this example, are \term{$\neg$(award; Oscar for Best Actor)}, \term{$\neg$(occupation; screenwriter)}, and \term{$\neg \exists x$(instagram; x)}. 

The ``if'' statement at line 12 is only needed when multiple peer groups are considered for an entity. In the case where a negative statement is inferred from more than 1 group, only the version with the highest score is added to the final set. In the original (\textit{full}) example, \textit{Pitt} belongs to the group \textit{actor} and the group \textit{model}. The negation \term{$\neg$(occupation; screenwriter)} was inferred twice, once from each group, with a relative frequency of 0.9 from the \textit{actor} group and 0.2 from the \textit{model} group. We add the one with the higher score to the final set and disregard the other one. An alternative is to combine or compute the average of the scores across groups.

Note that without proper thresholding, the candidate set grows very quickly, for instance, if using only 30 peers, the candidate set for \textit{Pitt} on Wikidata is already about 1500 statements.
\end{example}\\

\begin{table*}
  \caption{Discovering candidate statements for \textit{Brad Pitt} from one peer group with 3 peers.}
  \label{tab:brad}
   \resizebox{\textwidth}{!}{\begin{tabular}{lll|l|l}
    \toprule
 \multicolumn{1}{c}{\bf{Russel Crowe}} &  \multicolumn{1}{c}{\bf{Tom Hanks}} &  \multicolumn{1}{c}{\bf{Denzel Washington}} &  \multicolumn{1}{|c}{\bf{Brad Pitt}} & \multicolumn{1}{|c}{\bf{Candidate statements}}\\
    \midrule
(award; Oscar for Best Actor) & (award; Oscar for Best Actor) & (award; Oscar for Best Actor) & (citizen; U.S.) & $\neg$(award; Oscar for Best Actor), 1.0\\
(citizen; New Zealand) & (citizen; U.S.) & (citizen; U.S.) & (child; $x$)& $\neg$(occup.; screenwriter), 1.0\\
(child; $y$) & (child; $z$) & (child; $u$) &  & $\neg \exists l $(instagram; $l$), 0.67\\
(occup.; screenwriter) & (occup.; screenwriter) & (occup.; screenwriter) & & $\neg w$(convicted; $w$), 0.33\\
(convicted; $v$)  & (instagram; $r$) & (instagram; $f$) & & $\neg$(citizen; New Zealand), 0.33\\
(instagram; $t$) & & & & \\
    \bottomrule
  \end{tabular}}
\end{table*}

\noindent
\textbf{Ranking Negative Statements.\ }
Given potentially large candidate sets, in a second step, ranking methods are needed. Our rationale in the design of the following four ranking metrics is to combine frequency signals with popularity and probabilistic likelihoods in a \emph{learning-to-rank model}.
\begin{enumerate}[noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt]
\item \emph{Peer frequency (PEER):} The statement discovery procedure already provides a relative frequency, e.g., 0.9 of a given actor's peers are married, but only 0.1 are political activists. The former is an immediate candidate for ranking.
    \item \emph{Object popularity (POP):} When the discovered statement is of the form  $\neg$\term{(s; p; o)}, its relevance might be reflected by the popularity\footnote{Wikipedia page views.} of the \term{Object}. For example, $\neg$\term{(Brad Pitt; award; Oscar for Best Actor)} would get a higher score than $\neg$\term{(Brad Pitt; award; London Film Critics' Circle Award)}, because of the high popularity of the \textit{Academy Awards} over the \textit{London Film Awards}.
    \item \emph{Frequency of the Property (FRQ):} When the discovered statement has an empty \term{Object} \term{$\neg \exists x$(s; p; x)}, the frequency of the \term{Property} will reflect the authority of the statement. To compute the frequency of a \term{Property}, we refer to its frequency in the KB. For example, \term{$\neg \exists x$(Joel Slater; citizen; x)}  will get a higher score (4.1m citizenships in Wikidata) than \term{$\neg \exists x$(Joel Slater; twitter; x)} (294k Twitter usern\-ames).
    \item \emph{Pivoting likelihood (PIVO):} In addition to these \linebreak frequency/view-based metrics, we propose to consider textual background information about $e$ in order to better decide whether a negative statement is relevant. To this end, we build a set of statement pivoting classifier~\cite{razniewski2017doctoral}, i.e., classifiers that decide whether an entity has a certain statement (or property), each trained on the Wikipedia embeddings~\cite{wikipedia2vec} of 100 entities that have a certain statement (or property), and 100 that do not\footnote{On withheld data, linear regression classifiers achieve 74\% avg.\ accuracy on this task.}. To score a new statement (or property) candidate, we then use the pivoting score of the respective classifier, i.e., the likelihood of the classifier to assign the entity to the group of entities having that statement (or property).
\end{enumerate}

\noindent
The final score of a candidate statement is then computed as follows.

\begin{defn}[Ensemble Ranking Score]
\label{def:ensemble}
{\small
\begin{equation*}
Score=\begin{cases}
 \lambda_1 \text{PEER} + \lambda_2 \text{POP(\textit{o})} + \lambda_3 \text{PIVO}\\ \ \ if\ \ \ \neg(\textit{s; p; o}) \ is \ \mathit{satisfied}\\ \\
 \lambda_1 \text{PEER} + \lambda_4 \text{FRQ(\textit{p})} + \lambda_3 \text{PIVO}\\ \ \ if\ \ \ \neg \exists x (\textit{s; p; x}) \ is \ \mathit{satisfied}\\
\end{cases}
\end{equation*}}
\end{defn}


Hereby $\lambda_1$, $\lambda_2$, $\lambda_3$, and $\lambda_4$ are parameters to be tuned on data withheld from training.