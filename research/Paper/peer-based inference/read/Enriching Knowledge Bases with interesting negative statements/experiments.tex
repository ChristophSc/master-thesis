\section{Experimental Evaluation}
\label{sec:experiments}

\subsection{Peer-based Inference}
\label{subsec:similarityexp}
\noindent
\textbf{Setup.\ }We instantiated the peer-based inference method with 30 peers, popularity based on Wikipedia page views, and peer groups based on entity occupations.
The choice of this simple peering function was inspired by Recoin~\cite{RECOIN}.
In order to further ensure relevant peering, we also only considered entities as candidates for peers, if their Wikipedia viewcount was at least a quarter of that of the subject entity.
We randomly sampled 100 popular Wikidata people. For each of them, we collected 20 negative statement candidates: 10 with the highest \textit{PEER} score, 10 being chosen at random from the rest of retrieved candidates. We then used crowdsourcing\footnote{\url{https://www.mturk.com}} to annotate each of these 2000 statements on whether it was interesting enough to be added to a biographic summary text (Yes/Maybe/No). Each task was given to 3 annotators. Interpreting the answers as numeric scores (1/0.5/0), we found a standard deviation of 0.29, and full agreement of the 3 annotators on 25\% of the questions. Our final labels are the numeric averages among the 3 annotations.

\noindent
\textbf{Parameter Tuning.\ } To learn optimal parameters for the ensemble ranking function (Definition~\ref{def:ensemble}), we trained a linear regression model using 5-fold cross validation on the 2k labels for usefulness. Four example rows are shown in Table~\ref{tab:training}. Note that the ranking metrics were normalized using a ranked transformation to obtain a uniform distribution for every feature.

The average obtained optimal parameter values were -0.03 for \textit{PEER}, 0.09 for \textit{FRQ(p)}, -0.04 for \textit{POP(o)}, and 0.13 for \textit{PIVO},  and a constant value of 0.3., with a 71\% out-of-sample precision.

\begin{table*}
  \caption{Data samples for illustrating parameter tuning.}
  \label{tab:training}
 \resizebox{\textwidth}{!}{\begin{tabular}{llllll}
    \toprule
    \multicolumn{1}{c}{\bf{Statement}} & \multicolumn{1}{c}{\bf{PEER}} & \multicolumn{1}{c}{\bf{FRQ(p)}}& \multicolumn{1}{c}{\bf{POP(o)}}& \multicolumn{1}{c}{\bf{PIVO}}&\multicolumn{1}{c}{\bf{Label}}\\
    \midrule
$\neg$(Bruce Springsteen; award; Grammy Lifetime Achievement Award) & 0.8 & 0.8 & 0.55 & 0.25 & 0.83\\
$\neg$(Gordon Ramsay; lifestyle; mysticism) & 0.3 & 0.8 & 0.8 & 0.65 & 0.33\\
$\neg \exists x$(Albert Einstein; doctoral student; x) & 0.85 & 0.9 & 0.15 & 0.4 & 0.66\\
$\neg \exists x$(Celine Dion; educated at; x) & 0.95 & 0.95 & 0.25 & 0.95 & 0.5\\
    \bottomrule
  \end{tabular}}
  \end{table*}
\noindent
\textbf{Ranking Metric.\ }To compute the ranking quality of our method against a number of baselines, we used the Discounted Cumulative Gain (DCG)~\cite{NDCG}, which is a measure that takes into consideration the rank of relevant statements and can incorporate different relevance levels. DCG is defined as follows:
\begin{equation*}
DCG(i)=\begin{cases}
& G(1)\ \ \ if\ $i=1$\\
 & DCG(i-1)+\frac{G(i)}{log(i)} \ \ \ \text{\textit{otherwise}}
\end{cases}
\end{equation*}
where i is the rank of the result within the result set, and $G(i)$ is the relevance level of the result. We set $G(i)$ to a value between 1 and 3, depending on the annotator's assessment. We then averaged, for each result (statement), the ratings given by all annotators and used it as the relevance level for the result. Dividing the obtained DCG by the DCG of the ideal ranking, we obtained the normalized DCG (nDCG), which accounts for the variance in performance among queries (entities).

\noindent
\textbf{Baselines.\ }We used three \textit{baselines}: As a naive baseline, we randomly ordered the 20 statements per entity. This baseline gives a lower bound on what any ranking model should exceed. We also used two competitive embedding-based baselines, TransE~\cite{transE} and HolE~\cite{holE}. For these two, we used pretrained models, from~\cite{ho2018rule}, on Wikidata (300k statements) containing prominent entities of different types, which we enriched with all the statements about the sampled entities. We plugged their prediction score for each candidate grounded negative statement.\footnote{Note that both models are not able to score statements about universal absence, a trait shared with the object popularity heuristic in our ensemble.}

\begin{table*}
  \caption{Ranking metrics evaluation results for peer-based inference.}
  \label{tab:rankingNDCG}
  \centering
  \resizebox{0.8\textwidth}{!}{\begin{tabular}{llllll}
    \toprule
    \multicolumn{1}{l}{\bf{Ranking Model}} &\multicolumn{1}{c}{\bf{Coverage(\%)}} & \multicolumn{1}{c}{$\boldsymbol{nDCG_3}$} & \multicolumn{1}{c}{$\boldsymbol{nDCG_5}$}& \multicolumn{1}{c}{$\boldsymbol{nDCG_{10}}$}& \multicolumn{1}{c}{$\boldsymbol{nDCG_{20}}$}\\
    \midrule
Random & 100 & 0.37 & 0.41 & 0.50 & 0.73\\
TransE~\cite{transE} & 31 & 0.43 & 0.47 & 0.55 & 0.76\\
HolE~\cite{holE} & 12 &	0.44 & 0.48 & 0.57 & 0.76\\
\midrule
Property Frequency & 11 & \bf{0.61} & \bf{0.61} & \bf{0.66} & \bf{0.82}\\
Object Popularity & 89 & 0.39 & 0.43 & 0.52 & 0.74\\
Pivoting Score & 78 & 0.41 & 0.45 & 0.54 & 0.75\\
Peer Frequency & 100 &	\bf{0.54} &	\bf{0.57} &	\bf{0.63} &	\bf{0.80}\\
\midrule
Ensemble & 100 &	\bf{0.60} &	\bf{0.61} &	\bf{0.67} & \bf{0.82}\\
    \bottomrule
  \end{tabular}}
  \end{table*}
  \noindent
  \textbf{Results.\ }Table \ref{tab:rankingNDCG} shows the average $nDCG$ over the 100 entities for top-k negative statements for k equals 3, 5, 10, and 20. As one can see, our ensemble outperforms the best baseline by 6 to 16\% in $nDCG$. The coverage column reflects the percentage of statements that this model was able to score. For example, for the \textit{Popularity of Object}, $POP(o)$ metric, a universally negative statement will not be scored. The same applies to TransE and HolE.
  
 Ranking with the \textit{Ensemble} and ranking using the \textit{Frequency of Property} outperforms all other ranking metrics and the three baselines, with an improvement over the random baseline of 20\% for k=3 and k=5. Examples of ranked top-3 negative statements for \textit{Albert Einstein} are shown in Table \ref{tab:rank_qualitative}. The random rank basically display any candidate negation if it holds for at least one peer. For instance, \textit{Omar Sharif} is \textit{Einstein}'s peer under the \textit{non-fiction writer} group. This makes the negation ``Tarek Sharif not a child of Einstein'' possible, hence, the necessity for a ranking step. Moreover, \textit{Omar Sharif} is also an actor, which brings other topics to the result set of \textit{Einstein}, such as not winning \textit{film awards}. This is where peer frequency makes a difference, i.e., most of \textit{Einstein}'s peers are \textit{not} actors. By relying on the property frequency for ranking, we can see that only universally absent statements get the highest scores. Even though it displays interesting negations (e.g., despite his status as famous researcher, \textit{Einstein} truly never formally supervised any PhD student), the top-k result set lacks grounded negative statements. Ensemble ranking, on the other hand, takes into consideration several features simultaneously, and covers both classes of negation. It returns interesting statements such as that \textit{Einstein} notably refused to work on the \textit{Manhattan} project, and was suspected of communist sympathies. 
 

\begin{table*}
  \caption{Top-3 results for \textit{Albert Einstein} using 3 ranking metrics.}
  \label{tab:rank_qualitative}
  \centering 
   \resizebox{0.8\textwidth}{!}{\begin{tabular}{l|l|l}
    \toprule
    \multicolumn{1}{l}{\textbf{Random rank}} &    \multicolumn{1}{l}{\textbf{Property frequency}} & \multicolumn{1}{l}{\textbf{Ensemble}}\\
\toprule
\multicolumn{1}{l}{$\neg \exists x$(instagram; x)} &    \multicolumn{1}{l}{$\neg \exists x$(doctoral student; x)} & \multicolumn{1}{l}{$\neg$(occup.; astrophysicist)}\\
 \multicolumn{1}{l}{$\neg$(child; Tarek Sharif)} & \multicolumn{1}{l}{$\neg \exists x$(candidacy in election; x)} & \multicolumn{1}{l}{$\neg$(party; Communist Party USA)}\\
  \multicolumn{1}{l}{$\neg$(award; BAFTA)} & \multicolumn{1}{l}{$\neg \exists x$(noble title; x)} & \multicolumn{1}{l}{$\neg \exists x$(doctoral student; x)}\\
    \bottomrule
  \end{tabular}}
  \label{tbl:may:einstein}
\end{table*}


\noindent
\textbf{Correctness Evaluation.\ } We used crowdsourcing to assess the correctness of results from the peer-based method. We collected 1k negative statements belonging to the three types, namely people, literature work, and organizations. Every statement was annotated 3 times as either correct, incorrect, or ambiguous. 63\% of the statements were found to be correct, 31\% were incorrect, and 6\% were ambiguous. Most incorrect statements are due to KB completion issues. Interpreting the scores numerically (0/0.5/1), annotations showed a standard deviation of 0.23. 

\noindent
\textbf{PCA (Partial Completeness Assumption) vs.\ CWA\ }
For a sample of 200 statements about people (10 each for 20 entities), half generated only relying on the CWA, half additionally filtered to satisfy the PCA (subject has at least one other object for that property~\cite{AMIEP}), we manually checked correctness.
We observed  84\% accuracy
for PCA-based statements, and 57\% for CWA-based statements. So the PCA yields significantly more correct negative statements, though losing the ability to predict universally negative statements.

\noindent
\textbf{Subject coverage.} Our peer-based inference method offers a very high subject coverage and is able to discover negative statements about almost any existing entity in a given KB, whereas for pre-trained embedding-based baselines, many subjects are out-of-vocabulary, or come with too little information to predict statements.






%################################################
%################################################
%################################################


\subsection{Inference with Ordered Peers}
\label{sub:temporalexperiments}

In the following, we used temporal order on specific roles, or on specific attribute values, to compute ordered peer sets. In particular, we used two common forms of temporal information in Wikidata to compute such peer groups: 
\begin{itemize}
    \item \textbf{Time-based Qualifiers (TQ)}: Temporal qualifiers are time signals associated with statements about entities. In Wikidata, some of those qualifiers are \textit{point in time} (P585), \textit{start time} (P580), and \textit{end time} (P582). A few samples are shown in Table~\ref{tab:qualifiers}.
    \item \textbf{Time-based Properties (TP)}: Temporal properties are properties like \textit{follows} (P155) and \textit{followed by} (P156) indicating a chain of entities, ordered from oldest to newest, or from newest to oldest. For instance, \term{[The Cossacks; followed by; War and Peace; followed by; Anna Karenina; ..]}\footnote{Novels of by Leo Tolstoy.}
\end{itemize}

We created TQ groups from aggregating information about people sharing the same statements. For example, \term{position held; President of the U.S.} is one TQ group, where members will have a \textit{start time} for this position, as well as an \textit{end time}. In case of absence of an \textit{end time}, this implies that the statement holds to this day (\term{Donald Trump}'s statement in Table~\ref{tab:qualifiers}). In other words, we aggregated entities sharing the same predicate-object pair, which will be treated as the peer group's title, and ranked them in ascending order of time qualifiers. For the \textit{point in time} qualifier, we simply ranked the dates from oldest to newest, and for the \textit{start/end date}, we ranked the end date from oldest to newest.
If the \textit{end date} is missing, the entity will be moved to the newest slot.

We collected a total of 19.6k TQ groups (13.6k using the \textit{start/end date} qualifier and 6k using the \textit{point in time} qualifier). Based on a manual analysis of a random sample of 100 groups of different sizes, we only considered time series with at least 10 entities\footnote{This variable can be easily adjusted depending on the preference of the developers and/or the purpose of the application.}.

We created TP groups by first collecting all entities reachable by one of the transitive properties, \textit{follows} (P155) and \textit{followed by} (P156). Considering each of the collected entities as a source entity, we computed the longest possible path of entities with only transitive properties. This path consists in an ordered set of peers. To avoid the problem of double-branching (one entity followed by two entities), we considered the two directions separately. Again, one path will be chosen at the end; the one with maximum length. 
The total number of TP groups is 19.7k groups. We limited the size of the groups to at least 10 and at most 150\footnote{We did not truncate the groups, we simply disregarded any group smaller or larger than the thresholds.}. 

\begin{table*}
  \caption{Samples of temporal information in Wikidata.}
  \label{tab:qualifiers}
  \centering
  \scalebox{0.9}{
   \begin{tabular}{l|l}
    \toprule
    \multicolumn{1}{c}{\textbf{Statement}} & \multicolumn{1}{c}{\textbf{Time-based qualifier(s)}}\\
    \midrule
(Barack Obama; position held; U.S. senator) & {\small \textit{start time}}: 3 January 2005; {\small \textit{end time}}: 16 November 2008\\
(Maya Angelou; award received; Presidential Medal of Freedom) & {\small \textit{point in time}}: 2010\\
(Donald Trump; spouse; Melania Trump) & {\small \textit{start time}}: 22 January 2005\\
    \bottomrule
  \end{tabular}
  }
  \end{table*}
  
\noindent
\textbf{Setup and Baseline.} We chose 100 entities, that belongs to at least one ordered set of peers, from Wikidata: 50 people and 50 literature works. We collected top-5 negative statements for each of those entities (for people, we consider TQ groups, and for literature works, TP groups). We made this choice because of the lack of entities of type person with transitive properties. In case an entity belongs to several groups, we merged all the results it is receiving from different groups, ranked them, and retrieved the top-5 statements. Similarly, as a baseline, using the peer-based inference method of Section~\ref{sec:inference}, instantiated with cosine similarity on Wikipedia embeddings~\cite{wikipedia2vec} as similarity function, we collected the top-5 negative statements for the same entities. We ended up with 1k statements, 500 inferred by each model.

\noindent
\textbf{Correctness Evaluation.} We randomly retrieved 400 negative statements from the 1k statements collected above, 200 from each model (100 about people, and 100 about literature works). We then assessed the correctness of each method using crowdsourcing. We showed each statement to 3 annotators, asking them to choose whether this statement is correct, incorrect, or ambiguous. Results are shown in Table~\ref{tab:simvstempcorrec}. Our order-oriented inference method clearly infers less incorrect statements by 9 percentage points for people, and 5 for literature works. It also produces more correct statements for people by 10 percentage points, and literature work by 3. The percentage of queries with full agreement in this task is 37\%. Also, annotations show a standard deviation of 0.17.
\begin{table}
\centering
  \caption{Correctness of order-oriented and peer-based methods.}
  \label{tab:simvstempcorrec}
  \begin{tabular}{llc}
    \toprule
   & \multicolumn{1}{l}{\bf{People}}  & \multicolumn{1}{c}{\bf{Literature Work}}  \\
    \midrule
    \multicolumn{3}{c}{Peer-based inference}\\
    \midrule
           & \multicolumn{1}{l}{\bf{\%}}  & \multicolumn{1}{c}{\bf{\%}}  \\
    \midrule
Correct & 81 & 88\\
Incorrect &  18 & 12\\
Ambiguous & 1 & 0\\
\midrule
\multicolumn{3}{c}{Order-oriented inference}\\
\midrule
       & \multicolumn{1}{l}{\bf{\%}}  & \multicolumn{1}{c}{\bf{\%}}  \\
    \midrule
Correct & \bf{91} & \bf{91}\\
Incorrect & 9 & 7\\
Ambiguous & 0 & 2 \\
    \bottomrule
  \end{tabular}
  
  \end{table}

\noindent
\textbf{Subject Coverage.} To assess the subject coverage of the order-oriented method, we randomly sampled 1k entities from each dataset, and tested whether it is a member of at least one ordered set, thus the ability to infer useful negative statements about it. For TQ groups, we randomly sampled 1k people, which results in a  coverage of 54\%. And for TP groups, we randomly sampled 1k literature works, and also received a coverage of 54\%. Although the order-oriented method produces better negative statements on both notions of correctness and usefulness (as we will see next), it does not outperform the baseline on subject coverage. However, using a different function to order peers might affect this drastically (e.g., using real-valued similarity functions like cosine distance of embeddings).

\noindent
\textbf{Usefulness.} To assess the quality of our inferred statements from the order-oriented inference method against the baseline (the peer-based inference method), we presented to the annotators two sets of top-5 negative statements about a given entity, and asked them to choose the more interesting set. The total number of opinions collected, given 100 entities, 3 annotations each, is 300. To avoid biases, we repeatedly switched the position of the sets. Results are shown in Table~\ref{tab:WDinterestingness}. Overall results show that our method is preferred by 10\% of the entities for both domains. The standard deviation of this task is 0.24 and the percentage of queries with full agreement is 18\%. We observe two advantages of the ordered set of peers over the previous method: i) it gives better interpretations of what a peer is, by automatically producing labels for peer groups (e.g., Presidents of the U.S., Winners of the Best Actor Academy Award); and ii) it maximizes the \textit{peerness} within a group. For instance, with Wikipedia embedding~\cite{wikipedia2vec}, closest peers to \textit{Donald Trump} are \textit{Hillary Clinton} and \textit{Donald Trump Jr.}. While the peerness with the input entity is obvious, there is not much similarity between the peers themselves, hence, very sparse candidate negations. However, with the order-oriented peering, \textit{Trump}'s peers include \textit{Barack Obama} and \textit{George W. Bush}, who are also peers of each other. 

\noindent
\textbf{Evaluation of Verbalizations.} One main contribution that our order-oriented inference method offers are \textit{verbalizations} produced with every inferred negative statement. In other words, it can, unlike the peer-based inference method, produce more concrete explanations of the usefulness of the inferred negations. For example, the inferred negative statement \term{$\neg$(Abraham Lincoln; cause of death; natural} \term{causes)} was inferred by both of our methods. However, each method offers a different verbalization. For the peer-based method, the verbalization is ``unlike 10 of 30 similar people'', and for the order-oriented method is ``unlike 12 of the previous 12 presidents of the U.S.''. To assess the quality of the verbalizations more formally, we conducted a crowdsourcing task with 100 useful negations that were inferred by both methods from our previous experiment. For every negative statement, the annotator was shown two different verbalizations on ``why is this negative statement noteworthy''. We asked the annotator to choose the better verbalization, she can choose Verbalization1, Verbalization2, or Either/Neither. Results show that verbalizations produced by our order-oriented inference method were chosen 76\% of the time, by the peer-based inference method 23\% of the time, and the either or neither option only 1\% of the time. The standard deviation is 0.23, and the percentage of queries with full agreement is 20\%. Table~\ref{tab:explanations} shows a number of examples, using different grouping functions for the peer-based method.

\begin{table*}
  \caption{Negative statements and their verbalizations using peer-based and order-oriented methods.}
  \label{tab:explanations}
 \resizebox{\textwidth}{!}{\begin{tabular}{llll}
    \toprule
    \multicolumn{1}{c}{\bf{Statement}} & \multicolumn{1}{c}{\bf{Order-oriented}} & \multicolumn{1}{c}{\bf{Peer-based}}& \multicolumn{1}{c}{\bf{Peering}}\\
      \multicolumn{1}{c}{\bf{}} & \multicolumn{1}{c}{\textit{Unlike..}} & \multicolumn{1}{c}{\textit{Unlike..}}& \multicolumn{1}{c}{}\\
    \midrule
   $\neg$(Emmanuel Macron; member;  National Assembly)	& 29 of 36 members of La République En Marche party & 70 of 100 similar people &	WP embed.~\cite{wikipedia2vec}\\
   $\neg$(Tim Berners-Lee; citizenship; U.S.) & 101 of previous 115 winners of the MacArthur Fellowship & 53 of 100 sim. comp. scientists &	Structured facets\\
 $\neg$(Michael Jordan; occupation; basketball coach) & 27 of prev. 49 winners of the NBA All-Defensive Team & 31 of 100 sim. people & WP embed.~\cite{wikipedia2vec}\\
 $\neg$(Theresa May; position; Opposition Leader) & 11 of prev. 14 Leaders of the Conservative Party & 10 of 100 sim. people &	WP embed.~\cite{wikipedia2vec}\\
 $\neg$(Cristiano Ronaldo; citizenship; Brazil) & 4 of prev. 7 winners of the Ballon d'Or & 20 of 100 sim. football players & Structured facets\\
    \bottomrule
  \end{tabular}}
  \end{table*}

\begin{table}
\centering
  \caption{Usefulness of order-oriented and peer-based methods.}
  \label{tab:WDinterestingness}
  \begin{tabular}{llc}
    \toprule
    & \multicolumn{1}{l}{\bf{People}}  & \multicolumn{1}{c}{\bf{Literature Work}}  \\
    \midrule
           &     \multicolumn{1}{l}{\bf{\%}}  & \multicolumn{1}{c}{\bf{\%}}\\
           \midrule
    Peer-based inference & 42 & 44\\
Order-oriented inference  & \bf{52} & \bf{54} \\
Both & 6 & 2\\
    \bottomrule
  \end{tabular}
  
  \end{table}







%#####################################
\subsection{Conditional Negative Statements Evaluation}
\label{subsec:restrictedexp}

We evaluated our lifting technique to retrieve useful conditional negative statements, based on three criteria: (i) compression, (ii) correctness, and (iii) usefulness. We collected the top-200 negative statements about 100 entities (people, organizations, and art work), and then applied lifting on them.

\noindent
\textbf{Compression.}   On average, 200 statements are reduced to 33, which means that lifting compresses the result set by a factor of 6.

\noindent
\textbf{Correctness.} We asked the crowd to assess the correctness of 100 conditional negative statements (3 annotations per statement), chosen randomly. To make it easier for annotators who are unfamiliar with RDF  triples\footnote{Especially because of the triple-pattern condition.}, we manually converted them into natural language statements, for example ``\textit{Bing Crosby did not play any keyboard instruments}''. Results show that 57\% were correct, 23\% incorrect, and 20\% were uncertain. The standard deviation of this task is 0.24 and the percentage of queries with full agreement is 18\%.

\noindent
\textbf{Usefulness.} For every entity, we showed 3 annotators 2 sets of top-3 negative statements: a grounded and universally negative statements set and a conditional negative statement set, and asked them to choose the one with more interesting information. Results are shown in Table~\ref{tab:conditionaluse}. The conditional statements were chosen 45 percentage points more than the grounded and universally negative statements. The standard deviation of this task is 0.22 and the percentage of queries with full agreement is 21\%. The significant out-performance of the conditional class over the other two classes is that it encapsulates them. Without losing the information from the original result set, lifting summarizes negations in meaningful manner, at the same time, allowing more diverse statements to be displayed in a top-k set. An example is shown in Table~\ref{tab:leolifting}, with entity $e=$\textit{Leonardo Dicaprio}, and its top-3 results. Even though he is one of the most accomplished actors in the world, unlike many of his peers, he never attempted directing any kind of creative work (films, plays, television shows, etc..).

\begin{table}
  \caption{Usefulness of conditional negative statements.}
  \centering
  \label{tab:conditionaluse}
  \begin{tabular}{l|l}
    \toprule
        \multicolumn{1}{c}{\textbf{Preferred}} & 
        \multicolumn{1}{c}{\textbf{(\%)}}\\
            \midrule
    \multicolumn{1}{l}{Conditional negative statements} & \multicolumn{1}{c}{\textbf{70}}\\
    \multicolumn{1}{l}{Grounded and universally negative statements} & \multicolumn{1}{c}{25}\\
    \multicolumn{1}{l}{Either or neither} & \multicolumn{1}{c}{5}\\
    \bottomrule
  \end{tabular}
\end{table}

\begin{table*}
  \caption{Top-3 negative statements about \textit{Leonardo Dicaprio}, before and after lifting.}
  \label{tab:leolifting}
  \centering
  \scalebox{0.9}{
  \begin{tabular}{l|l}
    \toprule
    \multicolumn{1}{c}{\textbf{Negative statements}} & \multicolumn{1}{c}{\textbf{Conditional negative statements}}\\
    \midrule
$\neg$(occupation; film director) & $\neg\exists o$ (occupation; $o$) ($o$; subclass of; director)\\
$\neg$(occupation; theater director) & $\neg \exists x$(spouse; x) \\
$\neg$(occupation; television director) & $\neg \exists x$(child; x) \\
    \bottomrule
  \end{tabular}}
  \end{table*}










%######################################
